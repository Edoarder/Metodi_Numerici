{
  
    
        "post0": {
            "title": "Modulo 1",
            "content": "#collapse %matplotlib inline import matplotlib.pyplot as plt import numpy as np import scipy.sparse as ss from math import * rng = np.random.default_rng() . . In questa relazione mi occuper√≤ di studiare la termodinamica di un sistema tipo Ising, ma con interazioni a lunga distanza tra elementi casuali del network. . Per farlo inizieremo prima dalla implementazione di un modello di Ising 2D classico per poi generalizzarlo e applicarlo al caso di interesse. . Lo studio sar√† fatto attraverso un&#39;analisi Monte-Carlo: una famiglia di metodi computazionali basati sul campionamento random delle possibili realizzazioni del sistema, che sono pesate da una distribuzione di probabilit√† legata il modello trattato. Attraverso questo campionamento casuale si cerca di esplorare tutto lo spazio delle configurazioni per poi andare a calcolare le osservabili di interesse sul campione finale. L&#39;algoritmo Monte-Carlo utilizzato si basa su una catena di Markov, che si ottiene estraendo le varie configurazioni attraverso la distribuzione di Boltzmann. . Esistono vari algoritmi che soddisfano le condizioni richieste dai metodi Monte-Carlo, uno dei pi√π usati, che √® stato usato anche per lo studio dei sistemi in questa relazione, √® il cosiddetto algoritmo Metropolis. L&#39;implementazione richiederebbe l&#39;uso di un generatore di numeri random, ma nella pratica vengono utilizzate routine che producono numeri detti pseudo-random attraverso algoritmi che garantiscono, entro certi limiti, numeri scorrelati tra loro e una distribuzione uniforme. . Un esempio di generatore di numeri pseudo-random √® riportato nel codice sottostante, che rientra nella categoria dei cosiddetti generatori congruenti lineari, ovvero dove partendo da un certo valore $x_0$ la successione dei numeri pseudo casuali viene generata attraverso: $$X_{i+1}=mod(aX_i+c,m)$$ Per certi valori dei parametri √® dimostrato che questa successione si comporta piuttosto bene come generatore, in questo caso sono stati utilizzati: $$a=16807, quad c=0, quad m=2^{31}-1$$ . def rando(N,x0,ini,fin): x=np.zeros((N+2)) a=16807 c=0 e=31 m=2**e-1 x[0]=x0 # initial value for i in range(N+1): x[i+1]=(a*x[i]+c)%m return x[2:]/m*(fin-ini) + ini x=rando(1000000,101,0,1) plt.hist(x,100,rwidth=1) plt.show() . Come si pu√≤ notare dall&#39;istogramma precedente la distribuzione dei valori risulta piuttosto uniforme per un gran numero di estrazioni, condizione necessaria, ma comunque non sufficiente a dimostrare che si tratti di un buon generatore. . Nei codici seguenti per√≤ √® stato utilizzato un generatore di numeri pseudo-random presente all&#39;interno della libreria NumPy, poich√® risulta pi√π sicuro e pi√π facile da implementare. . Algoritmo Metropolis . Tra tutte le implementazioni che soddisfano alcune condizioni necessarie per il corretto funzionamento del Monte-Carlo, come quella di equilibrio, e altre condizioni utili seppur non necessarie, come il bilancio dettagliato, una classe di algoritmi tra i pi√π efficienti √® Metropolis. Per descrivere il funzionamento del Metropolis √® necessario descrivere il processo stocastico che permette di passare da uno stato $a$ ad uno stato $b$ in un passo elementare della catena, ovvero bisogna definire la probabilit√† della transizione. Detta $p_i$ la probabilit√† di realizzazione della configurazione $i$, l&#39;algoritmo si pu√≤ articolare come segue: . A partire dallo stato di partenza $a$ si seleziona uno stato $ tilde{b}$ a partire da una probabilit√† di transizione di prova | Se risulta che $p_{ tilde{b}}&gt;p_a$ allora si accetta $ tilde{b}$ come stato successivo $b$ | Se invece $p_{ tilde{b}}&lt;p_a$ allora si accetta $b= tilde{b}$ con probabilit√† $p_{ tilde{b}}/p_a$, e $a=b$ altrimenti. | . Implementazione . Per l&#39;implementazione dell&#39;algoritmo sono state create alcune funzioni: . Una che definisce lo stato iniziale: dato il numero di siti fornisce uno stato &quot;caldo&quot; con gli spin scelti tutti casualmente o &quot;freddo&quot; con gli spin tutti orientati nella stessa direzione, o +1 o -1 | . #collapse def initialstate(N, random=True, cold=+1): if random: state = 2*rng.integers(2, size=(N,N))-1 elif cold== 1: state = np.int_(np.ones((N,N))) elif cold== -1: state = -np.int_(np.ones((N,N))) else: return print(&quot;Put cold = +1 or -1&quot;) return state . . Il passo del Metropolis √® implementato dalla funzione mcmove, che prende in input la configurazione precedente, il valore di $ beta$, ovvero l&#39;inverso della temperatura, e la matrice di adiacenza del sistema considerato. La funzione restituisce la configurazione scelta attraverso il Metropolis. | . #collapse def mcmove(config, beta): &quot;Monte Carlo move using Metropolis algorithm&quot; for i in range(N): for j in range(N): a = rng.integers(N) b = rng.integers(N) s = config[a, b] h = config[(a+1)%N,b] + config[a,(b+1)%N] + config[(a-1)%N,b] + config[a,(b-1)%N] cost = 2*s*h if cost &lt; 0: s = -s elif rng.random() &lt; np.exp(-cost*beta): s = -s config[a,b] = s return config . . Le seguenti funzioni calcolano rispettivamente l&#39;energia e la magnetizzazione della configurazione di spin in input. | . #collapse def calcEnergy(config): &quot;Energy of a given configuration&quot; energy = 0 for i in range(len(config)): for j in range(len(config)): S = config[i,j] h = config[(i+1)%N, j] + config[i,(j+1)%N] + config[(i-1)%N, j] + config[i,(j-1)%N] energy += -h*S return energy/4 def calcMag(config): &quot;Magnetization of a given configuration&quot; mag = np.sum(config) return mag . . La funzione che restituisce la suscettivit√† magnetica o il calore specifico. | . #collapse def Di(m): return (np.mean(m*m)-np.mean(m)**2)*N*N . . La funzione che restituisce la varianza relativa alle grandezze calcolate con la funzione precedente, calcolata attraverso il metodo del Bootstrap. | . #collapse def boots(v,M,MM): N=len(v) y=np.zeros((M,N)) for i in range(M): nr=rng.integers((N-MM), size=N//MM) for j in range(0,N//MM): y[i,j:j+MM]=v[int(nr[j]):int(nr[j])+MM] FF=np.zeros(M) for i in range(M): FF[i]=Di(y[i])/MM varia=0 med1=0 med2=0 for i in range(M): med1+=FF[i]**2 med2+=FF[i] med1=med1/M varia=med1-(med2/M)**2 var=np.sqrt(varia) return var . . nB = 50 # number of points of beta decSteps= 10 # number of MC sweeps for decorrelation mcSteps = 100 # number of MC sweeps for calculation beta = np.linspace(0, 1, nB) N=8 # divide by number of samples, and by system size to get intensive values E,M,C,X,varx,varc,Esig,Msig = np.zeros((nB)), np.zeros((nB)), np.zeros((nB)), np.zeros((nB)), np.zeros((nB)), np.zeros((nB)), np.zeros((nB)), np.zeros((nB)) n1, n2 = 1.0/(N*N), 1.0/(N*N) config = initialstate(N) for bb in range(1,nB): Ene=np.zeros(mcSteps) Mag=np.zeros(mcSteps) iB=beta[bb]; iB2=iB*iB; for i in range(mcSteps): for j in range(decSteps): # steps of decorrelation mcmove(config, iB) # Metropolis step Ene[i] = calcEnergy(config) # calculate the energy Mag[i] = calcMag(config) # calculate the magnetization E[bb] = np.mean(Ene)*n1 Esig[bb] = np.mean((Ene*n1-E[bb])**2) M[bb] = np.mean(Mag)*n1 Msig[bb] = np.mean((Mag*n1-M[bb])**2) C[bb] = Di(Ene)*n2 varc[bb]=boots(Ene,25,5)*n2 X[bb] = Di(Mag)*n2 varx[bb]=boots(Mag,25,5)*n2 print(bb) f = plt.figure(figsize=(18, 10)); # plot the calculated values sp = f.add_subplot(2, 2, 1 ) plt.plot(beta, E, marker=&#39;o&#39;, color=&#39;RoyalBlue&#39;) plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Energy &quot;, fontsize=16) plt.twinx() plt.plot(beta,Esig,color=&quot;r&quot;,label=&#39;Sigma&#39;) sp = f.add_subplot(2, 2, 2 ) plt.plot(beta, abs(M), marker=&#39;o&#39;, color=&#39;RoyalBlue&#39;) plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Magnetization &quot;, fontsize=16) plt.twinx() plt.plot(beta,Msig,color=&quot;r&quot;,label=&#39;Sigma&#39;) sp = f.add_subplot(2, 2, 3 ) plt.plot(beta, C, marker=&#39;o&#39;, color=&#39;RoyalBlue&#39;) plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Specific Heat &quot;, fontsize=16) plt.twinx() plt.plot(beta,varc,color=&quot;r&quot;,label=&#39;Sigma&#39;) sp = f.add_subplot(2, 2, 4 ) plt.plot(beta, X, marker=&#39;o&#39;, color=&#39;RoyalBlue&#39;) plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Susceptibility&quot;, fontsize=16) plt.twinx() plt.plot(beta,varx,color=&quot;r&quot;,label=&#39;Sigma&#39;) #plt.legend() plt.show() . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 . varx . array([ 0. , 311.73219824, 178.10604311, 191.1476679 , 295.26226157, 297.07794073, 343.49786063, 451.53864225, 503.63836708, 430.41102397, 465.73474464, 587.1382228 , 596.57380073, 607.20098257, 505.46200926, 703.10038782, 777.35281715, 708.88052511, 576.96705838, 713.42544243, 694.86412822, 393.14027131, 389.3602578 , 413.2960157 , 269.36709668, 288.21950894, 242.29561473, 219.57262385, 262.1933442 , 170.62912503, 130.15116079, 142.80455109, 190.56165299, 140.90273308, 105.43767592, 126.51245495, 89.75946715, 116.11458445, 86.50919162, 83.5259665 , 97.98284887, 61.76572021, 130.64587799, 63.54486786, 76.3457325 , 61.91947239, 55.36905596, 60.93992449, 62.91501084, 61.79485417]) . nB = 20 # number of points of beta decSteps= 10 # number of MC sweeps for decorrelation mcSteps = 100 # number of MC sweeps for calculation beta = np.linspace(0, 2, nB) linell = np.array([8]) Nell=linell.size # divide by number of samples, and by system size to get intensive values for Ni in range(Nell): # DIMENSIONI VARIABILI N=linell[ii] E,M,C,X,varx,varc,Esig,Msig = np.zeros((nB,Nell)), np.zeros((nB,Nell)), np.zeros((nB,Nell)), np.zeros((nB,Nell)), np.zeros((nB,Nell)), np.zeros((nB,Nell)), np.zeros((nB,Nell)), np.zeros((nB,Nell)) n1, n2 = 1.0/(mcSteps*N*N), 1.0/(mcSteps*mcSteps*N*N) config = initialstate(N) for bb in range(1,nB): Ene=np.zeros(mcSteps) Mag=np.zeros(mcSteps) E1 = M1 = E2 = M2 = 0 iB=beta[bb]; iB2=iB*iB; for i in range(mcSteps): for j in range(decSteps): # steps of decorrelation mcmove(config, iB) # Metropolis step Ene[i] = calcEnergy(config) # calculate the energy Mag[i] = calcMag(config) # calculate the magnetization E1 = E1 + Ene[i] M1 = M1 + Mag[i] M2 = M2 + Mag[i]**2 E2 = E2 + Ene[i]**2 E[bb,Ni] = np.mean(Ene) Esig[bb,Ni] = np.mean((Ene-E[bb,Ni])**2) M[bb,Ni] = np.mean(Mag) Msig[bb,Ni] = np.mean((Mag-M[bb,Ni])**2)*n1 C[bb,Ni] = (n1*E2 - n2*E1*E1)*iB2 varc[bb,Ni]=boots(Ene,25,5) X[bb,Ni] = (n1*M2 - n2*M1*M1)*iB varx[bb,Ni]=boots(Mag,25,5) print(bb) . f = plt.figure(figsize=(18, 10)); # plot the calculated values sp = f.add_subplot(2, 2, 1 ) plt.plot(beta, E, marker=&#39;o&#39;, color=&#39;RoyalBlue&#39;) plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Energy &quot;, fontsize=16) plt.axis(&#39;tight&#39;) plt.twinx() plt.plot(beta,Esig,color=&quot;r&quot;,label=&#39;Sigma&#39;) sp = f.add_subplot(2, 2, 2 ) plt.plot(beta, abs(M), marker=&#39;o&#39;, color=&#39;RoyalBlue&#39;) plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Magnetization &quot;, fontsize=16) plt.axis(&#39;tight&#39;) plt.twinx() plt.plot(beta,Msig,color=&quot;r&quot;,label=&#39;Sigma&#39;) sp = f.add_subplot(2, 2, 3 ) plt.plot(beta, C, marker=&#39;o&#39;, color=&#39;RoyalBlue&#39;) plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Specific Heat &quot;, fontsize=16) plt.axis(&#39;tight&#39;) plt.errorbar(beta,C,yerr=varc/100) #,color=&quot;r&quot;,label=&#39;Sigma&#39; sp = f.add_subplot(2, 2, 4 ) plt.plot(beta, X, marker=&#39;o&#39;, color=&#39;RoyalBlue&#39;) plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Susceptibility&quot;, fontsize=16) plt.axis(&#39;tight&#39;) plt.errorbar(beta,X,yerr=varx/100) #plt.legend() plt.show() . np.sum((Ene-np.mean(Ene))**2) . 0.0 . Implementazione della matrice di adiacenza . #collapse def initialstate2(N, random=True, cold=+1): if random: state = 2*rng.integers(2, size=(N))-1 elif cold== 1: state = np.int_(np.ones((N))) elif cold== -1: state = -np.int_(np.ones((N))) else: return print(&quot;Put cold = +1 or -1&quot;) return state . . #collapse def mcmove2(config, beta, A): &quot;Monte Carlo move using Metropolis algorithm&quot; for i in range(N): a = rng.integers(N) s = config[a] cost=sum(-(s*A[a])*config)-sum((s*A[a])*config) if -cost &lt; 0: s = -s elif rng.random() &lt; np.exp(cost*beta): s = -s config[a] = s return config . . #collapse def calcEnergy2(config): &quot;Energy of a given configuration&quot; energy = 0 for i in range(len(config)): S = config[i] energy += sum(-S*A[i]*config) return energy/config.size/4 . . #collapse def AdD(d,L): # This function returns the sparse adjacency matrix of a d-dimensonal # integer lattice with side L and the total number of elements N=L**d # Only for Periodic Boundary Conditions import scipy.sparse as ss N=L**d A=ss.dok_matrix((N,N)) for i in range(N): for j in range(d): A[i,(i+L**j)%N] = 1 A[i,(i-L**j)%N] = 1 return A.copy(), N . . nB = 50 # number of points of beta decSteps= 10 # number of MC sweeps for decorrelation mcSteps = 100 # number of MC sweeps for calculation beta = np.linspace(0, 1, nB) L=7 N=AdD(2,L)[1] A=AdD(2,L)[0] # divide by number of samples, and by system size to get intensive values E,M,C,X,varx,varc,Esig,Msig = np.zeros((nB)), np.zeros((nB)), np.zeros((nB)), np.zeros((nB)), np.zeros((nB)), np.zeros((nB)), np.zeros((nB)), np.zeros((nB)) n1, n2 = 1.0/(N*N), 1.0/(N*N) config = initialstate2(N) for bb in range(1,nB): Ene=np.zeros(mcSteps) Mag=np.zeros(mcSteps) iB=beta[bb]; iB2=iB*iB; for i in range(mcSteps): for j in range(decSteps): # steps of decorrelation mcmove2(config, iB, A) # Metropolis step Ene[i] = calcEnergy2(config) # calculate the energy Mag[i] = calcMag(config) # calculate the magnetization E[bb] = np.mean(Ene)*n1 Esig[bb] = np.mean((Ene*n1-E[bb])**2) M[bb] = np.mean(Mag)*n1 Msig[bb] = np.mean((Mag*n1-M[bb])**2) C[bb] = Di(Ene)*n2 varc[bb]=boots(Ene,25,5)*n2 X[bb] = Di(Mag)*n2 varx[bb]=boots(Mag,25,5)*n2 print(bb) f = plt.figure(figsize=(18, 10)) # plot the calculated values sp = f.add_subplot(2, 2, 1 ) plt.plot(beta, E, marker=&#39;o&#39;, color=&#39;RoyalBlue&#39;) plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Energy &quot;, fontsize=16) plt.twinx() plt.plot(beta,Esig,color=&quot;r&quot;,label=&#39;Sigma&#39;) sp = f.add_subplot(2, 2, 2 ) plt.plot(beta, abs(M), marker=&#39;o&#39;, color=&#39;RoyalBlue&#39;) plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Magnetization &quot;, fontsize=16) plt.twinx() plt.plot(beta,Msig,color=&quot;r&quot;,label=&#39;Sigma&#39;) sp = f.add_subplot(2, 2, 3 ) plt.plot(beta, C, marker=&#39;o&#39;, color=&#39;RoyalBlue&#39;) plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Specific Heat &quot;, fontsize=16) plt.twinx() plt.plot(beta,varc,color=&quot;r&quot;,label=&#39;Sigma&#39;) sp = f.add_subplot(2, 2, 4 ) plt.plot(beta, X, marker=&#39;o&#39;, color=&#39;RoyalBlue&#39;) plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Susceptibility&quot;, fontsize=16) plt.twinx() plt.plot(beta,varx,color=&quot;r&quot;,label=&#39;Sigma&#39;) #plt.legend() plt.show() . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 . nB = 100 # number of points of beta decSteps= 10 # number of MC sweeps for decorrelation mcSteps = 100 # number of MC sweeps for calculation beta = np.linspace(0, 4, nB) L=7 N=L**2 # divide by number of samples, and by system size to get intensive values E,M,C,X,varx,varc,Esig,Msig = np.zeros((nB)), np.zeros((nB)), np.zeros((nB)), np.zeros((nB)), np.zeros((nB)), np.zeros((nB)), np.zeros((nB)), np.zeros((nB)) n1, n2 = 1.0/(N*N), 1.0/(N*N) config = initialstate2(N) for bb in range(1,nB): Ene=np.zeros(mcSteps) Mag=np.zeros(mcSteps) iB=beta[bb]; iB2=iB*iB; for i in range(mcSteps): for j in range(decSteps): # steps of decorrelation mcmove2(config, iB, ss.rand(N,N,3/N).todok() ) # Metropolis step Ene[i] = calcEnergy2(config) # calculate the energy Mag[i] = calcMag(config) # calculate the magnetization E[bb] = np.mean(Ene)*n1 Esig[bb] = np.mean((Ene*n1-E[bb])**2) M[bb] = np.mean(Mag)*n1 Msig[bb] = np.mean((Mag*n1-M[bb])**2) C[bb] = Di(Ene)*n2 varc[bb]=boots(Ene,25,5)*n2 X[bb] = Di(Mag)*n2 varx[bb]=boots(Mag,25,5)*n2 print(bb) f = plt.figure(figsize=(18, 10)) # plot the calculated values sp = f.add_subplot(2, 2, 1 ) plt.plot(beta, E, marker=&#39;o&#39;, color=&#39;RoyalBlue&#39;) plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Energy &quot;, fontsize=16) plt.twinx() plt.plot(beta,Esig,color=&quot;r&quot;,label=&#39;Sigma&#39;) sp = f.add_subplot(2, 2, 2 ) plt.plot(beta, abs(M), marker=&#39;o&#39;, color=&#39;RoyalBlue&#39;) plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Magnetization &quot;, fontsize=16) plt.twinx() plt.plot(beta,Msig,color=&quot;r&quot;,label=&#39;Sigma&#39;) sp = f.add_subplot(2, 2, 3 ) plt.plot(beta, C, marker=&#39;o&#39;, color=&#39;RoyalBlue&#39;) plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Specific Heat &quot;, fontsize=16) plt.twinx() plt.plot(beta,varc,color=&quot;r&quot;,label=&#39;Sigma&#39;) sp = f.add_subplot(2, 2, 4 ) plt.plot(beta, X, marker=&#39;o&#39;, color=&#39;RoyalBlue&#39;) plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Susceptibility&quot;, fontsize=16) plt.twinx() plt.plot(beta,varx,color=&quot;r&quot;,label=&#39;Sigma&#39;) #plt.legend() plt.show() . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 . meas=20 dec=20 tmin=0 tmax=2 tn=60 t=0 ma=np.zeros(tn) en=np.zeros(tn) xi=np.zeros(tn) cs=np.zeros(tn) ma2=np.zeros(tn) en2=np.zeros(tn) m2=np.zeros(meas) e2=np.zeros(meas) mm=0 ee=0 A=AdD(3,3)[0] N=AdD(3,3)[1] tempe=np.linspace(tmin,tmax,tn) for it in tempe: state=np.ones((meas,N)) state[0]=initialstate(N,random=True) #state[0]=np.ones(N) e=np.zeros(meas) m=np.zeros(meas) m[0]=calcMag(state[0]) for i in range(meas-1): # misure for j in range(dec): # decorrelazione #print(str(i)+&quot;: &quot;+str(state)) state[i+1]=mcmove(state[i], it, A).copy() m[i+1]=calcMag(state[i+1]) e[i+1]=calcEnergy(state[i+1]) m2[i+1]=m[i+1]*m[i+1] e2[i+1]=e[i+1]*e[i+1] m=m[meas//10:] ma[t]=sum(m)/meas ma2[t]=sum(m2)/meas en[t]=sum(e)/meas en2[t]=sum(e2)/meas xi[t]=ma2[t]-ma[t-1]**2 cs[t]=en2[t]-en[t-1]**2 t+=1 plt.figure(figsize=(12, 9)) plt.subplot(221) plt.scatter(tempe,abs(ma)) plt.xlabel(&#39;beta (1/T)&#39;) plt.ylabel(&#39;Magnetization&#39;) plt.subplot(222) plt.scatter(tempe,en) #plt.ylim(-1,1) plt.xlabel(&#39;beta (1/T)&#39;) plt.ylabel(&#39;Energy&#39;) plt.subplot(223) plt.scatter(tempe,(xi)) plt.xlabel(&#39;beta (1/T)&#39;) plt.ylabel(&#39;Susceptibility&#39;) plt.subplot(224) plt.scatter(tempe,(cs)) plt.xlabel(&#39;beta (1/T)&#39;) plt.ylabel(&#39;Specific heat capacity&#39;) plt.show() . config = initialstate(N) for bb in range(1,nB): E1 = M1 = E2 = M2 = 0 iB=beta[bb]; iB2=iB*iB; for i in range(mcSteps): for j in range(decSteps): # steps of decorrelation mcmove(config, iB) Ene = calcEnergy(config) # calculate the energy Mag = calcMag(config) # calculate the magnetization E1 = E1 + Ene M1 = M1 + Mag M2 = M2 + Mag*Mag E2 = E2 + Ene*Ene E[bb] = n1*E1 M[bb] = n1*M1 C[bb] = (n1*E2 - n2*E1*E1)*iB2 X[bb] = (n1*M2 - n2*M1*M1)*iB .",
            "url": "https://edoarder.github.io/Metodi_Numerici/2020/06/08/Pising-Copy1-2.html",
            "relUrl": "/2020/06/08/Pising-Copy1-2.html",
            "date": " ‚Ä¢ Jun 8, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Modulo 2",
            "content": "Il modello in esame rappresenta un sistema di $N$ spin $1/2$ quantistici che interagiscono tra di loro in maniera casuale (statica). Pu√≤ essere visto come la generalizzazione di un modello di Ising quantistico unidimensionale. √à importante sottolineare come la topologia del sistema non sia importante, ma lo √® considerare gli spin fissi e distinguibili, e dare un ordinamento ai singoli elementi del sistema in modo da poter rappresentare le loro interazioni attraverso una matrice di adiacenza. Infatti, una volta labellati gli elementi con un numero intero che va da 1 ad N, essi si possono considerare giacenti su una variet√† unidimensionale, avendo per√≤ interazioni tra spin anche a lunga distanza. Infatti la rappresentazione hamiltoniana di un sistema del genere differisce da quella di un modello di Ising quantistico 1D solamente per il fatto che la somma su $i$ e $j$ non √® ristretta ai primi vicini, ma a tutti gli elementi del sistema e $J_{ij}$ √® una matrice $N times N$ generica. In questo senso il modello pu√≤ essere visto come una generalizzazione del modello di Ising, in quanto esistono matrici di adiacenza $J_{ij}$, che rappresentano un reticolo unidimensionale, per cui il modello rappresentato √® il modello di Ising, ma, attraverso altre matrici di adiacenza, si possono rappresentare per esempio modelli di Ising quantistici 2D oppure, come nel caso sotto esame, sistemi con interazioni casuali. . Iniziamo dunque con l&#39;implementazione dell&#39;algoritmo che diagonalizza l&#39;Hamiltoniana di un modello di Ising quantistico 1D con interazioni a primi vicini standard. L&#39;implementazione consiste in alcuni step: . Si costruisce la base con cui si rappresenter√† l&#39;Hamiltoniana . | Si costruisce l&#39;Hamiltoniana: . Contributo delle interazioni a primi vicini | Implementazione delle condizioni al bordo periodiche | Contributo delle interazioni con un campo esterno longitudinale $h$ | Contributo delle interazioni con un campo esterno trasversale $g$ | . | Si trova l&#39;autovalore minore e l&#39;autovettore corrispondente (Ground State) . | Si calcola la magnetizzazione associata al Ground State . | Si calcola la suscettivit√† magnetica e il calore specifico . | #collapse %matplotlib inline import matplotlib.pyplot as plt import numpy as np import scipy.sparse.linalg as ssl import scipy.sparse as ss from scipy.sparse import lil_matrix . . Modello di Ising 1D . hh=0 volte=100 volte+=1 lins=np.linspace(0,2,volte) linell=np.arange(5,10) nell=linell.size magniz=np.zeros((volte,nell)) Egs=np.zeros((volte+1,nell)) xi=np.zeros((volte+1,nell)) ci=np.zeros((volte+1,nell)) ell1=0 for ell in linell: ggg=0 NumTot=2**ell ev=np.zeros((volte,NumTot)) Psii=np.zeros((volte,NumTot)) for gg in lins: PBC=True # Costruisco la base iSpin=np.zeros((NumTot,ell)) for ii in range(NumTot): itemp=ii for jj in range(ell): iSpin[ii,jj]=np.floor(itemp%2) itemp=itemp/2 # Costruisco l&#39;Hamiltoniana HamOut=lil_matrix(np.zeros((NumTot,NumTot))) # Sigma_z Sigma_z [coupling] for iHam in range(ell-1): for ii in range(NumTot): if iSpin[ii,iHam]==iSpin[ii,iHam+1]: HamOut[ii,ii]=HamOut[ii,ii]-1 else: HamOut[ii,ii]=HamOut[ii,ii]+1 # This implements periodic boundary conditions if PBC: for ii in range(NumTot): if iSpin[ii,ell-1]==iSpin[ii,0]: HamOut[ii,ii]=HamOut[ii,ii]-1 else: HamOut[ii,ii]=HamOut[ii,ii]+1 # Sigma_z [longitudinal field] for iHam in range(ell): for ii in range(NumTot): if iSpin[ii,iHam]==1: HamOut[ii,ii]=HamOut[ii,ii]-hh else: HamOut[ii,ii]=HamOut[ii,ii]+hh # Sigma_x [transverse field] contorta ma torna: in pratica cambia lo spin al sito j for iHam in range(ell): for ii in range(NumTot): if iSpin[ii,iHam]==1: Exc = ii - 2**(iHam) else: Exc = ii + 2**(iHam) HamOut[int(Exc),ii] = HamOut[int(Exc),ii] + gg # Trovo l&#39;autostato del GS Egs[ggg,ell1]=ssl.eigsh(HamOut,1,which=&#39;SA&#39;)[0] Psi=ssl.eigsh(HamOut,1,which=&#39;SA&#39;)[1].T[0] Psii[ggg]=Psi.copy() # Calcolo la magnetizzazione MagnetZ=0 for ii in range(NumTot): Mag_ii = 0 for iSite in range(ell): if iSpin[ii,iSite] == 1: Mag_ii = Mag_ii + 1 else: Mag_ii = Mag_ii - 1 MagnetZ = MagnetZ + abs(Mag_ii)*(abs(Psi[ii])**2) magniz[ggg,ell1]=MagnetZ/ell xi[ggg,ell1]=-(magniz[ggg-1,ell1]-magniz[ggg,ell1])/(lins[ggg-1]-lins[ggg]) ci[ggg,ell1]=-(Egs[ggg-1,ell1]-Egs[ggg,ell1])/(lins[ggg-1]-lins[ggg]) ggg+=1 ell1+=1 magniz1=magniz Egs1=Egs xi1=xi ci1=ci lins1=lins linell1=linell . #collapse plt.figure(figsize=(12, 9)) plt.subplot(221) for i in linell1: plt.plot(lins1[1:],magniz1[1:,i-linell[0]],label=i) plt.legend() plt.ylim(0,1.1) plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Magnetization&#39;) plt.subplot(222) for i in linell1: plt.plot(lins1[1:],Egs1[1:volte,i-linell[0]],label=i) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Energy&#39;) plt.subplot(223) for i in linell1: plt.plot(lins1[1:],xi1[1:volte,i-linell[0]],label=i) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Susceptibility&#39;) plt.subplot(224) for i in linell1: plt.plot(lins1[1:],ci1[1:volte,i-linell[0]],label=i) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Specific heat capacity&#39;) plt.show() . . Come possiamo notare la suscettivit√† magnetica presenta un picco in corrispondenza di $g=1$ che all&#39;aumentare della taglia del sistema diventa sempre pi√π accentuato, ad indicare che nel limite termodinamico si osserva un punto critico ed il sistema presenta una transizione da una fase ferromagnetica ad una paramagnetica all&#39;aumentare di $g$, effetto previsto dalla soluzione analitica del sistema. . Per studiare meglio questo effetto procediamo con uno studio del Finite Size Scaling. . Fmagniz=np.zeros((volte,nell)) Fxi=np.zeros((volte+1,nell)) Flins=np.zeros((volte,nell)) ell1=0 for ell in linell: Fmagniz[:,ell1]=magniz1[:,ell1]*(ell)**(1/8) Fxi[:,ell1]=(xi1[:,ell1]/(ell))**(7/4) Flins[:,ell1]=(lins1-1)*(ell) ell1+=1 plt.figure(figsize=(12,5)) plt.subplot(121) plt.plot(Flins[1:],Fmagniz[1:]) plt.xlabel(&#39;(g-gùñº) L&#39;) plt.ylabel(&#39;|Mz| L·µù&#39;) plt.subplot(122) plt.plot(Flins[1:],Fxi[1:volte]) plt.xlabel(&#39;(g-gùñº) L&#39;) plt.ylabel(&#39;œá L·µû&#39;) plt.show() . Il Finite Size Scaling √® stato operato inserendo come esponenti critici quelli conosciuti tramite la risoluzione analitica del problema ovvero: $$ beta=1/8, quad gamma=7/4, quad nu=1, quad Y_g=1$$ . avendo considerato il caso con campo esterno longitudinale $h=0$. . Poich√® i grafici per le varie taglie del sistema coincidono quasi esattamente possiamo concludere che gli esponenti critici sono giusti e quindi che siamo in presenza di una transizione di fase. . Implementazione della matrice di adiacenza . Al fine di poter calcolare le osservabili termodinamiche di un sistema con connessioni random √® necessario implementare nell&#39;algoritmo un modo per rappresentare le connessioni attraverso una matrice di adiacenza. . Per farlo andremo ad aggiungere uno step alla costruzione dell&#39;Hamiltoniana in cui, data una matrice di adiacenza con alcuni elementi diversi da zero, andremo a controllare per ogni collegamento, se gli spin corrispondenti agli indici dell&#39;elemento diverso da zero sono concordi o discordi ed andremo ad aggiungere o togliere energia alla configurazione considerata. . La matrice di adiacenza essendo sparsa non verr√† considerata tutta, ma solo gli elementi diversi da zero, per velocizzare la compilazione. . Per verificare il giusto comportamento del nuovo codice √® stato testato usando come matrice di adiacenza proprio quella relativa ad un reticolo 1D intero con connessioni a primi vicini, per poi essere confrontato con i risultati precedenti che devono risultare uguali. . Introduciamo quindi una funzione che genera le matrici di adiacenza relative a reticoli d-dimensionali interi aventi L elementi per lato: . def AdD(d,L): # This function returns the sparse adjacency matrix of a d-dimensonal # integer lattice with side L and the total number of elements N=L**d # Only for Periodic Boundary Conditions import scipy.sparse as ss N=L**d A=ss.dok_matrix((N,N)) for i in range(N): for j in range(d): A[i,(i+L**j)%N] = 1 A[i,(i-L**j)%N] = 1 return A.copy().tocoo(), N . E testiamo quindi il funzionamento del codice aggiornato per implementare le matrici di adiacenza: . #collapse hh=0 volte=100 volte+=1 lins=np.linspace(0,2,volte) linell=np.arange(5,10) nell=linell.size magniz=np.zeros((volte,nell)) Egs=np.zeros((volte+1,nell)) xi=np.zeros((volte+1,nell)) ci=np.zeros((volte+1,nell)) jay=-1/2 ell1=0 for ell in linell: ggg=0 NumTot=2**ell ev=np.zeros((volte,NumTot)) Psii=np.zeros((volte,NumTot)) net=AdD(1,ell)[0] # Matrice di Adiacenza per un reticolo 1D coll=ss.find(net) for gg in lins: PBC=True # Costruisco la base iSpin=np.zeros((NumTot,ell)) for ii in range(NumTot): itemp=ii for jj in range(ell): iSpin[ii,jj]=np.floor(itemp%2) itemp=itemp/2 # Costruisco l&#39;Hamiltoniana HamOut=lil_matrix(np.zeros((NumTot,NumTot))) for iHam in range(len(coll[2])): for ii in range(NumTot): if iSpin[ii,coll[0][iHam]]==iSpin[ii,coll[1][iHam]]: HamOut[ii,ii]=HamOut[ii,ii] + jay else: HamOut[ii,ii]=HamOut[ii,ii] - jay # Sigma_x [transverse field] contorta ma torna: in pratica cambia lo spin al sito j for iHam in range(ell): for ii in range(NumTot): if iSpin[ii,iHam]==1: Exc = ii - 2**(iHam) else: Exc = ii + 2**(iHam) HamOut[int(Exc),ii] = HamOut[int(Exc),ii] + gg # Trovo l&#39;autostato del GS Egs[ggg,ell1]=ssl.eigsh(HamOut,1,which=&#39;SA&#39;)[0] Psi=ssl.eigsh(HamOut,1,which=&#39;SA&#39;)[1].T[0] Psii[ggg]=Psi.copy() # Calcolo la magnetizzazione MagX=np.zeros(ell) MagZ=np.zeros(ell) MagnetZ=0 for iSite in range(ell): Mx_sum=0 for ii in range(NumTot): if iSpin[ii,iSite] == 1: MagZ[iSite] = MagZ[iSite] + abs(Psi[ii])**2 Exc = ii -2**(iSite) else: MagZ[iSite] = MagZ[iSite] - abs(Psi[ii])**2 Exc = ii +2**(iSite) Mx_sum = Mx_sum + np.conjugate(Psi[ii]) * Psi[Exc] if abs(np.imag(Mx_sum))&gt;10**(-10): print(&quot;Non real magnetization&quot;) MagX[iSite] = np.real(Mx_sum) for ii in range(NumTot): Mag_ii = 0 for iSite in range(ell): if iSpin[ii,iSite] == 1: Mag_ii = Mag_ii + 1 else: Mag_ii = Mag_ii - 1 MagnetZ = MagnetZ + abs(Mag_ii)*(abs(Psi[ii])**2) magniz[ggg,ell1]=MagnetZ/ell xi[ggg,ell1]=-(magniz[ggg-1,ell1]-magniz[ggg,ell1])/(lins[ggg-1]-lins[ggg]) ci[ggg,ell1]=-(Egs[ggg-1,ell1]-Egs[ggg,ell1])/(lins[ggg-1]-lins[ggg]) ggg+=1 ell1+=1 magniz2=magniz Egs2=Egs xi2=xi ci2=ci lins2=lins linell2=linell plt.figure(figsize=(12, 9)) plt.subplot(221) for i in linell2: plt.plot(lins2[1:],magniz2[1:,i-linell[0]],label=i) plt.legend() plt.ylim(0,1.1) plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Magnetization&#39;) plt.subplot(222) for i in linell2: plt.plot(lins2[1:],Egs2[1:volte,i-linell[0]],label=i) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Energy&#39;) plt.subplot(223) for i in linell2: plt.plot(lins2[1:],xi2[1:volte,i-linell[0]],label=i) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Susceptibility&#39;) plt.subplot(224) for i in linell2: plt.plot(lins2[1:],ci2[1:volte,i-linell[0]],label=i) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Specific heat capacity&#39;) plt.show() . . I grafici rislutano identici a quelli precedenti ed insieme al Finite Size Scaling sottostante, usando gli stessi esponenti critici, ci confermano il corretto funzionamento del codice. . #collapse Fmagniz=np.zeros((volte,nell)) Fxi=np.zeros((volte+1,nell)) Flins=np.zeros((volte,nell)) ell1=0 for ell in linell: Fmagniz[:,ell1]=magniz2[:,ell1]*(ell)**(1/8) Fxi[:,ell1]=(xi2[:,ell1]/(ell))**(7/4) Flins[:,ell1]=-(lins-1)*(ell) ell1+=1 plt.figure(figsize=(12,5)) plt.subplot(121) plt.plot(Flins[1:],Fmagniz[1:]) plt.xlabel(&#39;(g-gùñº) L&#39;) plt.ylabel(&#39;|Mz| L·µù&#39;) plt.subplot(122) plt.plot(Flins[1:],Fxi[1:volte]) plt.xlabel(&#39;(g-gùñº) L&#39;) plt.ylabel(&#39;œá L·µû&#39;) plt.show() . . Modello di Ising 2D . Ora sfruttiamo questo stesso procedimento per studiare il modello di Ising quantistico 2D, usando la matrice di adiacenza opportuna, e confrontarlo con i risultati Monte-Carlo della relazione precedente per un Ising classico 3D, verificando il Quantum to Classical Mapping. . #collapse hh=0 volte=60 volte+=1 lins=np.linspace(0,6,volte) linell=np.arange(2,5) nell=linell.size magniz=np.zeros((volte,nell)) Egs=np.zeros((volte+1,nell)) xi=np.zeros((volte+1,nell)) ci=np.zeros((volte+1,nell)) jay=-1/2 ell1=0 for ell in linell: ggg=0 net=AdD(2,ell)[0] coll=ss.find(net) ell=ell*ell NumTot=2**ell ev=np.zeros((volte,NumTot)) Psii=np.zeros((volte,NumTot)) for gg in lins: PBC=True # Costruisco la base iSpin=np.zeros((NumTot,ell)) for ii in range(NumTot): itemp=ii for jj in range(ell): iSpin[ii,jj]=np.floor(itemp%2) itemp=itemp/2 # Costruisco l&#39;Hamiltoniana HamOut=lil_matrix(np.zeros((NumTot,NumTot))) for iHam in range(len(coll[2])): for ii in range(NumTot): if iSpin[ii,coll[0][iHam]]==iSpin[ii,coll[1][iHam]]: HamOut[ii,ii]=HamOut[ii,ii] + jay else: HamOut[ii,ii]=HamOut[ii,ii] - jay # Sigma_x [transverse field] contorta ma torna: in pratica cambia lo spin al sito j for iHam in range(ell): for ii in range(NumTot): if iSpin[ii,iHam]==1: Exc = ii - 2**(iHam) else: Exc = ii + 2**(iHam) HamOut[int(Exc),ii] = HamOut[int(Exc),ii] + gg # Trovo l&#39;autostato del GS Egs[ggg,ell1]=ssl.eigsh(HamOut,1,which=&#39;SA&#39;)[0] Psi=ssl.eigsh(HamOut,1,which=&#39;SA&#39;)[1].T[0] Psii[ggg]=Psi.copy() # Calcolo la magnetizzazione MagX=np.zeros(ell) MagZ=np.zeros(ell) MagnetZ=0 for iSite in range(ell): Mx_sum=0 for ii in range(NumTot): if iSpin[ii,iSite] == 1: MagZ[iSite] = MagZ[iSite] + abs(Psi[ii])**2 Exc = ii -2**(iSite) else: MagZ[iSite] = MagZ[iSite] - abs(Psi[ii])**2 Exc = ii +2**(iSite) Mx_sum = Mx_sum + np.conjugate(Psi[ii]) * Psi[Exc] if abs(np.imag(Mx_sum))&gt;10**(-10): print(&quot;Non real magnetization&quot;) MagX[iSite] = np.real(Mx_sum) for ii in range(NumTot): Mag_ii = 0 for iSite in range(ell): if iSpin[ii,iSite] == 1: Mag_ii = Mag_ii + 1 else: Mag_ii = Mag_ii - 1 MagnetZ = MagnetZ + abs(Mag_ii)*(abs(Psi[ii])**2) magniz[ggg,ell1]=MagnetZ/ell xi[ggg,ell1]=-(magniz[ggg-1,ell1]-magniz[ggg,ell1])/(lins[ggg-1]-lins[ggg]) ci[ggg,ell1]=-(Egs[ggg-1,ell1]-Egs[ggg,ell1])/(lins[ggg-1]-lins[ggg]) ggg+=1 ell1+=1 magniz3=magniz Egs3=Egs xi3=xi ci3=ci lins3=lins linell3=linell plt.figure(figsize=(12, 9)) plt.subplot(221) plt.plot(lins[1:],magniz3[1:]) plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Magnetization&#39;) plt.subplot(222) plt.plot(lins[1:],Egs3[1:volte]) #plt.ylim(-1,1) plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Energy&#39;) plt.subplot(223) plt.plot(lins[1:],xi3[1:volte]) plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Susceptibility&#39;) plt.subplot(224) plt.plot(lins[1:],ci3[1:volte]) plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Specific heat capacity&#39;) plt.show() . . Per verificare che il sistema studiato sopra abbia le giuste propriet√† di scaling, ovvero quelle di un sistema di Ising 3D classico, procediamo pure in questo caso con uno studio del Finite Size Scaling, utilizzando gli esponenti critici noti in letteratura calcolati in via approssimata, poich√® per questo tipo di modello non sono disponibili soluzioni analitiche: . $$ beta=0.326419, quad gamma=1.237075, quad nu=0.629971, quad Y_g=1.412625$$ . sempre considerando il caso con campo esterno longitudinale $h=0$. . #collapse Fmagniz=np.zeros((volte,nell)) Fxi=np.zeros((volte+1,nell)) Flins=np.zeros((volte,nell)) ell1=0 for ell in linell3: Fmagniz[:,ell1]=magniz3[:,ell1]*(ell)**(0.326419/0.629971) Fxi[:,ell1]=(xi3[:,ell1]/(ell))**(1.237075/0.629971) Flins[:,ell1]=(lins3-3.4)*(ell)**1.412625 ell1+=1 plt.figure(figsize=(12,5)) plt.subplot(121) plt.plot(Flins[1:],Fmagniz[1:]) plt.xlabel(&#39;(g-gùñº) L&#39;) plt.ylabel(&#39;|Mz| L·µù&#39;) plt.subplot(122) plt.plot(Flins[1:],Fxi[1:volte]) plt.xlabel(&#39;(g-gùñº) L&#39;) plt.ylabel(&#39;œá L·µû&#39;) plt.show() . . Gli esponenti critici utilizzati per effettuare il Finite Size Scaling sono quelli noti in letteratura, calcolati in via approssimata, poich√® per questo tipo di modello, come per il suo corrispettivo classico, L&#39;Ising 3D, non sono disponibili soluzioni analitiche: . ESPONENTI CRITICI avendo considerato il caso con campo esterno longitudinale $h=0$. . Studio di sistemi con connessioni random . Passiamo dunque ora allo studio di un sistema le cui connessioni tra elementi sono rappresentate da una matrice di adiacenza random, ovvero in cui gli elementi della matrice diversi da zero sono disposti in maniera casuale all&#39;interno della matrice stessa. La matrice di adiacenza √® data da una funzione delle libreria SciPy che prende in input le dimensioni della matrice e la densit√† $ rho$ di elementi diversi da zero che si vuole ottenere. Per esempio per un sistema di 10 elementi come quello seguente, prendendo in input i valori (10, 10, 0.2) la funzione rand() restituisce una matrice 10x10 con 20 elementi diversi da zero scelti a caso. . Poich√® per gli stessi valori di input la funzione restituisce realizzazioni diverse di sistemi con le stesse caratteristiche, per avere un comportamento generale verranno calcolate varie realizzazioni di sistemi con gli stessi parametri e i valori di output verranno mediati fra tutte le realizzazioni. Il grafico della media √® evidenziato in rosso. . #collapse hh=0 volte=60 volte+=1 lins=np.linspace(0,3,volte) linum=np.arange(10) nell=linum.size magniz=np.zeros((volte,nell)) Egs=np.zeros((volte+1,nell)) xi=np.zeros((volte+1,nell)) ci=np.zeros((volte+1,nell)) jay=-1/2 ell=8 ell1=0 for numer in linum: ggg=0 NumTot=2**ell ev=np.zeros((volte,NumTot)) Psii=np.zeros((volte,NumTot)) net=ss.rand(ell,ell,2/ell) # Matrice di Adiacenza Random coll=ss.find(net) for gg in lins: PBC=True # Costruisco la base iSpin=np.zeros((NumTot,ell)) for ii in range(NumTot): itemp=ii for jj in range(ell): iSpin[ii,jj]=np.floor(itemp%2) itemp=itemp/2 # Costruisco l&#39;Hamiltoniana HamOut=lil_matrix(np.zeros((NumTot,NumTot))) for iHam in range(len(coll[2])): for ii in range(NumTot): if iSpin[ii,coll[0][iHam]]==iSpin[ii,coll[1][iHam]]: HamOut[ii,ii]=HamOut[ii,ii] + jay else: HamOut[ii,ii]=HamOut[ii,ii] - jay # Sigma_x [transverse field] contorta ma torna: in pratica cambia lo spin al sito j for iHam in range(ell): for ii in range(NumTot): if iSpin[ii,iHam]==1: Exc = ii - 2**(iHam) else: Exc = ii + 2**(iHam) HamOut[int(Exc),ii] = HamOut[int(Exc),ii] + gg # Trovo l&#39;autostato del GS Egs[ggg,ell1]=ssl.eigsh(HamOut,1,which=&#39;SA&#39;)[0] Psi=ssl.eigsh(HamOut,1,which=&#39;SA&#39;)[1].T[0] Psii[ggg]=Psi.copy() # Calcolo la magnetizzazione MagnetZ=0 for ii in range(NumTot): Mag_ii = 0 for iSite in range(ell): if iSpin[ii,iSite] == 1: Mag_ii = Mag_ii + 1 else: Mag_ii = Mag_ii - 1 MagnetZ = MagnetZ + abs(Mag_ii)*(abs(Psi[ii])**2) magniz[ggg,ell1]=MagnetZ/ell xi[ggg,ell1]=-(magniz[ggg-1,ell1]-magniz[ggg,ell1])/(lins[ggg-1]-lins[ggg]) ci[ggg,ell1]=-(Egs[ggg-1,ell1]-Egs[ggg,ell1])/(lins[ggg-1]-lins[ggg]) ggg+=1 ell1+=1 magniz5=magniz Egs5=Egs xi5=xi ci5=ci plt.figure(figsize=(12, 9)) plt.subplot(221) plt.plot(lins[1:],magniz5[1:]) plt.plot(lins[1:],np.mean(magniz5[1:],axis=1),color=&#39;r&#39;, linewidth=3.0,Label=&quot;Media&quot;) plt.legend() plt.ylim(0,1.1) plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Magnetization&#39;) plt.subplot(222) plt.plot(lins[1:],Egs5[1:volte]) plt.plot(lins[1:],np.mean(Egs5[1:volte],axis=1),color=&#39;r&#39;, linewidth=3.0,Label=&quot;Media&quot;) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Energy&#39;) plt.subplot(223) plt.plot(lins[2:],xi5[2:volte]) plt.plot(lins[2:],np.mean(xi5[2:volte],axis=1),color=&#39;r&#39;, linewidth=3.0,Label=&quot;Media&quot;) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Susceptibility&#39;) plt.subplot(224) plt.plot(lins[1:],ci5[1:volte]) plt.plot(lins[2:],np.mean(ci5[2:volte],axis=1),color=&#39;r&#39;, linewidth=3.0,Label=&quot;Media&quot;) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Specific heat capacity&#39;) plt.show() . . Per tutte le realizzazioni si osserva un punto di transizione di fase per valori di $g$ intorno ad 1 con una media poco superiore ad 1. . Il sistema precedente aveva come densit√† di elemeti diversi da zero $ rho=0.2$. √à possibile che il valore del punto di transizione dipenda da questo valore? Per rispondere a questa domanda possiamo procedere con uno studio delle medie di varie realizzazioni con densit√† variabili da 0.1 a 1, poich√® il caso $ rho=0$ √® banale e corrisponde ad un sistema senza interazioni e quindi senza magnetizzazione. . #collapse ell=8 # hh=0 volte=60 volte+=1 nrip=4 # lins=np.linspace(0,10,volte) linum=np.arange(1,ell+1) nell=linum.size magnizR=np.zeros((volte,nell,nrip)) EgsR=np.zeros((volte+1,nell,nrip)) xiR=np.zeros((volte+1,nell,nrip)) ciR=np.zeros((volte+1,nell,nrip)) jay=-1/2 ell1=0 for numer in linum: NumTot=2**ell ev=np.zeros((volte,NumTot)) Psii=np.zeros((volte,NumTot)) net=ss.rand(ell,ell,numer/ell) # Matrice di Adiacenza Random coll=ss.find(net) for rip in range(nrip): ggg=0 for gg in lins: # Costruisco la base iSpin=np.zeros((NumTot,ell)) for ii in range(NumTot): itemp=ii for jj in range(ell): iSpin[ii,jj]=np.floor(itemp%2) itemp=itemp/2 # Costruisco l&#39;Hamiltoniana HamOut=lil_matrix(np.zeros((NumTot,NumTot))) for iHam in range(len(coll[2])): for ii in range(NumTot): if iSpin[ii,coll[0][iHam]]==iSpin[ii,coll[1][iHam]]: HamOut[ii,ii]=HamOut[ii,ii] + jay else: HamOut[ii,ii]=HamOut[ii,ii] - jay # Sigma_x [transverse field] contorta ma torna: in pratica cambia lo spin al sito j for iHam in range(ell): for ii in range(NumTot): if iSpin[ii,iHam]==1: Exc = ii - 2**(iHam) else: Exc = ii + 2**(iHam) HamOut[int(Exc),ii] = HamOut[int(Exc),ii] + gg # Trovo l&#39;autostato del GS EgsR[ggg,ell1,rip]=ssl.eigsh(HamOut,1,which=&#39;SA&#39;)[0] Psi=ssl.eigsh(HamOut,1,which=&#39;SA&#39;)[1].T[0] Psii[ggg]=Psi.copy() # Calcolo la magnetizzazione MagnetZ=0 for ii in range(NumTot): Mag_ii = 0 for iSite in range(ell): if iSpin[ii,iSite] == 1: Mag_ii = Mag_ii + 1 else: Mag_ii = Mag_ii - 1 MagnetZ = MagnetZ + abs(Mag_ii)*(abs(Psi[ii])**2) magnizR[ggg,ell1,rip]=MagnetZ/ell xiR[ggg,ell1,rip]=-(magnizR[ggg-1,ell1,rip]-magnizR[ggg,ell1,rip])/(lins[ggg-1]-lins[ggg]) ciR[ggg,ell1,rip]=-(EgsR[ggg-1,ell1,rip]-EgsR[ggg,ell1,rip])/(lins[ggg-1]-lins[ggg]) ggg+=1 magniz6=np.mean(magnizR,axis=2) Egs6=np.mean(EgsR,axis=2) xi6=np.mean(xiR,axis=2) ci6=np.mean(ciR,axis=2) ell1+=1 plt.figure(figsize=(12, 9)) plt.subplot(221) for i in linum: plt.plot(lins[1:],magniz6[1:,i-1],label=round(i/ell,3)) plt.legend() plt.ylim(0,1.1) plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Magnetization&#39;) plt.subplot(222) for i in linum: plt.plot(lins[1:],Egs6[1:volte,i-1],label=round(i/ell,3)) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Energy&#39;) plt.subplot(223) for i in linum: plt.plot(lins[2:],xi6[2:volte,i-1],label=round(i/ell,3)) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Susceptibility&#39;) plt.subplot(224) for i in linum: plt.plot(lins[2:],ci6[2:volte,i-1],label=round(i/ell,3)) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Specific heat capacity&#39;) plt.show() . . Come possiamo notare dai grafici, all&#39;aumentare della densit√† di elementi diversi da zero, e quindi del numero di connessioni a lungo raggio, il punto di transizione avviene per valori di $g$ sempre maggiori, risultato verosimile in quanto pi√π connessioni ferromagnetiche ci sono pi√π √® probabile che il sistema presenti una fase ferromagnetica pi√π stabile all&#39;aumentare di $g$, che tende a distruggere la magnetizzazione del sistema. . Si pu√≤ osservare che anche per $ rho = 1$ esiste un valore di $g$ per cui si ha una transizione di fase e sopra il quale si ha una fase paramagnetica. Ci√≤ √® in qualche maniera sorprendente perche per $ rho = 1$ si ha una matrice di adiacenza piena alla quale corrisponde un network con connessioni all-to-all, quindi ogni elemento del sistema tende ad allinearsi con ogni altro elemento del sistema, formando una fase ferromagnetica particolarmente forte, come si pu√≤ notare dal plateau iniziale nella magnetizzazione, ma comunque non &quot;indistruttibile&quot;. . Pu√≤ essere interessante notare come il punto di transizione di un network all-to-all dipenda dalla taglia del sistema. . #collapse hh=0 volte=60 volte+=1 nrip=1 # lins=np.linspace(0,10,volte) linell=np.arange(6,11) nell=linum.size magnizR=np.zeros((volte,nell,nrip)) EgsR=np.zeros((volte+1,nell,nrip)) xiR=np.zeros((volte+1,nell,nrip)) ciR=np.zeros((volte+1,nell,nrip)) jay=-1/2 ell1=0 for ell in linell: NumTot=2**ell ev=np.zeros((volte,NumTot)) Psii=np.zeros((volte,NumTot)) net=ss.rand(ell,ell,1) # Matrice di Adiacenza All-to-All coll=ss.find(net) for rip in range(nrip): ggg=0 for gg in lins: # Costruisco la base iSpin=np.zeros((NumTot,ell)) for ii in range(NumTot): itemp=ii for jj in range(ell): iSpin[ii,jj]=np.floor(itemp%2) itemp=itemp/2 # Costruisco l&#39;Hamiltoniana HamOut=lil_matrix(np.zeros((NumTot,NumTot))) for iHam in range(len(coll[2])): for ii in range(NumTot): if iSpin[ii,coll[0][iHam]]==iSpin[ii,coll[1][iHam]]: HamOut[ii,ii]=HamOut[ii,ii] + jay else: HamOut[ii,ii]=HamOut[ii,ii] - jay # Sigma_x [transverse field] contorta ma torna: in pratica cambia lo spin al sito j for iHam in range(ell): for ii in range(NumTot): if iSpin[ii,iHam]==1: Exc = ii - 2**(iHam) else: Exc = ii + 2**(iHam) HamOut[int(Exc),ii] = HamOut[int(Exc),ii] + gg # Trovo l&#39;autostato del GS EgsR[ggg,ell1,rip]=ssl.eigsh(HamOut,1,which=&#39;SA&#39;)[0] Psi=ssl.eigsh(HamOut,1,which=&#39;SA&#39;)[1].T[0] Psii[ggg]=Psi.copy() # Calcolo la magnetizzazione MagnetZ=0 for ii in range(NumTot): Mag_ii = 0 for iSite in range(ell): if iSpin[ii,iSite] == 1: Mag_ii = Mag_ii + 1 else: Mag_ii = Mag_ii - 1 MagnetZ = MagnetZ + abs(Mag_ii)*(abs(Psi[ii])**2) magnizR[ggg,ell1,rip]=MagnetZ/ell xiR[ggg,ell1,rip]=-(magnizR[ggg-1,ell1,rip]-magnizR[ggg,ell1,rip])/(lins[ggg-1]-lins[ggg]) ciR[ggg,ell1,rip]=-(EgsR[ggg-1,ell1,rip]-EgsR[ggg,ell1,rip])/(lins[ggg-1]-lins[ggg]) ggg+=1 magniz6=np.mean(magnizR,axis=2) Egs6=np.mean(EgsR,axis=2) xi6=np.mean(xiR,axis=2) ci6=np.mean(ciR,axis=2) ell1+=1 plt.figure(figsize=(12, 9)) plt.subplot(221) for i in linell: plt.plot(lins[1:],magniz6[1:,i-linell[0]],label=i) plt.legend() plt.ylim(0,1.1) plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Magnetization&#39;) plt.subplot(222) for i in linell: plt.plot(lins[1:],Egs6[1:volte,i-linell[0]],label=i) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Energy&#39;) plt.subplot(223) for i in linell: plt.plot(lins[2:],xi6[2:volte,i-linell[0]],label=i) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Susceptibility&#39;) plt.subplot(224) for i in linell: plt.plot(lins[2:],ci6[2:volte,i-linell[0]],label=i) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Specific heat capacity&#39;) plt.show() . . Possiamo notare dai grafici sopra che il valore di $g$ corrispondente al punto di critico aumenta con l&#39;aumentare della taglia. √à legittimo dunque pensare che nel limite termodinamico il valore di $g$ alla transizione di fase diverga e dunque che un sistema con connessioni ferromagnetiche all-to-all rimanga magnetizzazato per qualunque valore di $g$. .",
            "url": "https://edoarder.github.io/Metodi_Numerici/2020/06/07/Random_Network_Diagonalization-7.html",
            "relUrl": "/2020/06/07/Random_Network_Diagonalization-7.html",
            "date": " ‚Ä¢ Jun 7, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Modulo 5",
            "content": "A differenza dei precedenti moduli in questa relazione non mi occuper√≤ di studiare sistemi con interazioni a lunga distanza random poich√© il DMRG si basa fondamentalmente su interazioni a primi vicini. Sono possibili estensioni per considerare interazioni a lungo raggio, ma richiedono un notevole sforzo sia di implementazione che computazionale. Studier√≤ comunque il comportamento di una catena unidimensionale tipo Ising, ma considerando i valori delle costanti di accoppiamento dipendenti dalla posizione del sito rispetto alla catena, con valori random. . La Density Matrix Renormalization Group √® una tecnica numerica iterativa che consente di trovare il ground state, ed eventualmente pochi altri stati eccitati, di un sistema quantistico a bassa dimensionalit√† in una maniera estremamente efficiente. √à un metodo approssimato che si ispira alla rinormalizzazione numerica alla Wilson, ma il cui funzionamento si basa sull&#39;entanglement bipartito per il ground state di $ hat{H}$. Se l&#39;Hamiltoniana del sistema pu√≤ essere scritta come somma di termini locali riferiti ad un sito e ai suoi primi vicini e nel caso in cui il Ground State risulti non degenere, per il caso unidimensionale √® dimostrata la validit√† dell&#39;Area Law: considerando uno stato puro $| psi‚ü©_{AB}$ di un sistema quantistico bipartito AB, questa propriet√† esprime la dipendenza dell&#39;entropia di Von Neumann della partizione A dalla dimensione del confine tra A e B. $$ S( rho_A) sim dim(bound(A|B))$$ in cui $$ rho_A = Tr_B(| psi‚ü©_{AB}‚ü® psi|)$$ . Una generica Hamiltoniana che soddisfa queste condizioni pu√≤ essere scritta come: . $$ hat{H}= sum_{i=1}^L ( sum_ alpha J_i^{( alpha)} hat{S}_i^{( alpha)} hat{T}_{i+1}^{( alpha)} + sum_ beta B_i^{( beta)} hat{V}_i^{( beta)} ) $$ . dove gli $J_i^{( alpha)}$ e i $B_i^{( beta)}$ sono le costanti di accoppiamento mentre i $ { hat{S}_i^{( alpha)} }$, $ { hat{T}_{i+1}^{( alpha)} }$ e $ { hat{V}_i^{( beta)} }$ sono operatori agenti sul sito i-esimo, mentre $ alpha$ e $ beta$ sono le varie componenti degli operatori. . L&#39;Hamiltoniana del modello di Ising quantistico 1D rientra in questa forma: . $$ hat{H} = - sum_{i}^{L-1} J_{i} sigma_i^z sigma_{i+1}^z - sum_{i}^L g_i sigma_i^x - sum_{i}^L h_i sigma_i^z$$ . dove le varie $ sigma_i$ sono gli operatori di spin agenti sul sito i-esmi che, nella base canonica, sono rappresentati dalle matrici di Pauli. . L&#39;algoritmo DMRG Infinite-System si articola in alcuni step: . Si parte da un blocco $B(1,d)$, composto dal solo sito estremo di sinistra, di cui si definisce l&#39;Hamiltoniana $ hat{H}_B$, nel codice BlockH. Spazio di Hilbert di dimensione $d$. . | Si costruisce l&#39;Enlarged Block aggiungendo al blocco precedente il sito adiacente destro e si costruisce l&#39;hamiltoniana corrispondente $ hat{H}_E$: $$ hat{H}_E = hat{H}_B otimes mathbb{1}_{sito} + mathbb{1}_B otimes hat{H}_{sito} + hat{H}_{B-sito} $$ Spazio di Hilbert di dimensione $d^2$. . | Si costruisce il Super-block aggiundendo al blocco precedente un blocco speculare, considerando il fatto che il sistema in esame √® simmetrico per riflessione rispetto al centro della catena, il collegamento √® dato dall&#39;interazione dei due siti esterni aggiunti al passo precedente. L&#39;Hamiltoniana del Super-block diventa: $$ hat{H}_{SB} = hat{H}_E otimes mathbb{1}_{E&#39;} + mathbb{1}_E otimes hat{H}_{E&#39;} + hat{H}_{E-E&#39;} $$ La cui dimensione √® $d^4$. . | Si trova l&#39;autovalore minore ed il corrispondente autovettore di $ hat{H}_{SB}$, ovvero il Ground State $| psi_{gs}‚ü©$ e la sua energia $E_{gs}$. . | Si calcola la matrice di densit√† ridotta $ rho_L$ relativa al blocco di sinistra: $ rho_L=Tr_R | psi_{gs}‚ü©‚ü® psi_{gs}|$ che operativamente nel codice √® stato implementato calcolando $ rho_L= psi_{gs} psi_{gs}^ dagger$. . | Si diagonalizza $ rho_L$ ordinando gli autovalori $ lambda_i$ in senso decrescente e si ricava la rappresentazione della matrice densit√†: $$ rho_L = sum_{i=1}^{d^2} lambda_i |w_i‚ü©‚ü®w_i| $$ dove $|w_i‚ü©$ √® l&#39;autovettore corrispondente all&#39;autovalore $ lambda_i$. . | Di questi stati si tengono soltalto i primi $m$, corrispondenti agli autovalori $ lambda_i$ maggiori. $m$ √® scelto come il minimo tra la dimensione dello spazio di Hilbert del sistema e un valore fissato all&#39;inizio dell&#39;algoritmo, che rappresenta la dimensione massima dello spazio di Hilbert del sistema approssimato. Da questi $m$ autovettori si costruiscono due matrici $O$ e $O^ dagger$ diaponendoli rispettivamente per colonne e per righe. $O$ e $O^ dagger$ non sono matrici quadrate e servono appunto per troncare la matrice dell&#39;Hamiltoniana. . | L&#39;ultimo step √® infatti quello di calcolare la matrice hamiltoniana dell&#39;Enlarged Block nella base troncata attraverso: $$ tilde{H}_E=O^ dagger H_E O$$ Si rinormalizzano inoltre anche gli altri operatori che serviranno per il ciclo successivo. . | Nel ciclo successivo dell&#39;algoritmo sar√† dunque utilizzato l&#39;Enlarged Block rinormalizzato come nuovo blocco di partenza, chiamato $B(2,m)$, e per procedere con l&#39;algoritmo saranno utilizzati gli altri operatori rinormalizzati. . #collapse %matplotlib inline import matplotlib.pyplot as plt import numpy as np import scipy.sparse.linalg as ssl import scipy.sparse as ss . . Per tutti i modelli presentati si considereranno le condizioni al bordo aperte. . L&#39;osservabile di cui andremo a studiare l&#39;andamento per ogni tipo di modello √® l&#39;entropia di Von Neumann definita come: $$ S( rho) = -Tr[ rho log( rho)] = - sum_i lambda_i log( lambda_i)$$ . Infinite-System DMRG per il modello di Ising . Di seguito √® presentato il codice che implementa l&#39;Infinite-System DMRG per un modello di Ising quantistico 1D. Il codice restituisce i grafici dell&#39;andamento dell&#39;entropia al variare dell&#39;intensit√† del campo trasverso, parametrizzato da $g$ che varia da 0 a 2. Per ogni run dell&#39;algoritmo $J_i=J$ e $g_i=g$ sono costanti e $h_i=0$ $ forall i$. Poich√® il comportamente del sistema non dipende da entrambe le variabili diverse da zero, ma solo dal loro rapporto, consideriamo $J=1$. Ad ogni grafico corrisponde una diversa taglia del sistema descritta in legenda. . m=9 # Dimensione massima dello spazio di Hilbert del sistema approssimato NIter=200 # Numero di iterazioni dell&#39;algoritmo rep=50 # Numero di punti del grafico rep+=1 gmax=1.02 ling=np.linspace(0.94,gmax,rep) linell=2**np.arange(3,8) Evec=np.zeros(rep) Entropy=np.zeros(rep) graphs=np.zeros((rep,NIter)) # inizializzazione degli operatori I=np.eye(2,2) Sz=np.array([[1, 0],[0,-1]]) Sx=np.array([[0, 1],[1,0]]) ggg=0 for g in ling: #blocchi iniziali BlockSz = Sz BlockSx = Sx BlockI = I BlockH = g*Sx Energy = 0 for l in range(NIter): SystSize = 2*l + 4 # Get the 2m-dimensional operators for the block + site BlockH = np.kron(BlockH, I) + np.kron(BlockI, g*Sx) - np.kron(BlockSz, Sz) BlockSz = np.kron(BlockI, Sz) BlockSx = np.kron(BlockI, Sx) BlockI = np.kron(BlockI, I) # Matrice dell&#39;Hamiltoniana per il Super-Blocco H_super = np.kron(BlockH, BlockI) + np.kron(BlockI, BlockH) - np.kron(BlockSz, BlockSz) H_super = 0.5 * (H_super + H_super.T); # ensure H is symmetric # Diagonalizzazione dell&#39;Hamiltoniana LastEnergy = Energy Energy= ssl.eigsh(H_super,1,which=&#39;SA&#39;)[0] #[0] Psi = ssl.eigsh(H_super,1,which=&#39;SA&#39;)[1].T #[0] EnergyPerBond = (Energy - LastEnergy) / 2 Ener2 = Energy / SystSize # Sigma = Psi&#39; *kron(BlockSz,BlockSz) * Psi; % n.n. ZZ correlation function # Costruzione della matrice densit√† nr=Psi.size Dim = int(np.sqrt(nr)) PsiMatrix = np.reshape(Psi,(Dim,Dim)) Rho = PsiMatrix @ PsiMatrix.T # Diagonalizzazione della matrice densit√† D,V = np.linalg.eigh(Rho) D=D[::-1] # descending Index=np.arange(Dim) Index=Index[::-1] V=V[:,Index] # Construzione dell&#39;operatore di troncamento NKeep = min(D.size, m) Omatr = V[:,:NKeep] TruncationError = 1 - sum(D[:NKeep]) Ent=-sum(D[:NKeep]*np.log(D[:NKeep])) # Trasformazione degli operatori dei blocchi nella base troncata BlockH = Omatr.T @ BlockH @ Omatr BlockSz = Omatr.T @ BlockSz @ Omatr BlockSx = Omatr.T @ BlockSx @ Omatr BlockI = Omatr.T @ BlockI @ Omatr graphs[ggg,l]=Ent ell=SystSize Evec[ggg]=Energy Entropy[ggg]=Ent ggg+=1 for i in range(5,11): plt.plot(ling,graphs[:,2*i*10-1],label=2*i*10) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Entropy&#39;) plt.show() . Si pu√≤ notare come all&#39;aumentare della taglia il valore di $g$ corrispondente al picco dell&#39;entropia converga ad 1, come previsto teoricamente, ed il picco si alzi diventando sempre pi√π stretto, indicando nel limite termodinamico una transizione di fase. Per valori di $g$ maggiori di 1 l&#39;entropia si assesta ad un valore maggiore di zero, effetto delle condizioni al bordo aperte. . Interazioni a valori random . Di seguito √® riportato il grafico, ed il codice espandibile, di un modello di Ising 1D in cui i valori di $J_i$ non sono tutti uguali ad 1, ma sono estratti random da una distribuzione normale con media 1 e varianza 0.3 in questo caso. Ad ogni ciclo sono estratti due numeri random, dalla stessa distribuzione, uno per l&#39;interazione Blocco-Sito ed uno per l&#39;interazione tra due Enlarged Block. . #collapse m=8 NIter=200 rep=40 rep+=1 gmax=2 ling=np.linspace(0,gmax,rep) Evec=np.zeros(rep) Entropy=np.zeros(rep) ggg=0 # inizialize local ops I=np.eye(2,2) Sz=np.array([[1, 0],[0,-1]]) Sx=np.array([[0, 1],[1,0]]) # initial blocks for gm in ling: Ent=0 BlockSz = Sz BlockSx = Sx BlockI = I BlockH = gm*Sx Energy = 0 for l in range(NIter): SystSize = 2*l + 4 g=gm j1=np.random.default_rng().normal(1, .3) # Get the 2m-dimensional operators for the block + site BlockH = np.kron(BlockH, I) + np.kron(BlockI, g*Sx) - j1*np.kron(BlockSz, Sz) BlockSz = np.kron(BlockI, Sz) BlockSx = np.kron(BlockI, Sx) BlockI = np.kron(BlockI, I) # HAMILTONIAN MATRIX for superblock j2=np.random.default_rng().normal(1, .3) H_super = np.kron(BlockH, BlockI) + np.kron(BlockI, BlockH) - j2*np.kron(BlockSz, BlockSz) H_super = 0.5 * (H_super + H_super.T); # ensure H is symmetric # Diagonalizing the Hamiltonian LastEnergy = Energy Energy= ssl.eigsh(H_super,1,which=&#39;SA&#39;)[0] #[0] Psi = ssl.eigsh(H_super,1,which=&#39;SA&#39;)[1].T #[0] EnergyPerBond = (Energy - LastEnergy) / 2 Ener2 = Energy / SystSize # Sigma = Psi&#39; *kron(BlockSz,BlockSz) * Psi; % n.n. ZZ correlation function # Form the reduced density matrix nr=Psi.size Dim = int(np.sqrt(nr)) PsiMatrix = np.reshape(Psi,(Dim,Dim)) Rho = PsiMatrix @ PsiMatrix.T # Diagonalize the density matrix D,V = np.linalg.eigh(Rho) D=D[::-1] # descending Index=np.arange(Dim) Index=Index[::-1] V=V[:,Index] # Construct the truncation operator NKeep = min(D.size, m) Omatr = V[:,:NKeep] TruncationError = 1 - sum(D[:NKeep]) # Transform the block operators into the truncated basis BlockH = Omatr.T @ BlockH @ Omatr BlockSz = Omatr.T @ BlockSz @ Omatr BlockSx = Omatr.T @ BlockSx @ Omatr BlockI = Omatr.T @ BlockI @ Omatr Ent=-sum(D[:NKeep]*np.log(D[:NKeep])) #Evec[ggg]=Energy Entropy[ggg]=Ent ggg+=1 plt.plot(ling,Entropy) plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Entropy&#39;) plt.show() . . Si pu√≤ notare come l&#39;andamento del grafico non si discosti troppo da quello del caso precedente. Risulta infatti un esserci una prima zona, per bassi valori di $g$, in cui l&#39;entropia √® piccola, un rapido aumento ed un picco nei pressi di $g=1$, in seguito un decremento ed un assestamento sempre dovuto alle condizioni al bordo aperte. Risultano evidenti comunque molte fluttuazioni lungo il grafico, effetto dovuto appunto alle interazioni tra primi vicini a valori random. . Bisogna comunque sottolineare che per ogni valore di $g$ l&#39;entropia calcolata si riferisce ad una catena con valori $J_i$ indipendenti dalle altre catene a diversi di $g$, per ogni valore di $g$ √® necessario infatti calcolare una Hamiltoniana che ha $J_i$ completamente nuovi e questo giustifica in parte le grandi oscillazioni che si possono osservare. . Un altro motivo, che verr√† studiato meglio nel proseguo della relazione, consiste nel fatto che per una catena di Ising &quot;normale&quot; il punto di transizioni si ha, come detto in precedenza, per valori di $g$ e $J$ tali che $g/J=1$, quindi delle fluttuazioni su $J$ possono comportare grandi fluttuazioni sull&#39;entropia finale del sistema. . Fluttuazioni mediate . Al fine di diminuire le fluttuazioni presenti in ogni grafico in modo da ricavare dei comportamenti generali, sono state calcolate varie realizzazioni di catene di Ising con interazioni a valori random aventi la stessa media 1 e la stessa varianza 0.3, per poi calcolarne la media, evidenziata con un tratto nero pi√π spesso. . #collapse m=10 NIter=200 rep=30 rep+=1 gmax=1.5 ling=np.linspace(0.5,gmax,rep) nrip=8 Evec=np.zeros((rep,nrip)) Entropy=np.zeros((rep,nrip)) ggg=0 # inizialize local ops I=np.eye(2,2) Sz=np.array([[1, 0],[0,-1]]) Sx=np.array([[0, 1],[1,0]]) # initial blocks for gm in ling: for rip in range(nrip): BlockSz = Sz BlockSx = Sx BlockI = I BlockH = gm*Sx Energy = 0 for l in range(NIter): SystSize = 2*l + 4 g=gm j1=np.random.default_rng().normal(1, .3) # Get the 2m-dimensional operators for the block + site BlockH = np.kron(BlockH, I) + np.kron(BlockI, g*Sx) - j1*np.kron(BlockSz, Sz) BlockSz = np.kron(BlockI, Sz) BlockSx = np.kron(BlockI, Sx) BlockI = np.kron(BlockI, I) # HAMILTONIAN MATRIX for superblock j2=np.random.default_rng().normal(1, .3) H_super = np.kron(BlockH, BlockI) + np.kron(BlockI, BlockH) - j2*np.kron(BlockSz, BlockSz) H_super = 0.5 * (H_super + H_super.T); # ensure H is symmetric # Diagonalizing the Hamiltonian LastEnergy = Energy Energy= ssl.eigsh(H_super,1,which=&#39;SA&#39;)[0] #[0] Psi = ssl.eigsh(H_super,1,which=&#39;SA&#39;)[1].T #[0] EnergyPerBond = (Energy - LastEnergy) / 2 Ener2 = Energy / SystSize # Sigma = Psi&#39; *kron(BlockSz,BlockSz) * Psi; % n.n. ZZ correlation function # Form the reduced density matrix nr=Psi.size Dim = int(np.sqrt(nr)) PsiMatrix = np.reshape(Psi,(Dim,Dim)) Rho = PsiMatrix @ PsiMatrix.T # Diagonalize the density matrix D,V = np.linalg.eigh(Rho) D=D[::-1] # descending Index=np.arange(Dim) Index=Index[::-1] V=V[:,Index] # Construct the truncation operator NKeep = min(D.size, m) Omatr = V[:,:NKeep] TruncationError = 1 - sum(D[:NKeep]) # Transform the block operators into the truncated basis BlockH = Omatr.T @ BlockH @ Omatr BlockSz = Omatr.T @ BlockSz @ Omatr BlockSx = Omatr.T @ BlockSx @ Omatr BlockI = Omatr.T @ BlockI @ Omatr Ent=-sum(D[:NKeep]*np.log(D[:NKeep])) Evec[ggg,rip]=Energy Entropy[ggg,rip]=Ent ggg+=1 plt.plot(ling[1:],Entropy[1:]) plt.plot(ling[1:],np.sum(Entropy[1:],axis=1)/nrip,color=&#39;k&#39;, linewidth=2.0) plt.show() . . Nonostante la permanenza di fluttuazioni importanti del grafico possiamo notare come la forma generale resti sostanzialmente invariata, ma possiamo osservare uno spostamento del picco verso sinistra, effetto non dovuto alla statistica, in quanto altre simulazioni, come mostrato sotto, portano alle stesse conclusioni. . Comportamento per diverse varianze . √à possibile quindi pensare ad una correlazione tra la varianza della distribuzione da cui viengono estratti i $J_i$ e la posizione del picco dell&#39;entropia? . Per rispondere a questa domanda sono state calcolate le medie di molte realizzazioni di catene di Ising con interazioni a valori random, corrispondenti a diversi valori di varianze, che vanno da 0.1 ad 1. . Di seguito sono riportati i grafici, insieme al codice espandibile, dell&#39;andamento dell&#39;entropia in funzione del parametro $g$, per i diversi valori della varianza. . #collapse m=8 NIter=200 rep=40 rep+=1 gmax=1.5 Nmedie=4 ling=np.linspace(0.5,gmax,rep) linsig=np.linspace(0.1,1,Nmedie) nrip=20 #ling=ling[int(30*rep/100):] Evec=np.zeros((rep,nrip)) Entropy=np.zeros((rep,nrip)) MEnt=np.zeros((Nmedie,rep-1)) ggg=0 # inizialize local ops I=np.eye(2,2) Sz=np.array([[1, 0],[0,-1]]) Sx=np.array([[0, 1],[1,0]]) # initial blocks sss=0 for sig in linsig: ggg=0 for gm in ling: for rip in range(nrip): BlockSz = Sz BlockSx = Sx BlockI = I BlockH = gm*Sx Energy = 0 for l in range(NIter): SystSize = 2*l + 4 g=gm j1=np.random.default_rng().normal(1, sig) # Get the 2m-dimensional operators for the block + site BlockH = np.kron(BlockH, I) + np.kron(BlockI, g*Sx) - j1*np.kron(BlockSz, Sz) BlockSz = np.kron(BlockI, Sz) BlockSx = np.kron(BlockI, Sx) BlockI = np.kron(BlockI, I) # HAMILTONIAN MATRIX for superblock j2=np.random.default_rng().normal(1, sig) H_super = np.kron(BlockH, BlockI) + np.kron(BlockI, BlockH) - j2*np.kron(BlockSz, BlockSz) H_super = 0.5 * (H_super + H_super.T); # ensure H is symmetric # Diagonalizing the Hamiltonian LastEnergy = Energy Energy= ssl.eigsh(H_super,1,which=&#39;SA&#39;)[0] #[0] Psi = ssl.eigsh(H_super,1,which=&#39;SA&#39;)[1].T #[0] EnergyPerBond = (Energy - LastEnergy) / 2 Ener2 = Energy / SystSize # Sigma = Psi&#39; *kron(BlockSz,BlockSz) * Psi; % n.n. ZZ correlation function # Form the reduced density matrix nr=Psi.size Dim = int(np.sqrt(nr)) PsiMatrix = np.reshape(Psi,(Dim,Dim)) Rho = PsiMatrix @ PsiMatrix.T # Diagonalize the density matrix D,V = np.linalg.eigh(Rho) D=D[::-1] # descending Index=np.arange(Dim) Index=Index[::-1] V=V[:,Index] # Construct the truncation operator NKeep = min(D.size, m) Omatr = V[:,:NKeep] TruncationError = 1 - sum(D[:NKeep]) # Transform the block operators into the truncated basis BlockH = Omatr.T @ BlockH @ Omatr BlockSz = Omatr.T @ BlockSz @ Omatr BlockSx = Omatr.T @ BlockSx @ Omatr BlockI = Omatr.T @ BlockI @ Omatr #print(SystSize, Energy, EnergyPerBond, Ener2, TruncationError) Ent=-sum(D[:NKeep]*np.log(D[:NKeep])) #Evec[ggg,rip]=Energy Entropy[ggg,rip]=Ent ggg+=1 MEnt[sss]=np.sum(Entropy[1:],axis=1)/nrip sss+=1 #Spectrum=ssl.eigsh(H_super)[0] for i in range(4): plt.plot(ling[1:],MEnt[i],label=linsig[i]) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Entropy&#39;) plt.show() . . Possiamo notare come per valori della varianza maggiori di 0.1 il picco, oltre ad abbassarsi notevolmente, sembra spostarsi progressivamente pi√π a sinistra. . Per approfondire meglio questo effetto andremo a fare gli stessi passaggi fatti finora, ma considerando le fluttuazioni random sul campo esterno $g$. . Campi esterni a valori random . Di seguito √® riportato il grafico, ed il codice espandibile, di un modello di Ising 1D in cui i valori di $g_i$ non sono uguali per tutta la catena, ma vengono estratti da una distribuzione normale con media $g_m$ variabile e varianza, in questo caso, uguale a 0.4. . #collapse m=10 NIter=200 rep=20 rep+=1 gmax=2 ling=np.linspace(0,gmax,rep) Evec=np.zeros(rep) Entropy=np.zeros(rep) ggg=0 # inizialize local ops I=np.eye(2,2) Sz=np.array([[1, 0],[0,-1]]) Sx=np.array([[0, 1],[1,0]]) # initial blocks for gm in ling: Ent=0 BlockSz = Sz BlockSx = Sx BlockI = I BlockH = np.random.default_rng().normal(gm, .4)*Sx Energy = 0 for l in range(NIter): SystSize = 2*l + 4 g=np.random.default_rng().normal(gm, .4) # Get the 2m-dimensional operators for the block + site BlockH = np.kron(BlockH, I) + np.kron(BlockI, g*Sx) - np.kron(BlockSz, Sz) BlockSz = np.kron(BlockI, Sz) BlockSx = np.kron(BlockI, Sx) BlockI = np.kron(BlockI, I) # HAMILTONIAN MATRIX for superblock H_super = np.kron(BlockH, BlockI) + np.kron(BlockI, BlockH) - np.kron(BlockSz, BlockSz) H_super = 0.5 * (H_super + H_super.T); # ensure H is symmetric # Diagonalizing the Hamiltonian LastEnergy = Energy Energy= ssl.eigsh(H_super,1,which=&#39;SA&#39;)[0] #[0] Psi = ssl.eigsh(H_super,1,which=&#39;SA&#39;)[1].T #[0] EnergyPerBond = (Energy - LastEnergy) / 2 Ener2 = Energy / SystSize # Sigma = Psi&#39; *kron(BlockSz,BlockSz) * Psi; % n.n. ZZ correlation function # Form the reduced density matrix nr=Psi.size Dim = int(np.sqrt(nr)) PsiMatrix = np.reshape(Psi,(Dim,Dim)) Rho = PsiMatrix @ PsiMatrix.T # Diagonalize the density matrix D,V = np.linalg.eigh(Rho) D=D[::-1] # descending Index=np.arange(Dim) Index=Index[::-1] V=V[:,Index] # Construct the truncation operator NKeep = min(D.size, m) Omatr = V[:,:NKeep] TruncationError = 1 - sum(D[:NKeep]) # Transform the block operators into the truncated basis BlockH = Omatr.T @ BlockH @ Omatr BlockSz = Omatr.T @ BlockSz @ Omatr BlockSx = Omatr.T @ BlockSx @ Omatr BlockI = Omatr.T @ BlockI @ Omatr #print(SystSize, Energy, EnergyPerBond, Ener2, TruncationError) Ent=-sum(D[:NKeep]*np.log(D[:NKeep])) #Evec[ggg]=Energy Entropy[ggg]=Ent ggg+=1 plt.plot(ling,Entropy) plt.show() . . Oltre a presentare le stesse caratteristiche di prima presenta uno spostamento del picco verso destra, in maniera opposta rispetto a prima. . Fluttuazioni mediate . Di seguito sono state calcolate varie realizzazioni di catene di Ising con campi esterni a valori random, per poi calcolarne la media, in maniera da essere sicuri che questo effetto non sia solo statistico. . #collapse m=8 NIter=200 rep=30 rep+=1 gmax=2 ling=np.linspace(0,gmax,rep) nrip=6 #ling=ling[int(30*rep/100):] Evec=np.zeros((rep,nrip)) Entropy=np.zeros((rep,nrip)) ggg=0 # inizialize local ops I=np.eye(2,2) Sz=np.array([[1, 0],[0,-1]]) Sx=np.array([[0, 1],[1,0]]) # initial blocks for gm in ling: for rip in range(nrip): BlockSz = Sz BlockSx = Sx BlockI = I BlockH = np.random.default_rng().normal(gm, .5)*Sx Energy = 0 for l in range(NIter): SystSize = 2*l + 4 g=np.random.default_rng().normal(gm, .5) # Get the 2m-dimensional operators for the block + site BlockH = np.kron(BlockH, I) + np.kron(BlockI, g*Sx) - np.kron(BlockSz, Sz) BlockSz = np.kron(BlockI, Sz) BlockSx = np.kron(BlockI, Sx) BlockI = np.kron(BlockI, I) # HAMILTONIAN MATRIX for superblock H_super = np.kron(BlockH, BlockI) + np.kron(BlockI, BlockH) - np.kron(BlockSz, BlockSz) H_super = 0.5 * (H_super + H_super.T); # ensure H is symmetric # Diagonalizing the Hamiltonian LastEnergy = Energy Energy= ssl.eigsh(H_super,1,which=&#39;SA&#39;)[0] #[0] Psi = ssl.eigsh(H_super,1,which=&#39;SA&#39;)[1].T #[0] EnergyPerBond = (Energy - LastEnergy) / 2 Ener2 = Energy / SystSize # Sigma = Psi&#39; *kron(BlockSz,BlockSz) * Psi; % n.n. ZZ correlation function # Form the reduced density matrix nr=Psi.size Dim = int(np.sqrt(nr)) PsiMatrix = np.reshape(Psi,(Dim,Dim)) Rho = PsiMatrix @ PsiMatrix.T # Diagonalize the density matrix D,V = np.linalg.eigh(Rho) D=D[::-1] # descending Index=np.arange(Dim) Index=Index[::-1] V=V[:,Index] # Construct the truncation operator NKeep = min(D.size, m) Omatr = V[:,:NKeep] TruncationError = 1 - sum(D[:NKeep]) # Transform the block operators into the truncated basis BlockH = Omatr.T @ BlockH @ Omatr BlockSz = Omatr.T @ BlockSz @ Omatr BlockSx = Omatr.T @ BlockSx @ Omatr BlockI = Omatr.T @ BlockI @ Omatr #print(SystSize, Energy, EnergyPerBond, Ener2, TruncationError) Ent=-sum(D[:NKeep]*np.log(D[:NKeep])) Evec[ggg,rip]=Energy Entropy[ggg,rip]=Ent ggg+=1 #Spectrum=ssl.eigsh(H_super)[0] plt.plot(ling[1:],Entropy[1:]) plt.plot(ling[1:],np.sum(Entropy[1:],axis=1)/nrip,color=&#39;k&#39;, linewidth=2.0) plt.show() . . Anche in questo caso si pu√≤ osservare come lo spostamento verso destra non dipenda dalla statistica. . Comportamento per diverse varianze . Guardiamo ora il comportamento delle medie dei grafici dell&#39;entropia per alcuni valori di varianze diverse. . #collapse m=8 NIter=200 rep=30 rep+=1 gmax=2 Nmedie=5 ling=np.linspace(0.5,gmax,rep) linsig=np.linspace(0.1,1,Nmedie) nrip=8 #ling=ling[int(30*rep/100):] Evec=np.zeros((rep,nrip)) Entropy=np.zeros((rep,nrip)) MEnt=np.zeros((Nmedie,rep-1)) ggg=0 # inizialize local ops I=np.eye(2,2) Sz=np.array([[1, 0],[0,-1]]) Sx=np.array([[0, 1],[1,0]]) # initial blocks sss=0 for sig in linsig: ggg=0 for gm in ling: for rip in range(nrip): BlockSz = Sz BlockSx = Sx BlockI = I #BlockH = gm*Sx BlockH = np.random.default_rng().normal(gm, sig)*Sx Energy = 0 for l in range(NIter): SystSize = 2*l + 4 g=np.random.default_rng().normal(gm, sig) # Get the 2m-dimensional operators for the block + site BlockH = np.kron(BlockH, I) + np.kron(BlockI, g*Sx) - np.kron(BlockSz, Sz) BlockSz = np.kron(BlockI, Sz) BlockSx = np.kron(BlockI, Sx) BlockI = np.kron(BlockI, I) # HAMILTONIAN MATRIX for superblock H_super = np.kron(BlockH, BlockI) + np.kron(BlockI, BlockH) - np.kron(BlockSz, BlockSz) H_super = 0.5 * (H_super + H_super.T); # ensure H is symmetric # Diagonalizing the Hamiltonian LastEnergy = Energy Energy= ssl.eigsh(H_super,1,which=&#39;SA&#39;)[0] #[0] Psi = ssl.eigsh(H_super,1,which=&#39;SA&#39;)[1].T #[0] EnergyPerBond = (Energy - LastEnergy) / 2 Ener2 = Energy / SystSize # Sigma = Psi&#39; *kron(BlockSz,BlockSz) * Psi; % n.n. ZZ correlation function # Form the reduced density matrix nr=Psi.size Dim = int(np.sqrt(nr)) PsiMatrix = np.reshape(Psi,(Dim,Dim)) Rho = PsiMatrix @ PsiMatrix.T # Diagonalize the density matrix D,V = np.linalg.eigh(Rho) D=D[::-1] # descending Index=np.arange(Dim) Index=Index[::-1] V=V[:,Index] # Construct the truncation operator NKeep = min(D.size, m) Omatr = V[:,:NKeep] TruncationError = 1 - sum(D[:NKeep]) # Transform the block operators into the truncated basis BlockH = Omatr.T @ BlockH @ Omatr BlockSz = Omatr.T @ BlockSz @ Omatr BlockSx = Omatr.T @ BlockSx @ Omatr BlockI = Omatr.T @ BlockI @ Omatr #print(SystSize, Energy, EnergyPerBond, Ener2, TruncationError) Ent=-sum(D[:NKeep]*np.log(D[:NKeep])) #Evec[ggg,rip]=Energy Entropy[ggg,rip]=Ent ggg+=1 MEnt[sss]=np.sum(Entropy[1:],axis=1)/nrip sss+=1 #Spectrum=ssl.eigsh(H_super)[0] plt.plot(ling[1:],MEnt.T) #plt.legend() plt.show() . . Anche in questo caso si pu√≤ apprezzare un graduale allontanamento del picco verso destra, nel senso opposto che nel caso delle interazioni a primi vicini con valori random. . Una possibile spiegazione . Ricordandoci che il punto critico del modello di Ising &quot;normale&quot; si trova per valori di $g$ e $J$ tali che $g/J=1$ possiamo notare che i due parametri competono in maniera reciproca al punto di transizione, quindi √® legittimo pensare di aver &quot;sbagliato&quot; ad imporre un tipo di rumore gaussiano ad entrambi i parametri, mentre in uno dei due avremmo dovuto imporre un rumore preso dall&#39;inverso di una distribuzione gaussiana. . Per testare questa ipotesi andiamo a cambiare il parametro $J$ nel codice in $1/J$ che rimane comunque con la stessa media poich√® essa √® sempre stata fissata ad 1. . #collapse m=9 NIter=200 rep=100 rep+=1 gmax=1.5 ling=np.linspace(0.5,gmax,rep) nrip=20 Evec=np.zeros((rep,nrip)) Entropy=np.zeros((rep,nrip)) ggg=0 # inizialize local ops I=np.eye(2,2) Sz=np.array([[1, 0],[0,-1]]) Sx=np.array([[0, 1],[1,0]]) # initial blocks for gm in ling: for rip in range(nrip): BlockSz = Sz BlockSx = Sx BlockI = I BlockH = gm*Sx Energy = 0 for l in range(NIter): SystSize = 2*l + 4 g=gm j1=np.random.default_rng().normal(1, .4) # Get the 2m-dimensional operators for the block + site BlockH = np.kron(BlockH, I) + np.kron(BlockI, g*Sx) - np.kron(BlockSz, Sz)/j1 BlockSz = np.kron(BlockI, Sz) BlockSx = np.kron(BlockI, Sx) BlockI = np.kron(BlockI, I) # HAMILTONIAN MATRIX for superblock j2=np.random.default_rng().normal(1, .4) H_super = np.kron(BlockH, BlockI) + np.kron(BlockI, BlockH) - np.kron(BlockSz, BlockSz)/j2 H_super = 0.5 * (H_super + H_super.T); # ensure H is symmetric # Diagonalizing the Hamiltonian LastEnergy = Energy Energy= ssl.eigsh(H_super,1,which=&#39;SA&#39;)[0] #[0] Psi = ssl.eigsh(H_super,1,which=&#39;SA&#39;)[1].T #[0] EnergyPerBond = (Energy - LastEnergy) / 2 Ener2 = Energy / SystSize # Sigma = Psi&#39; *kron(BlockSz,BlockSz) * Psi; % n.n. ZZ correlation function # Form the reduced density matrix nr=Psi.size Dim = int(np.sqrt(nr)) PsiMatrix = np.reshape(Psi,(Dim,Dim)) Rho = PsiMatrix @ PsiMatrix.T # Diagonalize the density matrix D,V = np.linalg.eigh(Rho) D=D[::-1] # descending Index=np.arange(Dim) Index=Index[::-1] V=V[:,Index] # Construct the truncation operator NKeep = min(D.size, m) Omatr = V[:,:NKeep] TruncationError = 1 - sum(D[:NKeep]) # Transform the block operators into the truncated basis BlockH = Omatr.T @ BlockH @ Omatr BlockSz = Omatr.T @ BlockSz @ Omatr BlockSx = Omatr.T @ BlockSx @ Omatr BlockI = Omatr.T @ BlockI @ Omatr Ent=-sum(D[:NKeep]*np.log(D[:NKeep])) Evec[ggg,rip]=Energy Entropy[ggg,rip]=Ent ggg+=1 EntMed1=np.sum(Entropy[1:],axis=1)/nrip plt.plot(ling[1:],Entropy[1:]) plt.plot(ling[1:],EntMed1,color=&#39;k&#39;, linewidth=2.0) plt.show() . . A parte la grande fluttuazione dei singoli grafici, possiamo notare come in media effettivamente il picco si sia spostato verso valori di $g$ maggiori, contrariamente a prima e in maniera concorde con il caso del rumore su $g$. . Per controllare quest&#39;ultima affermazione andremo a fare lo stesso calcolo con gli stessi valori dell&#39;algoritmo e con la stessa varianza ma su $g$ come prima. . #collapse m=9 NIter=200 rep=100 rep+=1 gmax=1.5 ling=np.linspace(0.5,gmax,rep) nrip=20 #ling=ling[int(30*rep/100):] Evec=np.zeros((rep,nrip)) Entropy=np.zeros((rep,nrip)) ggg=0 # inizialize local ops I=np.eye(2,2) Sz=np.array([[1, 0],[0,-1]]) Sx=np.array([[0, 1],[1,0]]) # initial blocks for gm in ling: for rip in range(nrip): BlockSz = Sz BlockSx = Sx BlockI = I BlockH = np.random.default_rng().normal(gm, .4)*Sx Energy = 0 for l in range(NIter): SystSize = 2*l + 4 g=np.random.default_rng().normal(gm, .4) # Get the 2m-dimensional operators for the block + site BlockH = np.kron(BlockH, I) + np.kron(BlockI, g*Sx) - np.kron(BlockSz, Sz) BlockSz = np.kron(BlockI, Sz) BlockSx = np.kron(BlockI, Sx) BlockI = np.kron(BlockI, I) # HAMILTONIAN MATRIX for superblock H_super = np.kron(BlockH, BlockI) + np.kron(BlockI, BlockH) - np.kron(BlockSz, BlockSz) H_super = 0.5 * (H_super + H_super.T); # ensure H is symmetric # Diagonalizing the Hamiltonian LastEnergy = Energy Energy= ssl.eigsh(H_super,1,which=&#39;SA&#39;)[0] #[0] Psi = ssl.eigsh(H_super,1,which=&#39;SA&#39;)[1].T #[0] EnergyPerBond = (Energy - LastEnergy) / 2 Ener2 = Energy / SystSize # Sigma = Psi&#39; *kron(BlockSz,BlockSz) * Psi; % n.n. ZZ correlation function # Form the reduced density matrix nr=Psi.size Dim = int(np.sqrt(nr)) PsiMatrix = np.reshape(Psi,(Dim,Dim)) Rho = PsiMatrix @ PsiMatrix.T # Diagonalize the density matrix D,V = np.linalg.eigh(Rho) D=D[::-1] # descending Index=np.arange(Dim) Index=Index[::-1] V=V[:,Index] # Construct the truncation operator NKeep = min(D.size, m) Omatr = V[:,:NKeep] TruncationError = 1 - sum(D[:NKeep]) # Transform the block operators into the truncated basis BlockH = Omatr.T @ BlockH @ Omatr BlockSz = Omatr.T @ BlockSz @ Omatr BlockSx = Omatr.T @ BlockSx @ Omatr BlockI = Omatr.T @ BlockI @ Omatr #print(SystSize, Energy, EnergyPerBond, Ener2, TruncationError) Ent=-sum(D[:NKeep]*np.log(D[:NKeep])) Evec[ggg,rip]=Energy Entropy[ggg,rip]=Ent ggg+=1 EntMed2=np.sum(Entropy[1:],axis=1)/nrip plt.plot(ling[1:],Entropy[1:]) plt.plot(ling[1:],EntMed2,color=&#39;k&#39;, linewidth=2.0) plt.show() . . Anche in questo caso osserviamo lo stesso spostamento del picco verso destra anche se il valore di $g$ relativo al picco non √® lo stesso che nel caso precedente. Questo fatto potrebbe essere dovuto o alla statistica o, pi√π probabilmente, al fatto che per ritrovare la stessa distribuzione la varianza su $g$ non deve essere la stessa di quella su $J$ che poi deve essere invertita, ma comunque il comportamento generale sembra essere pi√π o meno quello, come dimostrato dalla differenza delle due medie precedenti che mostrano comunque scostamenti non eccessivi dallo zero. . #collapse plt.plot(ling[1:],EntMed1-EntMed2) plt.show() . .",
            "url": "https://edoarder.github.io/Metodi_Numerici/2020/06/06/DMRG-Random_Ising-8.html",
            "relUrl": "/2020/06/06/DMRG-Random_Ising-8.html",
            "date": " ‚Ä¢ Jun 6, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Io sono Edoardo Maggioni, studente del secondo anno di Fisica Teorica presso l‚ÄôUniversit√† di Pisa. . Sono nato a Siena, dove sono cresciuto e dove ho frequentato il corso di laurea triennale in Fisica e Tecnologie Avanzate, laureandomi con lode. . Al di fuori del mondo della fisica ho coltivato svariati hobby, mi piace appassionarmi per certi periodi ad argomenti particolari di mio interesse che vanno dall‚Äôinformatica al Judo, dall‚ÄôUltimate Frisbee all‚Äôarte. Il mio interesse principale √® la musica, sia ascoltarla che suonarla: riesco ad apprezzare un gran numero di generi musicali, specialmente contemporanei. Suono tastiera e chitarra, ho studiato armonia e qualcosa di composizione. Conosco le basi della produzione musicale analogica e digitale ed ogni tanto faccio qualche brano. Ho infine seguito e conseguito l‚Äôesame di Fisica Musicale per il quale ho sviluppato interamente un sintetizzatore con Matlab. . Nella mia vita ho viaggiato tanto, specialmente in Europa, ma punto a viaggiare ancora di pi√π in futuro. . Considero l‚Äôoriginalit√† uno dei tratti essenziali della vita in generale. .",
          "url": "https://edoarder.github.io/Metodi_Numerici/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://edoarder.github.io/Metodi_Numerici/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}