{
  
    
        "post0": {
            "title": "Modulo 1",
            "content": "#collapse %matplotlib inline import matplotlib.pyplot as plt import numpy as np import networkx as nx import scipy.sparse as ss from math import * rng = np.random.default_rng() . . In questa relazione mi occuper√≤ di studiare la termodinamica di un sistema tipo Ising, ma con interazioni a lunga distanza tra elementi casuali del network. Per farlo inizieremo prima dall&#39;implementazione di un modello di Ising 2D classico per poi generalizzarlo e applicarlo al caso di interesse. . L&#39;Hamiltoniana del modello di Ising 2D classico √®: . $$H=- sum_{&lt;i,j&gt;}J_{ij}s_i s_j- sum_j h_j s_j$$ . dove gli $s_i$ sono i valori che ogni elemento del sistema pu√≤ assumere e possono essere $ pm 1$, versione classica dello spin-$ frac{1}{2}$, $J_{ij}$ rappresenta la matrici degli accoppiamenti tra $s_i$ ed $s_j$ e $h_j$ √® il valore del campo magnetico esterno per ogni sito $j$. La prima somma √® estesa a tutti gli $i$ e $j$ primi vicini, mentre la seconda a tutti gli elementi. I segni nell&#39;Hamiltoniana sono convenzionali ed implicano che per valori di $J_{ij}$ positivi una coppia di elementi rende minima l&#39;energia quando i valori dei due spin sono concordi, per $J_{ij}=0$ i due elementi sono non interagenti, mentre per $J_{ij}$ negativi la configurazione che minimizza l&#39;energia √® quella con gli spin antiparalleli. Per questo primo esempio verr√† considerato un modello di Ising standard, per cui si considera $J_{ij}=J$ e $h_i=h$ uniformi, quindi uguali per ogni elemento o coppia di elementi. . Lo scopo delle nostre analisi sar√† quella di studiare la stabilit√† della fase ferromagnetica al variare dell&#39;inverso della temperatura $ beta$, quindi considereremo sempre $J$ positivo e per semplificare la trattazione di valore unitario. Studiando il sistema infatti √® stato dimostrato che √® presente una transizione di fase ad una temperatura critica $T_c$, a cui √® associata un $ beta_c$. Il parametro d&#39;ordine che individua la transizione √® la magnetizzazione $‚ü®M‚ü©$ definita come: $$‚ü®M‚ü©= sum_{i=1}^N s_i$$ o la densit√† di magnetizzazione $‚ü®m‚ü©= frac{1}{N}‚ü®M‚ü©= frac{1}{N} sum_{i=1}^N s_i$ . Infatti si pu√≤ dimostrare che passando da $ beta&lt; beta_c$ a $ beta&gt; beta_c$ il sistema passa da una configurazione disordinata di spin, per cui $‚ü®M‚ü©= 0$, ad una ordinata, per cui $‚ü®M‚ü© neq 0$. . Detta $ sigma$ una generica configurazione di spin, la probabilit√† che tale configurazione si verifichi per un certo valore di $ beta$ √® data dalla distribuzione di Boltzmann: $$P( sigma)= frac{1}{Z}e^{- beta H( sigma)}$$ dove $Z= int e^{- beta H( sigma)}$ √® detta funzione di partizione e costituisce la costante di normalizzazione. Nota $P( sigma)$ √® possibile calcolare, in principio, qualsiasi grandezza fisica maroscopica, come ad esempio l&#39;energia o la magnetizzazione media associate ad una certa configurazione come: $$‚ü®m‚ü©= sum_ alpha m( alpha)P( alpha) quad o quad ‚ü® epsilon‚ü©= sum_ alpha epsilon( alpha)P( alpha)$$ . Le grandezze termodinamiche calcolate nei passaggi seguenti sono: . Densit√† di energia media $‚ü® epsilon‚ü©=‚ü®E‚ü©/N $ | Densit√† del valore assoluto della magnetizzazione $‚ü®m‚ü©=‚ü®M‚ü©/N = ( sum_i s_i)/N $ | Calore specifico $c equiv frac{ partial ‚ü® epsilon‚ü©}{ partial T} propto N(‚ü® epsilon^2‚ü©-‚ü® epsilon‚ü©^2) $ | Suscettivit√† magnetica $ chi equiv frac{ partial ‚ü®m‚ü©}{ partial h} propto N(‚ü®m^2‚ü©-‚ü®m‚ü©^2) $ | . Metodo Monte-Carlo . Lo studio sar√† fatto attraverso un&#39;analisi Monte-Carlo: una famiglia di metodi computazionali basati sul campionamento random delle possibili realizzazioni del sistema, che sono pesate da una distribuzione di probabilit√† legata il modello trattato. Attraverso questo campionamento casuale si cerca di esplorare tutto lo spazio delle configurazioni per poi andare a calcolare le osservabili di interesse sul campione finale. L&#39;algoritmo Monte-Carlo utilizzato si basa su una catena di Markov, che si ottiene estraendo le varie configurazioni attraverso la distribuzione di Boltzmann. . Esistono vari algoritmi che soddisfano le condizioni richieste dai metodi Monte-Carlo, uno dei pi√π usati, che √® stato usato anche per lo studio dei sistemi in questa relazione, √® il cosiddetto algoritmo Metropolis. L&#39;implementazione richiederebbe l&#39;uso di un generatore di numeri random, ma nella pratica vengono utilizzate routine che producono numeri detti pseudo-random attraverso algoritmi che garantiscono, entro certi limiti, numeri scorrelati tra loro e una distribuzione uniforme. . Un esempio di generatore di numeri pseudo-random √® riportato nel codice sottostante, che rientra nella categoria dei cosiddetti generatori congruenti lineari, ovvero dove partendo da un certo valore $x_0$ la successione dei numeri pseudo casuali viene generata attraverso: $$X_{i+1}=mod(aX_i+c,m)$$ Per certi valori dei parametri √® dimostrato che questa successione si comporta piuttosto bene come generatore, in questo caso sono stati utilizzati: $$a=16807, quad c=0, quad m=2^{31}-1$$ . def rando(N,x0,ini,fin): x=np.zeros((N+2)) a=16807 c=0 e=31 m=2**e-1 x[0]=x0 # initial value for i in range(N+1): x[i+1]=(a*x[i]+c)%m return x[2:]/m*(fin-ini) + ini x=rando(1000000,101,0,1) plt.hist(x,100,rwidth=1) plt.show() . Come si pu√≤ notare dall&#39;istogramma precedente la distribuzione dei valori risulta piuttosto uniforme per un gran numero di estrazioni, condizione necessaria, ma comunque non sufficiente a dimostrare che si tratti di un buon generatore. . Nei codici seguenti per√≤ √® stato utilizzato un generatore di numeri pseudo-random presente all&#39;interno della libreria NumPy, poich√® risulta pi√π sicuro e pi√π facile da implementare. . Algoritmo Metropolis . Tra tutte le implementazioni che soddisfano alcune condizioni necessarie per il corretto funzionamento del Monte-Carlo, come quella di equilibrio, e altre condizioni utili seppur non necessarie, come il bilancio dettagliato, una classe di algoritmi tra i pi√π efficienti √® il Metropolis. Per descrivere il funzionamento del Metropolis √® necessario descrivere il processo stocastico che permette di passare da uno stato $a$ ad uno stato $b$ in un passo elementare della catena, ovvero bisogna definire la probabilit√† della transizione. Detta $p_i$ la probabilit√† di realizzazione della configurazione $i$, l&#39;algoritmo si pu√≤ articolare come segue: . A partire dallo stato di partenza $a$ si seleziona uno stato $ tilde{b}$ a partire da una probabilit√† di transizione di prova | Se risulta che $p_{ tilde{b}}&gt;p_a$ allora si accetta $ tilde{b}$ come stato successivo $b$ | Se invece $p_{ tilde{b}}&lt;p_a$ allora si accetta $b= tilde{b}$ con probabilit√† $p_{ tilde{b}}/p_a$, e $a=b$ altrimenti. | . Implementazione . Per l&#39;implementazione dell&#39;algoritmo sono state create alcune funzioni: . Una che definisce lo stato iniziale: dato il numero di siti fornisce uno stato &quot;caldo&quot; con gli spin scelti tutti casualmente o &quot;freddo&quot; con gli spin tutti orientati nella stessa direzione, o +1 o -1 | . #collapse def initialstate(N, random=True, cold=+1): if random: state = 2*rng.integers(2, size=(N,N))-1 elif cold== 1: state = np.int_(np.ones((N,N))) elif cold== -1: state = -np.int_(np.ones((N,N))) else: return print(&quot;Put cold = +1 or -1&quot;) return state . . Il passo del Metropolis √® implementato dalla funzione mcmove, che prende in input la configurazione precedente, il valore di $ beta$, ovvero l&#39;inverso della temperatura, e la matrice di adiacenza del sistema considerato. La funzione restituisce la configurazione scelta attraverso il Metropolis. | . #collapse def mcmove(config, beta): &quot;Monte Carlo move using Metropolis algorithm&quot; for i in range(N): for j in range(N): a = rng.integers(N) b = rng.integers(N) s = config[a, b] cf = config[(a+1)%N,b] + config[a,(b+1)%N] + config[(a-1)%N,b] + config[a,(b-1)%N] cost = 2*s*cf if cost &lt; 0: s = -s elif rng.random() &lt; np.exp(-cost*beta): s = -s config[a,b] = s return config . . Le seguenti funzioni calcolano rispettivamente l&#39;energia e la magnetizzazione della configurazione di spin in input. | . #collapse def calcEnergy(config): &quot;Energy of a given configuration&quot; energy = 0 for i in range(len(config)): for j in range(len(config)): S = config[i,j] h = config[(i+1)%N, j] + config[i,(j+1)%N] + config[(i-1)%N, j] + config[i,(j-1)%N] energy += -h*S return energy/4 def calcMag(config): &quot;Magnetization of a given configuration&quot; mag = np.sum(config) return mag . . La funzione che restituisce la suscettivit√† magnetica o il calore specifico. | . #collapse def Di(Vet): return (np.mean((Vet*nn)**2)-(np.mean(Vet*nn))**2)/nn . . Inoltre per ogni grandezza √® stato calcolato l&#39;errore per ogni punto. Infatti ogni punto di grafici rappresenter√† la media di un gran numero di realizzazioni ottenute con gli stessi parametri. A questa distribuzione √® associata anche una varianza che √® stata calcolata, per le prime due grandezze, come: $$ sigma^2= frac{1}{N} frac{1}{N-1} sum_{j=1}^N (x_j- bar{x})^2$$ . Questa grandezza √® un buon stimatore della varianza per campioni statistici scorrelati, ma pu√≤ sottostimare la varianza effettiva se ci sono correlazioni tra i vari punti. L&#39;algoritmo sottostante √® stato implementato considerando un numero di passi di decorrelazione tra una misurazione e un&#39;altra, in modo che ogni passo Metropolis sia scorrelato dal precedente. Questa precauzione fa s√¨ che la stima della varianza come sopra sia corretta per le prime due grandezze, ma potrebbe non essere altrettanto buona per le ultime due, per le quali si √® utilizzato il metodo del Bootstrap per campioni correlati. . Il metodo consiste nel rimescolare il campione, attraverso dei numeri pseudo-random, e per ogni &quot;nuovo&quot; campione calcolarne la media. Alla fine la varianza delle medie dei nuovi campioni approssima meglio il valore della varianza del campione iniziale, ma per campioni correlati bisogna aggiungere l&#39;accortezza di rimescolare il campione iniziale in blocchi, invece che per singoli elementi, in modo da considerare anche eventuali correlazioni esistenti. . La funzione che restituisce la varianza relativa alle grandezze calcolate con la funzione precedente, calcolata attraverso il metodo del Bootstrap. | . #collapse def boots(v,M,MM): N=len(v) y=np.zeros((M,N)) for i in range(M): nr=rng.integers((N-MM), size=N//MM) for j in range(0,N//MM): y[i,j:j+MM]=v[int(nr[j]):int(nr[j])+MM] FF=np.zeros(M) for i in range(M): FF[i]=Di(y[i])/MM return (np.sum((FF-np.mean(FF))**2)/(M*(M-1))) . . Modello di Ising 2D classico . A questo punto √® possibile scrivere il codice che implementa l&#39;algoritmo Metropolis e calcola le variabili termodinamiche di interesse. Nell&#39;esempio seguente √® stata eseguita una simulazione per un modello di Ising 2D classico. . Per la seguente simulazione i parametri del Metropolis sono: . Reticolo di $15 times 15$ elementi | 50 punti relativi a valori di $ beta$ compresi tra 0 e 1 | 400 passi Metropolis per ogni $ beta$ | 40 passi di decorrelazione per ogni passo Metropolis | 100 ricampionamenti per il Bootstrap | 50 elementi per blocco nel Bootstrap | . #collapse nB = 50 # number of points of beta decSteps= 40 # number of MC sweeps for decorrelation mcSteps = 400 # number of MC sweeps for calculation beta = np.linspace(0, 1, nB) N=15 E,M,C,X,varX,varC,varE,varM = np.zeros((nB)), np.zeros((nB)), np.zeros((nB)), np.zeros((nB)), np.zeros((nB)), np.zeros((nB)), np.zeros((nB)), np.zeros((nB)) mn1, mm, nn = 1.0/(mcSteps*N*N), 1.0/(mcSteps*(mcSteps-1)) , 1/(N*N) config = initialstate(N) for bb in range(1,nB): Ene=np.zeros(mcSteps) Mag=np.zeros(mcSteps) iB=beta[bb]; iB2=iB*iB; for i in range(mcSteps): for j in range(decSteps): # steps of decorrelation mcmove(config, iB) # Metropolis step Ene[i] = calcEnergy(config) # calculate the energy Mag[i] = calcMag(config) # calculate the magnetization E[bb] = np.sum(Ene)*mn1 varE[bb] = np.sum((Ene*nn-E[bb])**2)*mm M[bb] = np.sum(Mag)*mn1 varM[bb] = np.sum((Mag*nn-M[bb])**2)*mm C[bb] = Di(Ene) varC[bb]=boots(Ene,100,50) X[bb] = Di(Mag) varX[bb]=boots(Mag,100,50) . . #collapse beta1=beta[1:] E1=E[1:] errE1=np.sqrt(varE[1:]) M1=M[1:] errM1=np.sqrt(varM[1:]) C1=C[1:] errC1=np.sqrt(varC[1:]) X1=X[1:] errX1=np.sqrt(varX[1:]) f = plt.figure(figsize=(18, 10)); # plot the calculated values sp = f.add_subplot(2, 2, 1 ) plt.plot(beta1, E1, marker=&#39;o&#39;,label=&#39;e&#39;) plt.legend() plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Energy &quot;, fontsize=16) plt.twinx() plt.plot(beta1,errE1,color=&quot;r&quot;, marker=&#39;x&#39;,label=&#39;Errore&#39;) plt.legend() sp = f.add_subplot(2, 2, 2 ) plt.plot(beta1, abs(M1), marker=&#39;o&#39;,label=&#39;|m|&#39;) plt.legend() plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Magnetization &quot;, fontsize=16) plt.twinx() plt.plot(beta1,errM1,color=&quot;r&quot;, marker=&#39;x&#39;,label=&#39;Errore&#39;) plt.legend() sp = f.add_subplot(2, 2, 3 ) plt.plot(beta1, C1, marker=&#39;o&#39;,label=&#39;c&#39;) plt.legend() plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Specific Heat &quot;, fontsize=16) plt.twinx() plt.plot(beta1,errC1,color=&quot;r&quot;, marker=&#39;x&#39;,label=&#39;Errore con Bootstrap&#39;) plt.legend() sp = f.add_subplot(2, 2, 4 ) plt.plot(beta1, X1, marker=&#39;o&#39;,label=&quot;X&quot;) plt.legend() plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Susceptibility&quot;, fontsize=16) plt.twinx() plt.plot(beta1,errX1,color=&quot;r&quot;, marker=&#39;x&#39;,label=&#39;Errore con Bootstrap&#39;) plt.legend() plt.show() . . Nei grafici sopra i punti in blu rappresentano le medie delle grandezze specificate a sinistra di ogni grafico per ogni valore di $ beta$, mentre in rosso sono gli errori associati al valore della funzione corrispondente allo stesso $ beta$, per i grafici in alto l&#39;errore √® calcolato come la radice quadrata della varianza standard, mentre per i due in basso come la radice della varianza calcolata con il metodo del Bootstrap. La costante $h$ che parametrizza il campo magnetico esterno agente sul sistema, in questa simulazione √® stata considerata nulla. . Come possiamo notare dai grafici, risulta evidente la presenza di un punto di transizione dalla fase paramagnetica a quella ferromagnetica al crescere di $ beta$, infatti il valore assoluto della magnetizzazione presenta una netta crescita per valori di $ beta$ intorno a 0.44 come previsto analiticamente e la suscettivit√† magnetica associata presenta un picco nello stesso range di valori. . Al punto critico di un sistema fisico in teoria il picco della suscettivit√† dovrebbe divergere, questo in un sistema simulato non pu√≤ succedere in quanto si tratta comunque di un sistema di taglia finita in cui la lunghezza di correlazione non pu√≤ diventare infinita, per cui si parla di $ beta$ semicritico per tagli finite, ma se ne pu√≤ studiare l&#39;andamento, come vedremo in seguito. . La transizione di fase si pu√≤ apprezzare anche osservando l&#39;andamento delle varianze associate ad ogni osservabile. Esse infatti presentano tutte un netto picco per valori corrispondenti a quelli del punto critico, sia che sia calcolata nel modo standard, come nel caso dell&#39;energia e della magnetizzazione, sia che sia calcolata attraverso Bootstrap. . Per assicurarci per√≤ che si tratti di una vera transizione di fase procediamo con l&#39;anailsi attraverso il Finite Size Scaling, che, se dovesse far combaciare i grafici relativi a sistemi di taglia diversa, ci assicurerebbe allo stesso tempo due importanti propriet√†: la prima che la lunghezza di correlazione diverge, effetto caratteristico della transizione di fase, la seconda che gli esponenti critici utilizzati sono giusti e quindi il sistema appartiene alla giusta classe di universalit√†. . Gli esponenti critici calcolati analiticamente relativi al modello di Ising 2D classico sono: . $$ beta=1/8, quad gamma=7/4, quad nu=1, quad alpha=0$$ . La simulazione seguente √® stata eseguita con gli stessi parametri per tutte le varie taglie (10, 15 ,20) del sistema e i risultati sono riportati senza errori per semplicit√† e per chiarezza dei grafici. . #collapse nB = 40 # number of points of beta decSteps= 30 # number of MC sweeps for decorrelation mcSteps = 300 # number of MC sweeps for calculation beta = np.linspace(0.35, 0.6, nB) linell = np.array([10,15,20]) Nell=linell.size E,M,C,X = np.zeros((nB,Nell)), np.zeros((nB,Nell)), np.zeros((nB,Nell)), np.zeros((nB,Nell)) for Ni in range(Nell): N=linell[Ni] # DIMENSIONI VARIABILI mn1, mm, nn = 1.0/(mcSteps*N*N), 1.0/(mcSteps*(mcSteps-1)) , 1/(N*N) config = initialstate(N) for bb in range(1,nB): Ene=np.zeros(mcSteps) Mag=np.zeros(mcSteps) iB=beta[bb]; iB2=iB*iB; for i in range(mcSteps): for j in range(decSteps): # steps of decorrelation mcmove(config, iB) # Metropolis step Ene[i] = calcEnergy(config) # calculate the energy Mag[i] = calcMag(config) # calculate the magnetization E[bb,Ni] = np.sum(Ene)*mn1 M[bb,Ni] = np.sum(Mag)*mn1 C[bb,Ni] = Di(Ene) X[bb,Ni] = Di(Mag) beta2=beta[1:] E2=E[1:] M2=M[1:] C2=C[1:] X2=X[1:] . . #collapse f = plt.figure(figsize=(18, 10)) # plot the calculated values sp = f.add_subplot(2, 2, 1 ) for i in range(Nell): plt.plot(beta2[1:], E2[1:,i], marker=&#39;o&#39;,label=linell[i]) plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Energy &quot;, fontsize=16) plt.legend() sp = f.add_subplot(2, 2, 2 ) for i in range(Nell): plt.plot(beta2[1:], abs(M2[1:,i]), marker=&#39;o&#39;,label=linell[i]) plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Magnetization &quot;, fontsize=16) plt.legend() sp = f.add_subplot(2, 2, 3 ) for i in range(Nell): plt.plot(beta2[1:], C2[1:,i], marker=&#39;o&#39;,label=linell[i]) plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Specific Heat &quot;, fontsize=16) plt.legend() sp = f.add_subplot(2, 2, 4 ) for i in range(Nell): plt.plot(beta2[1:], X2[1:,i], marker=&#39;o&#39;,label=linell[i]) plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Susceptibility&quot;, fontsize=16) plt.legend() plt.show() . . I grafici sopra rappresentano l&#39;andamento delle grandezze specificate per valori di $ beta$ che vanno da 0.35 a 0.5, ovvero in uno stretto intorno del punto critico noto analiticamente $ beta_c simeq 0.44$ ed ogni colore corrisponde ad una diversa taglia del sistema, riportato in legenda. Essi purtroppo sono affetti da importanti fluttuazioni statistiche dovute principalmente a due fattori: . in prossimit√† del punto critico si verifica non solo una divergenza nella lunghezza di correlazione, ma anche nei tempi di correlazione e per avere dei punti pi√π puliti nell&#39;intorno di $ beta_c$ andrebbe aumentato di molto il numero di passi di decorrelazione | l&#39;implementazione dell&#39;algoritmo complessivo √® risultato computazionalmente molto pesante, anche purtroppo perch√® scritto in Python, e non √® stato possibile aggiungere altre misurazioni e passi di decorrelazione | . L&#39;andamento complessivo dei grafici si riesce comunque a distinguere, come il picco nel calore specifico, ma le differenze tra le diverse taglie del sistema sono difficili da notare, seppur presenti. . Considerate queste difficiolt√† procediamo comunque con lo studio del Finite Size Scaling: . #collapse Fmagniz=np.zeros((nB,Nell)) Fxi=np.zeros((nB,Nell)) Flins=np.zeros((nB,Nell)) FC=np.zeros((nB,Nell)) ell1=0 for ell in linell: Fmagniz[1:,ell1]=abs(M2)[:,ell1].copy()*(ell)**(1/8) Fxi[1:,ell1]=(X2[:,ell1].copy()/(ell))**(7/4) FC[1:,ell1]=C2[:,ell1].copy() Flins[1:,ell1]=(beta2.copy()-0.4406868)*(ell) ell1+=1 plt.figure(figsize=(18, 10)) plt.subplot(221) plt.plot(beta2-0.4406868,E[1:],marker=&quot;o&quot;) plt.xlabel(&#39;(b-bùñº)&#39;) plt.ylabel(&#39;E&#39;) plt.subplot(222) plt.plot(Flins[1:],Fmagniz[1:],marker=&quot;o&quot;) plt.xlabel(&#39;(b-bùñº) L&#39;) plt.ylabel(&#39;|M| L·µù&#39;) plt.subplot(223) plt.plot(Flins[1:],FC[1:],marker=&quot;o&quot;) plt.xlabel(&#39;(b-bùñº) L&#39;) plt.ylabel(&#39;c L·µû&#39;) plt.subplot(224) plt.plot(Flins[1:],Fxi[1:],marker=&quot;o&quot;) plt.xlabel(&#39;(b-bùñº) L&#39;) plt.ylabel(&#39;œá L·µû&#39;) plt.show() . . Nonostante le difficolt√† descritte sopra si pu√≤ comunque notare che le funzioni tendono a combaciare per ogni taglia del sistema, dopo aver applicato il giusto Finite Size Scaling, prova del fatto che siamo in presenza di un effettivo punto di transizione e che il nostro modello rientra nella giusta classe di universalit√†. . Implementazione della matrice di adiacenza . Nei prossimi passaggi verr√† implementata la matrice di adiacenza che descrive le interazioni tra i vari elementi del sistema. Per definire la matrice di adiacenza √® necessario considerare gli elementi del sistema come un vettore e ad ogni interazione tra due elementi del sistema far corrispondere un elemento della matrice di adiacenza diverso da zero. In pratica la topologia del sistema diventa non pi√π fondamentale e si pu√≤ considerare giacente su una variet√† unidimensionale. Quello che diventa fondamentale sono le interazioni tra elementi, rappresentate dalla matrice di adiacenza, che possono essere pure a lunga distanza. . Per andare a controllare il corretto funzionamento dell&#39;implementazione procediamo con lo studio di un sistema con matrice di adiacenza corrispondente a quella di un reticolo bidimensionale, andando poi a confrontare i risultati con quelli ottenuti in precedenza. Si definisce quindi un ordinamento dei singoli elementi per passare da una matrice $L times L$ ad un vettore di dimensione $N=L^2$, per un reticolo bidimensionale, mentre $N=L^d$ per un reticolo intero generico d-dimensionale. Su questa premessa si basa la seguente funzione che prende in input la dimensione del reticolo e il numero di elementi per lato e restituisce la matrice di adiacenza $N times N$ relativa al reticolo. . #collapse def AdD(d,L): # This function returns the sparse adjacency matrix of a d-dimensonal # integer lattice with side L and the total number of elements N=L**d # Only for Periodic Boundary Conditions import scipy.sparse as ss N=L**d A=ss.dok_matrix((N,N)) for i in range(N): for j in range(d): A[i,(i+L**j)%N] = 1 A[i,(i-L**j)%N] = 1 return A.copy(), N . . Conseguentemente devono essere modificate alcune funzioni usate in precedenza: . Quella che definisce stato iniziale: | . #collapse def initialstate2(N, random=True, cold=+1): if random: state = 2*rng.integers(2, size=(N))-1 elif cold== 1: state = np.int_(np.ones((N))) elif cold== -1: state = -np.int_(np.ones((N))) else: return print(&quot;Put cold = +1 or -1&quot;) return state . . La funzione che implememta il passo del Metropolis: | . #collapse def mcmove2(config, beta, A): &quot;Monte Carlo move using Metropolis algorithm&quot; for i in range(N): a = rng.integers(N) s = config[a] cost=sum(-(s*A[a])*config)-sum((s*A[a])*config) if -cost &lt; 0: s = -s elif rng.random() &lt; np.exp(cost*beta): s = -s config[a] = s return config . . E il calcolo dell&#39;energia di una configurazione: | . #collapse def calcEnergy2(config): &quot;Energy of a given configuration&quot; energy = 0 for i in range(len(config)): S = config[i] energy += sum(-S*A[i]*config)#/min(A[i].count_nonzero(),1) return energy def calcMag(config): &quot;Magnetization of a given configuration&quot; mag = np.sum(config) return mag . . A questo punto √® possibile procedere con il Monte-Carlo per studiare le osservabili del sistema e gli errori ad esse associate: . #collapse nB = 50 # number of points of beta decSteps= 20 # number of MC sweeps for decorrelation mcSteps = 200 # number of MC sweeps for calculation beta = np.linspace(0, 1, nB) L=9 N=AdD(2,L)[1] A=AdD(2,L)[0] E,M,C,X,varX,varC,varE,varM = np.zeros((nB)), np.zeros((nB)), np.zeros((nB)), np.zeros((nB)), np.zeros((nB)), np.zeros((nB)), np.zeros((nB)), np.zeros((nB)) mn1, mm, nn = 1.0/(mcSteps*N), 1.0/(mcSteps*(mcSteps-1)) , 1/(N) config = initialstate2(N) for bb in range(1,nB): Ene=np.zeros(mcSteps) Mag=np.zeros(mcSteps) iB=beta[bb]; iB2=iB*iB; for i in range(mcSteps): for j in range(decSteps): # steps of decorrelation mcmove2(config, iB, A) # Metropolis step Ene[i] = calcEnergy2(config) # calculate the energy Mag[i] = calcMag(config) # calculate the magnetization E[bb] = np.sum(Ene)*mn1 varE[bb] = np.sum((Ene*nn-E[bb])**2)*mm M[bb] = np.sum(Mag)*mn1 varM[bb] = np.sum((Mag*nn-M[bb])**2)*mm C[bb] = Di(Ene) varC[bb]=boots(Ene,25,5) X[bb] = Di(Mag) varX[bb]=boots(Mag,25,5) . . #collapse beta3=beta[1:] E3=E[1:] errE3=np.sqrt(varE[1:]) M3=M[1:] errM3=np.sqrt(varM[1:]) C3=C[1:] errC3=np.sqrt(varC[1:]) X3=X[1:] errX3=np.sqrt(varX[1:]) f = plt.figure(figsize=(18, 10)); # plot the calculated values sp = f.add_subplot(2, 2, 1 ) plt.plot(beta3, E3, marker=&#39;o&#39;,label=&#39;e&#39;) plt.legend() plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Energy &quot;, fontsize=16) plt.twinx() plt.plot(beta3,errE3,color=&quot;r&quot;, marker=&#39;x&#39;,label=&#39;Errore&#39;) plt.legend() sp = f.add_subplot(2, 2, 2 ) plt.plot(beta3, abs(M3), marker=&#39;o&#39;,label=&#39;|m|&#39;) plt.legend() plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Magnetization &quot;, fontsize=16) plt.twinx() plt.plot(beta3,errM3,color=&quot;r&quot;, marker=&#39;x&#39;,label=&#39;Errore&#39;) plt.legend() sp = f.add_subplot(2, 2, 3 ) plt.plot(beta3, C3, marker=&#39;o&#39;,label=&#39;c&#39;) plt.legend() plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Specific Heat &quot;, fontsize=16) plt.twinx() plt.plot(beta3,errC3,color=&quot;r&quot;, marker=&#39;x&#39;,label=&#39;Errore&#39;) plt.legend() sp = f.add_subplot(2, 2, 4 ) plt.plot(beta3, X3, marker=&#39;o&#39;,label=&quot;X&quot;) plt.legend() plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Susceptibility&quot;, fontsize=16) plt.twinx() plt.plot(beta3,errX3,color=&quot;r&quot;, marker=&#39;x&#39;,label=&#39;Errore&#39;) plt.legend() plt.show() . . Possiamo notare che i grafici mostrano lo stesso andamento che nel caso precedente, in particolare presentano una transizione di fase per un valore di $ beta$ molto simile a quello del modello di Ising, intorno a 0.44. . A questo punto procediamo con lo studio del Finite Size Scaling per verificare che siano conservate anche tutte le corrette propriet√† di scaling nel punto critico, quindi andremo a calcolare le osservabili, stavolta senza rappresentare gli errori per semplicit√† e maggiore chiarezza dei grafici, per varie taglie del sistema in modo da permettere l&#39;analisi: . #collapse nB = 40 # number of points of beta decSteps= 20 # number of MC sweeps for decorrelation mcSteps = 200 # number of MC sweeps for calculation beta = np.linspace(0.3, 0.6, nB) linell = np.array([7,10,13]) Nell=linell.size E,M,C,X,varx,varc,Esig,Msig = np.zeros((nB,Nell)), np.zeros((nB,Nell)), np.zeros((nB,Nell)), np.zeros((nB,Nell)), np.zeros((nB,Nell)), np.zeros((nB,Nell)), np.zeros((nB,Nell)), np.zeros((nB,Nell)) for Ni in range(Nell): # DIMENSIONI VARIABILI L=linell[Ni] N=AdD(2,L)[1] A=AdD(2,L)[0] mn1, mm, nn = 1.0/(mcSteps*N), 1.0/(mcSteps*(mcSteps-1)) , 1/(N) config = initialstate2(N) for bb in range(1,nB): Ene=np.zeros(mcSteps) Mag=np.zeros(mcSteps) iB=beta[bb]; iB2=iB*iB; for i in range(mcSteps): for j in range(decSteps): # steps of decorrelation mcmove2(config, iB, A) # Metropolis step Ene[i] = calcEnergy2(config) # calculate the energy Mag[i] = calcMag(config) # calculate the magnetization E[bb,Ni] = np.sum(Ene)*mn1 M[bb,Ni] = np.sum(Mag)*mn1 C[bb,Ni] = Di(Ene) X[bb,Ni] = Di(Mag) beta4=beta[1:] E4=E[1:] M4=M[1:] C4=C[1:] X4=X[1:] f = plt.figure(figsize=(18, 10)); # plot the calculated values sp = f.add_subplot(2, 2, 1 ) for i in range(Nell): plt.plot(beta, E[1:,i], marker=&#39;o&#39;,label=linell[i]) plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Energy &quot;, fontsize=16) plt.legend() sp = f.add_subplot(2, 2, 2 ) for i in range(Nell): plt.plot(beta, abs(M[1:,i]), marker=&#39;o&#39;,label=linell[i]) plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Magnetization &quot;, fontsize=16) plt.legend() sp = f.add_subplot(2, 2, 3 ) for i in range(Nell): plt.plot(beta, C[1:,i], marker=&#39;o&#39;,label=linell[i]) plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Specific Heat &quot;, fontsize=16) plt.legend() sp = f.add_subplot(2, 2, 4 ) for i in range(Nell): plt.plot(beta, X[1:,i], marker=&#39;o&#39;,label=linell[i]) plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Susceptibility&quot;, fontsize=16) plt.legend() plt.show() . . ValueError Traceback (most recent call last) &lt;ipython-input-10-b2d48affc8c4&gt; in &lt;module&gt; 50 sp = f.add_subplot(2, 2, 1 ) 51 for i in range(Nell): &gt; 52 plt.plot(beta, E[1:,i], marker=&#39;o&#39;,label=linell[i]) 53 plt.xlabel(&#34;Beta (1/kT)&#34;, fontsize=16) 54 plt.ylabel(&#34;Energy &#34;, fontsize=16) /opt/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py in plot(scalex, scaley, data, *args, **kwargs) 2793 return gca().plot( 2794 *args, scalex=scalex, scaley=scaley, **({&#34;data&#34;: data} if data -&gt; 2795 is not None else {}), **kwargs) 2796 2797 /opt/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in plot(self, scalex, scaley, data, *args, **kwargs) 1664 &#34;&#34;&#34; 1665 kwargs = cbook.normalize_kwargs(kwargs, mlines.Line2D._alias_map) -&gt; 1666 lines = [*self._get_lines(*args, data=data, **kwargs)] 1667 for line in lines: 1668 self.add_line(line) /opt/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py in __call__(self, *args, **kwargs) 223 this += args[0], 224 args = args[1:] --&gt; 225 yield from self._plot_args(this, kwargs) 226 227 def get_next_color(self): /opt/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py in _plot_args(self, tup, kwargs) 389 x, y = index_of(tup[-1]) 390 --&gt; 391 x, y = self._xy_from_xy(x, y) 392 393 if self.command == &#39;plot&#39;: /opt/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py in _xy_from_xy(self, x, y) 268 if x.shape[0] != y.shape[0]: 269 raise ValueError(&#34;x and y must have same first dimension, but &#34; --&gt; 270 &#34;have shapes {} and {}&#34;.format(x.shape, y.shape)) 271 if x.ndim &gt; 2 or y.ndim &gt; 2: 272 raise ValueError(&#34;x and y can be no greater than 2-D, but have &#34; ValueError: x and y must have same first dimension, but have shapes (40,) and (39,) . #collapse f = plt.figure(figsize=(18, 10)); # plot the calculated values sp = f.add_subplot(2, 2, 1 ) for i in range(Nell): plt.plot(beta4, E4[:,i], marker=&#39;o&#39;,label=linell[i]) plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Energy &quot;, fontsize=16) plt.legend() sp = f.add_subplot(2, 2, 2 ) for i in range(Nell): plt.plot(beta4, abs(M4[:,i]), marker=&#39;o&#39;,label=linell[i]) plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Magnetization &quot;, fontsize=16) plt.legend() sp = f.add_subplot(2, 2, 3 ) for i in range(Nell): plt.plot(beta4, C4[:,i], marker=&#39;o&#39;,label=linell[i]) plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Specific Heat &quot;, fontsize=16) plt.legend() sp = f.add_subplot(2, 2, 4 ) for i in range(Nell): plt.plot(beta4, X4[:,i], marker=&#39;o&#39;,label=linell[i]) plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Susceptibility&quot;, fontsize=16) plt.legend() plt.show() . . #collapse Fmagniz=np.zeros((nB,Nell)) Fxi=np.zeros((nB,Nell)) Flins=np.zeros((nB,Nell)) FC=np.zeros((nB,Nell)) ell1=0 for ell in linell: Fmagniz[1:,ell1]=abs(M4)[:,ell1].copy()*(ell)**(1/8) Fxi[1:,ell1]=(X4[:,ell1].copy()/(ell))**(7/4) FC[1:,ell1]=C4[:,ell1].copy() Flins[1:,ell1]=(beta4.copy()-0.4406868)*(ell) ell1+=1 plt.figure(figsize=(18, 10)) plt.subplot(221) plt.plot(beta4-0.4406868,E[1:],marker=&quot;o&quot;) plt.xlabel(&#39;(b-bùñº)&#39;) plt.ylabel(&#39;E&#39;) plt.subplot(222) plt.plot(Flins[1:],Fmagniz[1:],marker=&quot;o&quot;) plt.xlabel(&#39;(b-bùñº) L&#39;) plt.ylabel(&#39;|M| L·µù&#39;) plt.subplot(223) plt.plot(Flins[1:],FC[1:],marker=&quot;o&quot;) plt.xlabel(&#39;(b-bùñº) L&#39;) plt.ylabel(&#39;c L·µû&#39;) plt.subplot(224) plt.plot(Flins[1:],Fxi[1:],marker=&quot;o&quot;) plt.xlabel(&#39;(b-bùñº) L&#39;) plt.ylabel(&#39;œá L·µû&#39;) plt.show() . . Anche in questo caso possiamo notare come, nonostante le fluttuazioni statistiche, il sistema presenti le stesse caratteristiche critiche del modello di Ising 2D e possiamo dunque concludere che l&#39;implementazione della matrice di adiacenza funziona correttamente e quindi procedere con lo studio dei sistemi con interazioni a lunga distanza random. . Studio di sistemi con connessioni random . Per studiare questo tipo di sistemi verranno generate delle matrici di adiacenza attraverso la funzione rand( ) della libreria SciPy che prende in input due numeri interi, relativi alle dimensioni della matrice, e un valore reale compreso tra 0 e 1 che indica la densit√† di elementi della matrice diversi da zero $ rho$. . Nell&#39;implementazione verr√† generata una matrice con gli stessi parametri di input per ogni iterazione Metropolis, in maniera da considerare gli effetti gi√† mediati e i corrispondenti errori. Il nostro scopo √® studiare l&#39;andamento delle osservabili del sistema, come in precedenza, mediando tra le possibili realizzazioni di una matrice random aventi gli stessi parametri di input, e quindi, in seguito, studiare gli andamenti al variare della densit√† di elementi diversi da zero. . La seguente simulazione studia la media dei sistemi a 8 elementi con densit√† di connessioni $ rho=1/8$. . #collapse nB = 30 # number of points of beta decSteps= 100 # number of MC sweeps for decorrelation mcSteps = 500 # number of MC sweeps for calculation beta = np.linspace(0, 5, nB) N=8 E,M,C,X,varX,varC,varE,varM = np.zeros((nB)), np.zeros((nB)), np.zeros((nB)), np.zeros((nB)), np.zeros((nB)), np.zeros((nB)), np.zeros((nB)), np.zeros((nB)) mn1, mm, nn = 1.0/(mcSteps*N), 1.0/(mcSteps*(mcSteps-1)) , 1/(N) config = initialstate2(N) for bb in range(1,nB): Ene=np.zeros(mcSteps) Mag=np.zeros(mcSteps) iB=beta[bb]; iB2=iB*iB; for i in range(mcSteps): for j in range(decSteps): # steps of decorrelation A=ss.rand(N,N,4/N).todok() mcmove2(config, iB, A) # Metropolis step Ene[i] = calcEnergy2(config) # calculate the energy Mag[i] = calcMag(config) # calculate the magnetization E[bb] = np.sum(Ene)*mn1 varE[bb] = np.sum((Ene*nn-E[bb])**2)*mm M[bb] = np.sum(Mag)*mn1 varM[bb] = np.sum((Mag*nn-M[bb])**2)*mm C[bb] = Di(Ene) varC[bb]=boots(Ene,25,5) X[bb] = Di(Mag) varX[bb]=boots(Mag,25,5) . . #collapse beta5=beta[1:] E5=E[1:] errE5=np.sqrt(varE[1:]) M5=M[1:] errM5=np.sqrt(varM[1:]) C5=C[1:] errC5=np.sqrt(varC[1:]) X5=X[1:] errX5=np.sqrt(varX[1:]) f = plt.figure(figsize=(18, 10)); # plot the calculated values sp = f.add_subplot(2, 2, 1 ) plt.plot(beta5, E5, marker=&#39;o&#39;,label=&#39;e&#39;) plt.legend() plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Energy &quot;, fontsize=16) plt.twinx() plt.plot(beta5,errE5,color=&quot;r&quot;, marker=&#39;x&#39;,label=&#39;Errore&#39;) plt.legend() sp = f.add_subplot(2, 2, 2 ) plt.plot(beta5, abs(M5), marker=&#39;o&#39;,label=&#39;|m|&#39;) plt.legend() plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Magnetization &quot;, fontsize=16) plt.twinx() plt.plot(beta5,errM5,color=&quot;r&quot;, marker=&#39;x&#39;,label=&#39;Errore&#39;) plt.legend() sp = f.add_subplot(2, 2, 3 ) plt.plot(beta5, C5, marker=&#39;o&#39;,label=&#39;c&#39;) plt.legend() plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Specific Heat &quot;, fontsize=16) plt.twinx() plt.plot(beta5,errC5,color=&quot;r&quot;, marker=&#39;x&#39;,label=&#39;Errore&#39;) plt.legend() sp = f.add_subplot(2, 2, 4 ) plt.plot(beta5, X5, marker=&#39;o&#39;,label=&quot;X&quot;) plt.legend() plt.xlabel(&quot;Beta (1/kT)&quot;, fontsize=16) plt.ylabel(&quot;Susceptibility&quot;, fontsize=16) plt.twinx() plt.plot(beta5,errX5,color=&quot;r&quot;, marker=&#39;x&#39;,label=&#39;Errore&#39;) plt.legend() plt.show() . . Possiamo notare che i grafici dell&#39;energia e del calore specifico ricordano in forma quelli precedenti, facendo pensare all&#39;esistenza di un punto critico intorno a $ beta=1$ dove possiamo trovare il picco del calore specifico e degli errori in entrambi i grafici, ma in questo caso la magnetizzazione e la suscettivit√† assumono valori anomali. . Il problema potrebbe essere dovuto ad un errore nell&#39;implementazione anche se le verifiche sul modello di Ising sembrano corrette. . La causa invece non dovrebbe essere lo scarso numero di misurazioni in quanto incrementandoli non √® stato visto nessun cambiamento qualitativo. . Una possibile spiegazione, ma poco verosimile, √® che il codice sia giusto e che i grafici relativi alla magnetizzazione, suscettivit√† e corrispondenti errori siano corretti. In questo caso sorgerebbe il problema della corrispondenza con il modello quantistico. Infatti secondo il Quantum to Classical Mapping il modello di Ising 2D classico appartiene alla stessa classe di universalit√† del modello di Ising 1D quantistico. Nella relazione relativa al Modulo 2 ho studiato gli stessi network random ma da un punto di vista quantistico, ed essi presentano una transizione di fase ben definita. Il problema potrebbe risiedere nel fatto che il Quantum to Classical Mapping fa corrispondere sistemi quantistici d-dimensionali a sistemi classici di dimensione d+1, ma, secondo quanto detto per l&#39;introduzione della matrice di adiacenza, entrambi i modelli si possono interpretare come giacenti su una variet√† unidimensionale, ma soggetti a interazioni a lunga distanza, quindi entrambi della stessa dimensione uguale ad 1 e perci√≤ il Quantum to Classical Mapping potrebbe non valere per questo tipo di network random. . In ogni caso per raggiungere delle spiegazioni definitive sarebbero necessari numerosi altri studi, possibilmente molto pi√π precisi. .",
            "url": "https://edoarder.github.io/Metodi_Numerici/2020/07/08/Monte-Carlo-Random-almost-fin-2-4.html",
            "relUrl": "/2020/07/08/Monte-Carlo-Random-almost-fin-2-4.html",
            "date": " ‚Ä¢ Jul 8, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Modulo 2",
            "content": "$$ hat{H} = - sum_{i,j}^N J_{ij} hat{ sigma}_i^z hat{ sigma}_{j}^z - sum_{i}^N g_i hat{ sigma}_i^x - sum_{i}^N h_i hat{ sigma}_i^z$$ . Il modello in esame rappresenta un sistema di $N$ spin-$ frac{1}{2}$ quantistici che interagiscono tra di loro in maniera casuale (statica). Pu√≤ essere visto come la generalizzazione di un modello di Ising quantistico unidimensionale. √à importante sottolineare come la topologia del sistema non sia importante, ma lo √® considerare gli spin fissi e distinguibili, e dare un ordinamento ai singoli elementi del sistema in modo da poter rappresentare le loro interazioni attraverso una matrice di adiacenza. Infatti, una volta labellati gli elementi con un numero intero che va da 1 ad N, essi si possono considerare giacenti su una variet√† unidimensionale, avendo per√≤ interazioni tra spin anche a lunga distanza. Infatti la rappresentazione hamiltoniana di un sistema del genere differisce da quella di un modello di Ising quantistico 1D solamente per il fatto che la somma su $i$ e $j$ non √® ristretta ai primi vicini, ma a tutti gli elementi del sistema e $J_{ij}$ √® una matrice $N times N$ generica. In questo senso il modello pu√≤ essere visto come una generalizzazione del modello di Ising, in quanto esistono matrici di adiacenza $J_{ij}$, che rappresentano un reticolo unidimensionale, per cui il modello rappresentato √® il modello di Ising, ma, attraverso altre matrici di adiacenza, si possono rappresentare per esempio modelli di Ising quantistici 2D oppure, come nel caso sotto esame, sistemi con interazioni casuali. . #collapse %matplotlib inline import matplotlib.pyplot as plt import numpy as np import scipy.sparse.linalg as ssl import scipy.sparse as ss from scipy.sparse import lil_matrix . . Modello di Ising 1D . Iniziamo dunque con l&#39;implementazione dell&#39;algoritmo che diagonalizza l&#39;Hamiltoniana di un modello di Ising quantistico 1D con interazioni a primi vicini standard. . $$ hat{H} = -J sum_{&lt;i,j&gt;}^N hat{ sigma}_i^z hat{ sigma}_{j}^z - g sum_{i}^N hat{ sigma}_i^x - h sum_{i}^N hat{ sigma}_i^z$$ . L&#39;implementazione consiste in alcuni step: . Si costruisce la base con cui si rappresenter√† l&#39;Hamiltoniana . | Si costruisce l&#39;Hamiltoniana: . Contributo delle interazioni a primi vicini | Implementazione delle condizioni al bordo periodiche | Contributo delle interazioni con un campo esterno longitudinale $h$ | Contributo delle interazioni con un campo esterno trasversale $g$ | . | Si trova l&#39;autovalore minore e l&#39;autovettore corrispondente (Ground State) attraverso una routine che implementa il metodo . | Si calcola la magnetizzazione associata al Ground State . | Si calcola la suscettivit√† magnetica e il calore specifico . | #collapse hh=0 volte=100 volte+=1 lins=np.linspace(0,2,volte) linell=np.arange(3,8)*2-1 nell=linell.size magniz=np.zeros((volte,nell)) Egs=np.zeros((volte+1,nell)) xi=np.zeros((volte+1,nell)) ci=np.zeros((volte+1,nell)) ell1=0 for ell in linell: ggg=0 NumTot=2**ell ev=np.zeros((volte,NumTot)) Psii=np.zeros((volte,NumTot)) for gg in lins: PBC=True # Costruisco la base iSpin=np.zeros((NumTot,ell)) for ii in range(NumTot): itemp=ii for jj in range(ell): iSpin[ii,jj]=np.floor(itemp%2) itemp=itemp/2 # Costruisco l&#39;Hamiltoniana HamOut=lil_matrix(np.zeros((NumTot,NumTot))) # Sigma_z Sigma_z [coupling] for iHam in range(ell-1): for ii in range(NumTot): if iSpin[ii,iHam]==iSpin[ii,iHam+1]: HamOut[ii,ii]=HamOut[ii,ii]-1 else: HamOut[ii,ii]=HamOut[ii,ii]+1 # This implements periodic boundary conditions if PBC: for ii in range(NumTot): if iSpin[ii,ell-1]==iSpin[ii,0]: HamOut[ii,ii]=HamOut[ii,ii]-1 else: HamOut[ii,ii]=HamOut[ii,ii]+1 # Sigma_z [longitudinal field] for iHam in range(ell): for ii in range(NumTot): if iSpin[ii,iHam]==1: HamOut[ii,ii]=HamOut[ii,ii]-hh else: HamOut[ii,ii]=HamOut[ii,ii]+hh # Sigma_x [transverse field] for iHam in range(ell): for ii in range(NumTot): if iSpin[ii,iHam]==1: Exc = ii - 2**(iHam) else: Exc = ii + 2**(iHam) HamOut[int(Exc),ii] = HamOut[int(Exc),ii] + gg # Trovo l&#39;autostato del GS Egs[ggg,ell1]=ssl.eigsh(HamOut,1,which=&#39;SA&#39;)[0] Psi=ssl.eigsh(HamOut,1,which=&#39;SA&#39;)[1].T[0] Psii[ggg]=Psi.copy() # Calcolo la magnetizzazione MagnetZ=0 for ii in range(NumTot): Mag_ii = 0 for iSite in range(ell): if iSpin[ii,iSite] == 1: Mag_ii = Mag_ii + 1 else: Mag_ii = Mag_ii - 1 MagnetZ = MagnetZ + abs(Mag_ii)*(abs(Psi[ii])**2) magniz[ggg,ell1]=MagnetZ/ell xi[ggg,ell1]=-(magniz[ggg-1,ell1]-magniz[ggg,ell1])/(lins[ggg-1]-lins[ggg]) ci[ggg,ell1]=-(Egs[ggg-1,ell1]-Egs[ggg,ell1])/(lins[ggg-1]-lins[ggg]) ggg+=1 ell1+=1 magniz1=magniz Egs1=Egs xi1=xi ci1=ci lins1=lins linell1=linell . . #collapse plt.figure(figsize=(12, 9)) plt.subplot(221) for i in range(nell): elle=linell1[i] plt.plot(lins1[1:],magniz1[1:,i],label=elle) plt.legend() plt.ylim(0,1.1) plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Magnetization&#39;) plt.subplot(222) for i in range(nell): elle=linell1[i] plt.plot(lins1[1:],Egs1[1:volte,i],label=elle) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Energy&#39;) plt.subplot(223) for i in range(nell): elle=linell1[i] plt.plot(lins1[1:],xi1[1:volte,i],label=elle) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Susceptibility&#39;) plt.subplot(224) for i in range(nell): elle=linell1[i] plt.plot(lins1[1:],ci1[1:volte,i],label=elle) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Specific heat capacity&#39;) plt.show() . . Come possiamo notare la suscettivit√† magnetica presenta un picco in corrispondenza di $g=1$ che all&#39;aumentare della taglia del sistema diventa sempre pi√π accentuato, ad indicare che nel limite termodinamico si osserva un punto critico ed il sistema presenta una transizione da una fase ferromagnetica ad una paramagnetica all&#39;aumentare di $g$, effetto previsto dalla soluzione analitica del sistema. . Per studiare meglio questo effetto procediamo con uno studio del Finite Size Scaling. . Fmagniz=np.zeros((volte,nell)) Fxi=np.zeros((volte+1,nell)) Flins=np.zeros((volte,nell)) ell1=0 for ell in linell: Fmagniz[:,ell1]=magniz1[:,ell1]*(ell)**(1/8) Fxi[:,ell1]=(xi1[:,ell1]/(ell))**(7/4) Flins[:,ell1]=(lins1-1)*(ell) ell1+=1 plt.figure(figsize=(12,5)) plt.subplot(121) plt.plot(Flins[1:],Fmagniz[1:]) plt.xlabel(&#39;(g-gùñº) L&#39;) plt.ylabel(&#39;|Mz| L·µù&#39;) plt.subplot(122) plt.plot(Flins[1:],Fxi[1:volte]) plt.xlabel(&#39;(g-gùñº) L&#39;) plt.ylabel(&#39;œá L·µû&#39;) plt.show() . Il Finite Size Scaling √® stato operato inserendo come esponenti critici quelli conosciuti tramite la risoluzione analitica del problema ovvero: $$ beta=1/8, quad gamma=7/4, quad nu=1, quad Y_g=1$$ . avendo considerato il caso con campo esterno longitudinale $h=0$. . Poich√® i grafici per le varie taglie del sistema coincidono quasi esattamente possiamo concludere che gli esponenti critici sono giusti e che quindi siamo in presenza di una transizione di fase relativa alla corretta classe di universalit√†. . Implementazione della matrice di adiacenza . Al fine di poter calcolare le osservabili termodinamiche di un sistema con connessioni random √® necessario implementare nell&#39;algoritmo un modo per rappresentare le connessioni attraverso una matrice di adiacenza. . Per farlo andremo ad aggiungere uno step alla costruzione dell&#39;Hamiltoniana in cui, data una matrice di adiacenza con alcuni elementi diversi da zero, andremo a controllare per ogni collegamento, se gli spin corrispondenti agli indici dell&#39;elemento diverso da zero sono concordi o discordi ed andremo ad aggiungere o togliere energia alla configurazione considerata. . La matrice di adiacenza essendo sparsa non verr√† considerata tutta, ma solo gli elementi diversi da zero, per velocizzare la compilazione. . Per verificare il giusto comportamento del nuovo codice √® stato testato usando come matrice di adiacenza proprio quella relativa ad un reticolo 1D intero con connessioni a primi vicini, per poi essere confrontato con i risultati precedenti che devono risultare uguali. . Introduciamo quindi una funzione che genera le matrici di adiacenza relative a reticoli d-dimensionali interi aventi L elementi per lato: . def AdD(d,L): # This function returns the sparse adjacency matrix of a d-dimensonal # integer lattice with side L and the total number of elements N=L**d # Only for Periodic Boundary Conditions import scipy.sparse as ss N=L**d A=ss.dok_matrix((N,N)) for i in range(N): for j in range(d): A[i,(i+L**j)%N] = 1 A[i,(i-L**j)%N] = 1 return A.copy().tocoo(), N . E testiamo quindi il funzionamento del codice aggiornato per implementare le matrici di adiacenza: . #collapse hh=0 volte=100 volte+=1 lins=np.linspace(0,2,volte) linell=np.arange(3,8)*2-1 nell=linell.size magniz=np.zeros((volte,nell)) Egs=np.zeros((volte+1,nell)) xi=np.zeros((volte+1,nell)) ci=np.zeros((volte+1,nell)) jay=-1/2 ell1=0 for ell in linell: ggg=0 NumTot=2**ell ev=np.zeros((volte,NumTot)) Psii=np.zeros((volte,NumTot)) net=AdD(1,ell)[0] # Matrice di Adiacenza per un reticolo 1D coll=ss.find(net) for gg in lins: PBC=True # Costruisco la base iSpin=np.zeros((NumTot,ell)) for ii in range(NumTot): itemp=ii for jj in range(ell): iSpin[ii,jj]=np.floor(itemp%2) itemp=itemp/2 # Costruisco l&#39;Hamiltoniana HamOut=lil_matrix(np.zeros((NumTot,NumTot))) for iHam in range(len(coll[2])): for ii in range(NumTot): if iSpin[ii,coll[0][iHam]]==iSpin[ii,coll[1][iHam]]: HamOut[ii,ii]=HamOut[ii,ii] + jay else: HamOut[ii,ii]=HamOut[ii,ii] - jay # Sigma_x [transverse field] for iHam in range(ell): for ii in range(NumTot): if iSpin[ii,iHam]==1: Exc = ii - 2**(iHam) else: Exc = ii + 2**(iHam) HamOut[int(Exc),ii] = HamOut[int(Exc),ii] + gg # Trovo l&#39;autostato del GS Egs[ggg,ell1]=ssl.eigsh(HamOut,1,which=&#39;SA&#39;)[0] Psi=ssl.eigsh(HamOut,1,which=&#39;SA&#39;)[1].T[0] Psii[ggg]=Psi.copy() # Calcolo la magnetizzazione MagX=np.zeros(ell) MagZ=np.zeros(ell) MagnetZ=0 for iSite in range(ell): Mx_sum=0 for ii in range(NumTot): if iSpin[ii,iSite] == 1: MagZ[iSite] = MagZ[iSite] + abs(Psi[ii])**2 Exc = ii -2**(iSite) else: MagZ[iSite] = MagZ[iSite] - abs(Psi[ii])**2 Exc = ii +2**(iSite) Mx_sum = Mx_sum + np.conjugate(Psi[ii]) * Psi[Exc] if abs(np.imag(Mx_sum))&gt;10**(-10): print(&quot;Non real magnetization&quot;) MagX[iSite] = np.real(Mx_sum) for ii in range(NumTot): Mag_ii = 0 for iSite in range(ell): if iSpin[ii,iSite] == 1: Mag_ii = Mag_ii + 1 else: Mag_ii = Mag_ii - 1 MagnetZ = MagnetZ + abs(Mag_ii)*(abs(Psi[ii])**2) magniz[ggg,ell1]=MagnetZ/ell xi[ggg,ell1]=-(magniz[ggg-1,ell1]-magniz[ggg,ell1])/(lins[ggg-1]-lins[ggg]) ci[ggg,ell1]=-(Egs[ggg-1,ell1]-Egs[ggg,ell1])/(lins[ggg-1]-lins[ggg]) ggg+=1 ell1+=1 magniz2=magniz Egs2=Egs xi2=xi ci2=ci lins2=lins linell2=linell plt.figure(figsize=(12, 9)) plt.subplot(221) for i in range(nell): elle=linell1[i] plt.plot(lins1[1:],magniz1[1:,i],label=elle) plt.legend() plt.ylim(0,1.1) plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Magnetization&#39;) plt.subplot(222) for i in range(nell): elle=linell1[i] plt.plot(lins1[1:],Egs1[1:volte,i],label=elle) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Energy&#39;) plt.subplot(223) for i in range(nell): elle=linell1[i] plt.plot(lins1[1:],xi1[1:volte,i],label=elle) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Susceptibility&#39;) plt.subplot(224) for i in range(nell): elle=linell1[i] plt.plot(lins1[1:],ci1[1:volte,i],label=elle) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Specific heat capacity&#39;) plt.show() . . I grafici rislutano identici a quelli precedenti ed insieme al Finite Size Scaling sottostante, usando gli stessi esponenti critici, ci confermano il corretto funzionamento del codice. . #collapse Fmagniz=np.zeros((volte,nell)) Fxi=np.zeros((volte+1,nell)) Flins=np.zeros((volte,nell)) ell1=0 for ell in linell: Fmagniz[:,ell1]=magniz2[:,ell1]*(ell)**(1/8) Fxi[:,ell1]=(xi2[:,ell1]/(ell))**(7/4) Flins[:,ell1]=-(lins-1)*(ell) ell1+=1 plt.figure(figsize=(12,5)) plt.subplot(121) plt.plot(Flins[1:],Fmagniz[1:]) plt.xlabel(&#39;(g-gùñº) L&#39;) plt.ylabel(&#39;|Mz| L·µù&#39;) plt.subplot(122) plt.plot(Flins[1:],Fxi[1:volte]) plt.xlabel(&#39;(g-gùñº) L&#39;) plt.ylabel(&#39;œá L·µû&#39;) plt.show() . . Modello di Ising 2D . Ora sfruttiamo questo stesso procedimento per studiare il modello di Ising quantistico 2D, usando la matrice di adiacenza opportuna, e confrontarlo con i risultati Monte-Carlo della relazione precedente per un modello Ising classico 3D, verificando il Quantum to Classical Mapping. . #collapse hh=0 volte=60 volte+=1 lins=np.linspace(0,6,volte) linell=np.arange(2,5) nell=linell.size magniz=np.zeros((volte,nell)) Egs=np.zeros((volte+1,nell)) dm=np.zeros((volte+1,nell)) de=np.zeros((volte+1,nell)) jay=-1/2 ell1=0 for ell in linell: ggg=0 net=AdD(2,ell)[0] coll=ss.find(net) ell=ell*ell NumTot=2**ell ev=np.zeros((volte,NumTot)) Psii=np.zeros((volte,NumTot)) for gg in lins: PBC=True # Costruisco la base iSpin=np.zeros((NumTot,ell)) for ii in range(NumTot): itemp=ii for jj in range(ell): iSpin[ii,jj]=np.floor(itemp%2) itemp=itemp/2 # Costruisco l&#39;Hamiltoniana HamOut=lil_matrix(np.zeros((NumTot,NumTot),dtype=&quot;float32&quot;),dtype=&quot;float32&quot;) for iHam in range(len(coll[2])): for ii in range(NumTot): if iSpin[ii,coll[0][iHam]]==iSpin[ii,coll[1][iHam]]: HamOut[ii,ii]=HamOut[ii,ii] + jay else: HamOut[ii,ii]=HamOut[ii,ii] - jay # Sigma_x [transverse field] contorta ma torna: in pratica cambia lo spin al sito j for iHam in range(ell): for ii in range(NumTot): if iSpin[ii,iHam]==1: Exc = ii - 2**(iHam) else: Exc = ii + 2**(iHam) HamOut[int(Exc),ii] = HamOut[int(Exc),ii] + gg # Trovo l&#39;autostato del GS Egs[ggg,ell1]=ssl.eigsh(HamOut,1,which=&#39;SA&#39;)[0] Psi=ssl.eigsh(HamOut,1,which=&#39;SA&#39;)[1].T[0] Psii[ggg]=Psi.copy() # Calcolo la magnetizzazione MagX=np.zeros(ell) MagZ=np.zeros(ell) MagnetZ=0 for iSite in range(ell): Mx_sum=0 for ii in range(NumTot): if iSpin[ii,iSite] == 1: MagZ[iSite] = MagZ[iSite] + abs(Psi[ii])**2 Exc = ii -2**(iSite) else: MagZ[iSite] = MagZ[iSite] - abs(Psi[ii])**2 Exc = ii +2**(iSite) Mx_sum = Mx_sum + np.conjugate(Psi[ii]) * Psi[Exc] if abs(np.imag(Mx_sum))&gt;10**(-10): print(&quot;Non real magnetization&quot;) MagX[iSite] = np.real(Mx_sum) for ii in range(NumTot): Mag_ii = 0 for iSite in range(ell): if iSpin[ii,iSite] == 1: Mag_ii = Mag_ii + 1 else: Mag_ii = Mag_ii - 1 MagnetZ = MagnetZ + abs(Mag_ii)*(abs(Psi[ii])**2) magniz[ggg,ell1]=MagnetZ/ell dm[ggg,ell1]=-(magniz[ggg-1,ell1]-magniz[ggg,ell1])/(lins[ggg-1]-lins[ggg]) de[ggg,ell1]=-(Egs[ggg-1,ell1]-Egs[ggg,ell1])/(lins[ggg-1]-lins[ggg]) ggg+=1 ell1+=1 . . magniz3=magniz Egs3=Egs dm3=dm de3=de lins3=lins linell3=linell plt.figure(figsize=(12, 9)) plt.subplot(221) for i in range(nell): elle=linell3[i] plt.plot(lins3[1:],magniz3[1:,i],label=elle) plt.legend() plt.ylim(0,1.1) plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Magnetization&#39;) plt.subplot(222) for i in range(nell): elle=linell3[i] plt.plot(lins3[1:],Egs3[1:volte,i],label=elle) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Energy&#39;) plt.subplot(223) for i in range(nell): elle=linell3[i] plt.plot(lins3[1:],dm3[1:volte,i],label=elle) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;dM/dg&#39;) plt.subplot(224) for i in range(nell): elle=linell3[i] plt.plot(lins3[1:],de3[1:volte,i],label=elle) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;dE/dg&#39;) plt.show() . Per verificare che il sistema studiato sopra abbia le giuste propriet√† di scaling, ovvero quelle di un sistema di Ising 3D classico, procediamo pure in questo caso con uno studio del Finite Size Scaling, utilizzando gli esponenti critici noti in letteratura calcolati in via approssimata, poich√® per questo tipo di modello non sono disponibili soluzioni analitiche: . $$ beta=0.326419, quad gamma=1.237075, quad nu=0.629971, quad Y_g=1.412625$$ . sempre considerando il caso con campo esterno longitudinale $h=0$. . #collapse Fmagniz=np.zeros((volte,nell)) Flins=np.zeros((volte,nell)) ell1=0 for ell in linell3: Fmagniz[:,ell1]=magniz3[:,ell1]*(ell)**(0.326419/0.629971) Flins[:,ell1]=(lins3-3.1)*(ell)**1.412625 ell1+=1 plt.figure(figsize=(10,5)) plt.subplot(111) plt.plot(Flins[1:],Fmagniz[1:]) plt.xlabel(&#39;(g-gùñº) L&#39;) plt.ylabel(&#39;|Mz| L·µù&#39;) plt.show() . . Anche in questo caso possiamo notare che i grafici per le diverse taglie del sistema tendono a sovrapporsi dopo il Finite Size Scaling, anche se non esattamente in questo caso. Probabilmente questo difetto √® dovuto alle basse taglie dei sistemi studiati, soprattutto la $2 times 2$ , ma studiare sistemi $5 times 5$ o pi√π grandi attraverso diagonalizzazione esatta della matrice hamiltoniana comporterebbe dei tempi di computazione proibitivi. . Studio di sistemi con connessioni random . Passiamo dunque ora allo studio di un sistema le cui connessioni tra elementi sono rappresentate da una matrice di adiacenza random, ovvero in cui gli elementi della matrice diversi da zero sono disposti in maniera casuale all&#39;interno della matrice stessa. La matrice di adiacenza √® data da una funzione delle libreria SciPy che prende in input le dimensioni della matrice e la densit√† $ rho$ di elementi diversi da zero che si vuole ottenere. Per esempio per un sistema di 10 elementi come quello seguente, prendendo in input i valori (10, 10, 0.2) la funzione rand() restituisce una matrice 10x10 con 20 elementi diversi da zero scelti a caso. . Poich√® per gli stessi valori di input la funzione restituisce realizzazioni diverse di sistemi con le stesse caratteristiche, per avere un comportamento generale verranno calcolate varie realizzazioni di sistemi con gli stessi parametri e i valori di output verranno mediati fra tutte le realizzazioni. Il grafico della media √® evidenziato in rosso. . #collapse hh=0 volte=50 volte+=1 lins=np.linspace(0,3,volte) linum=np.arange(25) nell=linum.size magniz=np.zeros((volte,nell)) Egs=np.zeros((volte+1,nell)) xi=np.zeros((volte+1,nell)) ci=np.zeros((volte+1,nell)) jay=-1 ell=10 ell1=0 for numer in linum: ggg=0 NumTot=2**ell ev=np.zeros((volte,NumTot)) Psii=np.zeros((volte,NumTot)) net=ss.rand(ell,ell,0.1)#1/ell) # Matrice di Adiacenza Random coll=ss.find(net) for gg in lins: PBC=True # Costruisco la base iSpin=np.zeros((NumTot,ell)) for ii in range(NumTot): itemp=ii for jj in range(ell): iSpin[ii,jj]=np.floor(itemp%2) itemp=itemp/2 # Costruisco l&#39;Hamiltoniana HamOut=lil_matrix(np.zeros((NumTot,NumTot))) for iHam in range(len(coll[2])): for ii in range(NumTot): if iSpin[ii,coll[0][iHam]]==iSpin[ii,coll[1][iHam]]: HamOut[ii,ii]=HamOut[ii,ii] + jay else: HamOut[ii,ii]=HamOut[ii,ii] - jay # Sigma_x [transverse field] contorta ma torna: in pratica cambia lo spin al sito j for iHam in range(ell): for ii in range(NumTot): if iSpin[ii,iHam]==1: Exc = ii - 2**(iHam) else: Exc = ii + 2**(iHam) HamOut[int(Exc),ii] = HamOut[int(Exc),ii] + gg # Trovo l&#39;autostato del GS Egs[ggg,ell1]=ssl.eigsh(HamOut,1,which=&#39;SA&#39;)[0] Psi=ssl.eigsh(HamOut,1,which=&#39;SA&#39;)[1].T[0] Psii[ggg]=Psi.copy() # Calcolo la magnetizzazione MagnetZ=0 for ii in range(NumTot): Mag_ii = 0 for iSite in range(ell): if iSpin[ii,iSite] == 1: Mag_ii = Mag_ii + 1 else: Mag_ii = Mag_ii - 1 MagnetZ = MagnetZ + abs(Mag_ii)*(abs(Psi[ii])**2) magniz[ggg,ell1]=MagnetZ/ell xi[ggg,ell1]=-(magniz[ggg-1,ell1]-magniz[ggg,ell1])/(lins[ggg-1]-lins[ggg]) ci[ggg,ell1]=-(Egs[ggg-1,ell1]-Egs[ggg,ell1])/(lins[ggg-1]-lins[ggg]) ggg+=1 ell1+=1 magniz5=magniz Egs5=Egs xi5=xi ci5=ci plt.figure(figsize=(12, 9)) plt.subplot(221) plt.plot(lins[1:],magniz5[1:],color=&quot;b&quot;, linewidth=0.5) plt.plot(lins[1:],np.mean(magniz5[1:],axis=1),color=&#39;r&#39;, linewidth=3.0,Label=&quot;Media&quot;) plt.legend() plt.ylim(0,1.1) plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Magnetization&#39;) plt.subplot(222) plt.plot(lins[1:],Egs5[1:volte],color=&quot;b&quot;, linewidth=0.5) plt.plot(lins[1:],np.mean(Egs5[1:volte],axis=1),color=&#39;r&#39;, linewidth=3.0,Label=&quot;Media&quot;) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Energy&#39;) plt.subplot(223) plt.plot(lins[2:],xi5[2:volte],color=&quot;b&quot;, linewidth=0.5) plt.plot(lins[2:],np.mean(xi5[2:volte],axis=1),color=&#39;r&#39;, linewidth=3.0,Label=&quot;Media&quot;) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Susceptibility&#39;) plt.subplot(224) plt.plot(lins[1:],ci5[1:volte],color=&quot;b&quot;, linewidth=0.5) plt.plot(lins[2:],np.mean(ci5[2:volte],axis=1),color=&#39;r&#39;, linewidth=3.0,Label=&quot;Media&quot;) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Specific heat capacity&#39;) plt.show() . . Per tutte le realizzazioni si osserva un punto di transizione di fase per valori di $g$ intorno ad 1 con una media poco superiore ad 1. Si pu√≤ notare inoltre che alcune realizzazioni non producono una magnetizzazione completa nemmeno per $g=0$. . Il sistema precedente, composto da 10 elemtni, aveva come densit√† di elemeti diversi da zero $ rho=0.1$ . √à possibile che il valore del punto di transizione dipenda da questo valore? Per rispondere a questa domanda possiamo procedere con uno studio delle medie di varie realizzazioni con densit√† variabili da 0.1 a 1, poich√® il caso $ rho=0$ √® banale e corrisponde ad un sistema senza interazioni e quindi senza magnetizzazione. . #collapse ell=10 hh=0 volte=50 volte+=1 nrip=10 lins=np.linspace(0,5,volte) linum=np.arange(2,12)*2 nell=linum.size magnizR=np.zeros((volte,nell,nrip)) EgsR=np.zeros((volte+1,nell,nrip)) xiR=np.zeros((volte+1,nell,nrip)) ciR=np.zeros((volte+1,nell,nrip)) jay=-1 ell1=0 for numer in linum: NumTot=2**ell ev=np.zeros((volte,NumTot)) Psii=np.zeros((volte,NumTot)) net=ss.rand(ell,ell,numer/ell**2) # Matrice di Adiacenza Random coll=ss.find(net) for rip in range(nrip): ggg=0 for gg in lins: # Costruisco la base iSpin=np.zeros((NumTot,ell)) for ii in range(NumTot): itemp=ii for jj in range(ell): iSpin[ii,jj]=np.floor(itemp%2) itemp=itemp/2 # Costruisco l&#39;Hamiltoniana HamOut=lil_matrix(np.zeros((NumTot,NumTot))) for iHam in range(len(coll[2])): for ii in range(NumTot): if iSpin[ii,coll[0][iHam]]==iSpin[ii,coll[1][iHam]]: HamOut[ii,ii]=HamOut[ii,ii] + jay else: HamOut[ii,ii]=HamOut[ii,ii] - jay # Sigma_x [transverse field] contorta ma torna: in pratica cambia lo spin al sito j for iHam in range(ell): for ii in range(NumTot): if iSpin[ii,iHam]==1: Exc = ii - 2**(iHam) else: Exc = ii + 2**(iHam) HamOut[int(Exc),ii] = HamOut[int(Exc),ii] + gg # Trovo l&#39;autostato del GS EgsR[ggg,ell1,rip]=ssl.eigsh(HamOut,1,which=&#39;SA&#39;)[0] Psi=ssl.eigsh(HamOut,1,which=&#39;SA&#39;)[1].T[0] Psii[ggg]=Psi.copy() # Calcolo la magnetizzazione MagnetZ=0 for ii in range(NumTot): Mag_ii = 0 for iSite in range(ell): if iSpin[ii,iSite] == 1: Mag_ii = Mag_ii + 1 else: Mag_ii = Mag_ii - 1 MagnetZ = MagnetZ + abs(Mag_ii)*(abs(Psi[ii])**2) magnizR[ggg,ell1,rip]=MagnetZ/ell xiR[ggg,ell1,rip]=-(magnizR[ggg-1,ell1,rip]-magnizR[ggg,ell1,rip])/(lins[ggg-1]-lins[ggg]) ciR[ggg,ell1,rip]=-(EgsR[ggg-1,ell1,rip]-EgsR[ggg,ell1,rip])/(lins[ggg-1]-lins[ggg]) ggg+=1 magniz7=np.mean(magnizR,axis=2) Egs7=np.mean(EgsR,axis=2) xi7=np.mean(xiR,axis=2) ci7=np.mean(ciR,axis=2) ell1+=1 . . #collapse plt.figure(figsize=(12, 9)) plt.subplot(221) for i in range(nell-1): plt.plot(lins[1:],magniz7[1:,i],label=round(linum[i]/ell**2,2)) plt.legend() plt.ylim(0,1.1) plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Magnetization&#39;) plt.subplot(222) for i in range(nell-1): plt.plot(lins[1:],Egs7[1:volte,i],label=round(linum[i]/ell**2,2)) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Energy&#39;) plt.subplot(223) for i in range(nell-1): plt.plot(lins[2:],xi7[2:volte,i],label=round(linum[i]/ell**2,2)) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Susceptibility&#39;) plt.subplot(224) for i in range(nell-1): plt.plot(lins[2:],ci7[2:volte,i],label=round(linum[i]/ell**2,2)) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Specific heat capacity&#39;) plt.show() . . Dal grafico sopra possiamo notare due cose: . a valori di $ rho$ maggiori corrispondono punti di transizione a valori di $g$ maggiori | esiste un valore di $ rho$ sotto il quale il sistema non raggiunge la magnetizzazione totale nemmeno per $g=0$ | Per studiare meglio il secondo effetto verranno calcolate di seguito le medie di 100 realizzazioni di sistemi con $g=0$ andando a vedere l&#39;andamento per valori crescenti di $ rho$ , pi√π precisamente, considerando un sistema composto da 8 elementi, studiando l&#39;andamento al variare del numero di elementi della matrice di adiacenza diversi da zero. . # collapse ell=8 hh=0 gg=0 nrip=100 linum=np.arange(1,21) nell=linum.size magnizR=np.zeros((nell,nrip)) EgsR=np.zeros((nell,nrip)) xiR=np.zeros((nell,nrip)) ciR=np.zeros((nell,nrip)) jay=-1 NumTot=2**ell ell1=0 for numer in linum: ev=np.zeros((NumTot)) Psii=np.zeros((NumTot)) net=ss.rand(ell,ell,numer/ell**2) # Matrice di Adiacenza Random coll=ss.find(net) for rip in range(nrip): # Costruisco la base iSpin=np.zeros((NumTot,ell)) for ii in range(NumTot): itemp=ii for jj in range(ell): iSpin[ii,jj]=np.floor(itemp%2) itemp=itemp/2 # Costruisco l&#39;Hamiltoniana HamOut=lil_matrix(np.zeros((NumTot,NumTot))) for iHam in range(len(coll[2])): for ii in range(NumTot): if iSpin[ii,coll[0][iHam]]==iSpin[ii,coll[1][iHam]]: HamOut[ii,ii]=HamOut[ii,ii] + jay else: HamOut[ii,ii]=HamOut[ii,ii] - jay # Sigma_x [transverse field] for iHam in range(ell): for ii in range(NumTot): if iSpin[ii,iHam]==1: Exc = ii - 2**(iHam) else: Exc = ii + 2**(iHam) HamOut[int(Exc),ii] = HamOut[int(Exc),ii] + gg # Trovo l&#39;autostato del GS EgsR[ell1,rip]=ssl.eigsh(HamOut,1,which=&#39;SA&#39;)[0] Psi=ssl.eigsh(HamOut,1,which=&#39;SA&#39;)[1].T[0] # Calcolo la magnetizzazione MagnetZ=0 for ii in range(NumTot): Mag_ii = 0 for iSite in range(ell): if iSpin[ii,iSite] == 1: Mag_ii = Mag_ii + 1 else: Mag_ii = Mag_ii - 1 MagnetZ = MagnetZ + abs(Mag_ii)*(abs(Psi[ii])**2) magnizR[ell1,rip]=MagnetZ/ell # xiR[ell1,rip]=-(magnizR[ggg-1,ell1-1,rip]-magnizR[ggg,ell1,rip])/(lins[ggg-1]-lins[ggg]) # ciR[ell1,rip]=-(EgsR[ggg-1,ell1,rip]-EgsR[ggg,ell1,rip])/(lins[ggg-1]-lins[ggg]) magniz8=np.mean(magnizR,axis=1) Egs8=np.mean(EgsR,axis=1) # xi8=np.mean(xiR,axis=1) # ci8=np.mean(ciR,axis=1) ell1+=1 . . #collapse plt.plot(linum,magniz8) plt.ylabel(&#39;M(g=0)&#39;) plt.xlabel(&#39;Numero di collegamenti&#39;) plt.show() . . Nelle ascisse sebbene siano rappresentati anche valori decimali, i valori considerati sono solo numeri interi che vanno da 1 a 20. Possiamo notare una sorta di transizione di fase, anche se non lo √® propriamente: la magnetizzazione infatti cresce fino ad arrivare alla saturazione per valori intorno ad 8, ovvero il numero di elementi del sistema. . Questo dimostra un fenomeno abbastanza intuitivo ovvero, la magnetizzazione completa si pu√≤ ottenere solamente se ogni elemento interagisce con almeno un altro elemento del sistema e questo pu√≤ accadere solo se il numero di collegamenti √® almeno pari al numero degli elementi. Questa condizione √® solo necessaria in quanto, come possiamo notare dal grafico con la media di varie realizzazioni, che hanno la stessa densit√† pari a 0.1, quindi 10 elementi di matrice di adiacenza diversi da zero, ci sono alcune realizzazioni che per $g=0$ non hanno $M=1$ ma inferiore, segno che alcuni collegamenti sono stati fatti tra elementi gi√† collegati e quindi altri elementi sono rimasti isolati. . Per quanto riguarda il primo punto invece, possiamo notare dai grafici precedenti che all&#39;aumentare della densit√† di elementi diversi da zero, e quindi del numero di connessioni a lungo raggio, il punto di transizione avviene per valori di $g$ sempre maggiori, risultato verosimile in quanto pi√π connessioni ferromagnetiche ci sono pi√π √® probabile che il sistema presenti una fase ferromagnetica pi√π stabile all&#39;aumentare di $g$, che tende a distruggere la magnetizzazione del sistema. . Si pu√≤ osservare che anche per $ rho = 1$ esiste un valore di $g$ per cui si ha una transizione di fase e sopra il quale si ha una fase paramagnetica. Ci√≤ √® in qualche maniera sorprendente perch√® per $ rho = 1$ si ha una matrice di adiacenza piena alla quale corrisponde un network con connessioni all-to-all, quindi ogni elemento del sistema tende ad allinearsi con ogni altro elemento del sistema, formando una fase ferromagnetica particolarmente forte, come si pu√≤ notare dal plateau iniziale nella magnetizzazione, ma comunque non &quot;indistruttibile&quot;. . Pu√≤ essere interessante andare a studiare come il punto di transizione di un network all-to-all dipenda dalla taglia del sistema. . #collapse hh=0 volte=60 volte+=1 nrip=1 # lins=np.linspace(0,10,volte) linell=np.arange(6,11) nell=linum.size magnizR=np.zeros((volte,nell,nrip)) EgsR=np.zeros((volte+1,nell,nrip)) xiR=np.zeros((volte+1,nell,nrip)) ciR=np.zeros((volte+1,nell,nrip)) jay=-1/2 ell1=0 for ell in linell: NumTot=2**ell ev=np.zeros((volte,NumTot)) Psii=np.zeros((volte,NumTot)) net=ss.rand(ell,ell,1) # Matrice di Adiacenza All-to-All coll=ss.find(net) for rip in range(nrip): ggg=0 for gg in lins: # Costruisco la base iSpin=np.zeros((NumTot,ell)) for ii in range(NumTot): itemp=ii for jj in range(ell): iSpin[ii,jj]=np.floor(itemp%2) itemp=itemp/2 # Costruisco l&#39;Hamiltoniana HamOut=lil_matrix(np.zeros((NumTot,NumTot))) for iHam in range(len(coll[2])): for ii in range(NumTot): if iSpin[ii,coll[0][iHam]]==iSpin[ii,coll[1][iHam]]: HamOut[ii,ii]=HamOut[ii,ii] + jay else: HamOut[ii,ii]=HamOut[ii,ii] - jay # Sigma_x [transverse field] contorta ma torna: in pratica cambia lo spin al sito j for iHam in range(ell): for ii in range(NumTot): if iSpin[ii,iHam]==1: Exc = ii - 2**(iHam) else: Exc = ii + 2**(iHam) HamOut[int(Exc),ii] = HamOut[int(Exc),ii] + gg # Trovo l&#39;autostato del GS EgsR[ggg,ell1,rip]=ssl.eigsh(HamOut,1,which=&#39;SA&#39;)[0] Psi=ssl.eigsh(HamOut,1,which=&#39;SA&#39;)[1].T[0] Psii[ggg]=Psi.copy() # Calcolo la magnetizzazione MagnetZ=0 for ii in range(NumTot): Mag_ii = 0 for iSite in range(ell): if iSpin[ii,iSite] == 1: Mag_ii = Mag_ii + 1 else: Mag_ii = Mag_ii - 1 MagnetZ = MagnetZ + abs(Mag_ii)*(abs(Psi[ii])**2) magnizR[ggg,ell1,rip]=MagnetZ/ell xiR[ggg,ell1,rip]=-(magnizR[ggg-1,ell1,rip]-magnizR[ggg,ell1,rip])/(lins[ggg-1]-lins[ggg]) ciR[ggg,ell1,rip]=-(EgsR[ggg-1,ell1,rip]-EgsR[ggg,ell1,rip])/(lins[ggg-1]-lins[ggg]) ggg+=1 magniz6=np.mean(magnizR,axis=2) Egs6=np.mean(EgsR,axis=2) xi6=np.mean(xiR,axis=2) ci6=np.mean(ciR,axis=2) ell1+=1 plt.figure(figsize=(12, 9)) plt.subplot(221) for i in linell: plt.plot(lins[1:],magniz6[1:,i-linell[0]],label=i) plt.legend() plt.ylim(0,1.1) plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Magnetization&#39;) plt.subplot(222) for i in linell: plt.plot(lins[1:],Egs6[1:volte,i-linell[0]],label=i) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Energy&#39;) plt.subplot(223) for i in linell: plt.plot(lins[2:],xi6[2:volte,i-linell[0]],label=i) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Susceptibility&#39;) plt.subplot(224) for i in linell: plt.plot(lins[2:],ci6[2:volte,i-linell[0]],label=i) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Specific heat capacity&#39;) plt.show() . . Possiamo notare dai grafici sopra che il valore di $g$ corrispondente al punto di critico aumenta con l&#39;aumentare della taglia. √à legittimo dunque pensare che nel limite termodinamico il valore di $g$ alla transizione di fase diverga e dunque che un sistema con connessioni ferromagnetiche all-to-all rimanga magnetizzazato per qualunque valore di $g$. .",
            "url": "https://edoarder.github.io/Metodi_Numerici/2020/07/07/Random_Network_Diagonalization-1-3.html",
            "relUrl": "/2020/07/07/Random_Network_Diagonalization-1-3.html",
            "date": " ‚Ä¢ Jul 7, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Modulo 5",
            "content": "A differenza dei precedenti moduli in questa relazione non mi occuper√≤ di studiare sistemi con interazioni a lunga distanza random poich√© il DMRG si basa fondamentalmente su interazioni a primi vicini. Sono possibili estensioni per considerare interazioni a lungo raggio, ma richiedono un notevole sforzo sia di implementazione che computazionale. Studier√≤ comunque il comportamento di una catena unidimensionale tipo Ising, ma considerando i valori delle costanti di accoppiamento dipendenti dalla posizione del sito rispetto alla catena, con valori random. . La Density Matrix Renormalization Group √® una tecnica numerica iterativa che consente di trovare il ground state, ed eventualmente pochi altri stati eccitati, di un sistema quantistico a bassa dimensionalit√† in una maniera estremamente efficiente. √à un metodo approssimato che si ispira alla rinormalizzazione numerica alla Wilson, ma il cui funzionamento si basa sull&#39;entanglement bipartito per il ground state di $ hat{H}$. Se l&#39;Hamiltoniana del sistema pu√≤ essere scritta come somma di termini locali riferiti ad un sito e ai suoi primi vicini e nel caso in cui il Ground State risulti non degenere, per il caso unidimensionale √® dimostrata la validit√† dell&#39;Area Law: considerando uno stato puro $| psi‚ü©_{AB}$ di un sistema quantistico bipartito AB, questa propriet√† esprime la dipendenza dell&#39;entropia di Von Neumann della partizione A dalla dimensione del confine tra A e B. $$ S( rho_A) sim dim(bound(A|B))$$ in cui $$ rho_A = Tr_B(| psi‚ü©_{AB}‚ü® psi|)$$ . Una generica Hamiltoniana che soddisfa queste condizioni pu√≤ essere scritta come: . $$ hat{H}= sum_{i=1}^L ( sum_ alpha J_i^{( alpha)} hat{S}_i^{( alpha)} hat{T}_{i+1}^{( alpha)} + sum_ beta B_i^{( beta)} hat{V}_i^{( beta)} ) $$ . dove gli $J_i^{( alpha)}$ e i $B_i^{( beta)}$ sono le costanti di accoppiamento mentre i $ { hat{S}_i^{( alpha)} }$, $ { hat{T}_{i+1}^{( alpha)} }$ e $ { hat{V}_i^{( beta)} }$ sono operatori agenti sul sito i-esimo, mentre $ alpha$ e $ beta$ sono le varie componenti degli operatori. . L&#39;Hamiltoniana del modello di Ising quantistico 1D rientra in questa forma: . $$ hat{H} = - sum_{i}^{L-1} J_{i} hat{ sigma}_i^z hat{ sigma}_{i+1}^z - sum_{i}^L g_i hat{ sigma}_i^x - sum_{i}^L h_i hat{ sigma}_i^z$$ . dove le varie $ sigma_i$ sono gli operatori di spin agenti sul sito i-esimo che, nella base canonica, sono rappresentati dalle matrici di Pauli. . L&#39;algoritmo DMRG Infinite-System si articola in alcuni step: . Si parte da un blocco $B(1,d)$, composto dal solo sito estremo di sinistra, di cui si definisce l&#39;Hamiltoniana $ hat{H}_B$, nel codice BlockH. Spazio di Hilbert di dimensione $d$. . | Si costruisce l&#39;Enlarged Block aggiungendo al blocco precedente il sito adiacente destro e si costruisce l&#39;hamiltoniana corrispondente $ hat{H}_E$: $$ hat{H}_E = hat{H}_B otimes mathbb{1}_{sito} + mathbb{1}_B otimes hat{H}_{sito} + hat{H}_{B-sito} $$ Spazio di Hilbert di dimensione $d^2$. . | Si costruisce il Super-block aggiundendo al blocco precedente un blocco speculare, considerando il fatto che il sistema in esame √® simmetrico per riflessione rispetto al centro della catena, il collegamento √® dato dall&#39;interazione dei due siti esterni aggiunti al passo precedente. L&#39;Hamiltoniana del Super-block diventa: $$ hat{H}_{SB} = hat{H}_E otimes mathbb{1}_{E&#39;} + mathbb{1}_E otimes hat{H}_{E&#39;} + hat{H}_{E-E&#39;} $$ La cui dimensione √® $d^4$. . | Si trova l&#39;autovalore minore ed il corrispondente autovettore di $ hat{H}_{SB}$, ovvero il Ground State $| psi_{gs}‚ü©$ e la sua energia $E_{gs}$. . | Si calcola la matrice di densit√† ridotta $ rho_L$ relativa al blocco di sinistra: $ rho_L=Tr_R | psi_{gs}‚ü©‚ü® psi_{gs}|$ che operativamente nel codice √® stato implementato calcolando $ rho_L= psi_{gs} psi_{gs}^ dagger$. . | Si diagonalizza $ rho_L$ ordinando gli autovalori $ lambda_i$ in senso decrescente e si ricava la rappresentazione della matrice densit√†: $$ rho_L = sum_{i=1}^{d^2} lambda_i |w_i‚ü©‚ü®w_i| $$ dove $|w_i‚ü©$ √® l&#39;autovettore corrispondente all&#39;autovalore $ lambda_i$. . | Di questi stati si tengono soltalto i primi $m$, corrispondenti agli autovalori $ lambda_i$ maggiori. $m$ √® scelto come il minimo tra la dimensione dello spazio di Hilbert del sistema e un valore fissato all&#39;inizio dell&#39;algoritmo, che rappresenta la dimensione massima dello spazio di Hilbert del sistema approssimato. Da questi $m$ autovettori si costruiscono due matrici $O$ e $O^ dagger$ diaponendoli rispettivamente per colonne e per righe. $O$ e $O^ dagger$ non sono matrici quadrate e servono appunto per troncare la matrice dell&#39;Hamiltoniana. . | L&#39;ultimo step √® infatti quello di calcolare la matrice hamiltoniana dell&#39;Enlarged Block nella base troncata attraverso: $$ tilde{H}_E=O^ dagger H_E O$$ Si rinormalizzano inoltre anche gli altri operatori che serviranno per il ciclo successivo. . | Nel ciclo successivo dell&#39;algoritmo sar√† dunque utilizzato l&#39;Enlarged Block rinormalizzato come nuovo blocco di partenza, chiamato $B(2,m)$, e per procedere con l&#39;algoritmo saranno utilizzati gli altri operatori rinormalizzati. . #collapse %matplotlib inline import matplotlib.pyplot as plt import numpy as np import scipy.sparse.linalg as ssl import scipy.sparse as ss . . Per tutti i modelli presentati si considereranno le condizioni al bordo aperte. . L&#39;osservabile di cui andremo a studiare l&#39;andamento per ogni tipo di modello √® l&#39;entropia di Von Neumann definita come: $$ S( rho) = -Tr[ rho log( rho)] = - sum_i lambda_i log( lambda_i)$$ . Infinite-System DMRG per il modello di Ising . Di seguito √® presentato il codice che implementa l&#39;Infinite-System DMRG per un modello di Ising quantistico 1D. Il codice restituisce i grafici dell&#39;andamento dell&#39;entropia al variare dell&#39;intensit√† del campo trasverso, parametrizzato da $g$ che varia da 0.6 a 1.4. Per ogni run dell&#39;algoritmo $J_i=J$ e $g_i=g$ sono costanti e $h_i=0$ $ forall i$. Poich√® il comportamente del sistema non dipende da entrambe le variabili diverse da zero, ma solo dal loro rapporto, consideriamo $J=1$. Ad ogni grafico corrisponde una diversa taglia del sistema descritta in legenda. . m=10 # Dimensione massima dello spazio di Hilbert del sistema approssimato NIter=300 # Numero di iterazioni dell&#39;algoritmo rep=100 # Numero di punti del grafico rep+=1 gmax=1.4 ling=np.linspace(0.6,gmax,rep) Evec=np.zeros(rep) Entropy=np.zeros(rep) graphs=np.zeros((rep,NIter)) # inizializzazione degli operatori I=np.eye(2,2) Sz=np.array([[1, 0],[0,-1]]) Sx=np.array([[0, 1],[1,0]]) ggg=0 for g in ling: #blocchi iniziali BlockSz = Sz BlockSx = Sx BlockI = I BlockH = g*Sx Energy = 0 for l in range(NIter): SystSize = 2*l + 4 # Get the 2m-dimensional operators for the block + site BlockH = np.kron(BlockH, I) + np.kron(BlockI, g*Sx) - np.kron(BlockSz, Sz) BlockSz = np.kron(BlockI, Sz) BlockSx = np.kron(BlockI, Sx) BlockI = np.kron(BlockI, I) # Matrice dell&#39;Hamiltoniana per il Super-Blocco H_super = np.kron(BlockH, BlockI) + np.kron(BlockI, BlockH) - np.kron(BlockSz, BlockSz) H_super = 0.5 * (H_super + H_super.T); # ensure H is symmetric # Diagonalizzazione dell&#39;Hamiltoniana LastEnergy = Energy Energy= ssl.eigsh(H_super,1,which=&#39;SA&#39;)[0] #[0] Psi = ssl.eigsh(H_super,1,which=&#39;SA&#39;)[1].T #[0] EnergyPerBond = (Energy - LastEnergy) / 2 Ener2 = Energy / SystSize # Sigma = Psi&#39; *kron(BlockSz,BlockSz) * Psi; % n.n. ZZ correlation function # Costruzione della matrice densit√† nr=Psi.size Dim = int(np.sqrt(nr)) PsiMatrix = np.reshape(Psi,(Dim,Dim)) Rho = PsiMatrix @ PsiMatrix.T # Diagonalizzazione della matrice densit√† D,V = np.linalg.eigh(Rho) D=D[::-1] # descending Index=np.arange(Dim) Index=Index[::-1] V=V[:,Index] # Construzione dell&#39;operatore di troncamento NKeep = min(D.size, m) Omatr = V[:,:NKeep] TruncationError = 1 - sum(D[:NKeep]) Ent=-sum(D[:NKeep]*np.log(D[:NKeep])) # Trasformazione degli operatori dei blocchi nella base troncata BlockH = Omatr.T @ BlockH @ Omatr BlockSz = Omatr.T @ BlockSz @ Omatr BlockSx = Omatr.T @ BlockSx @ Omatr BlockI = Omatr.T @ BlockI @ Omatr graphs[ggg,l]=Ent ell=SystSize Evec[ggg]=Energy Entropy[ggg]=Ent ggg+=1 for i in np.arange(2,8)*2+1: plt.plot(ling,graphs[:,2*i*10-1],label=2*i*10) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Entropy&#39;) plt.show() . Si pu√≤ notare come all&#39;aumentare della taglia il valore di $g$ corrispondente al picco dell&#39;entropia converga ad 1, come previsto teoricamente, ed il picco si alzi diventando sempre pi√π stretto, indicando nel limite termodinamico una transizione di fase. Per valori di $g$ maggiori di 1 l&#39;entropia si assesta ad un valore maggiore di zero, effetto delle condizioni al bordo aperte. . Interazioni a valori random . Di seguito √® riportato il grafico, ed il codice espandibile, di un modello di Ising 1D in cui i valori di $J_i$ non sono tutti uguali ad 1, ma sono estratti random da una distribuzione normale con media 1 e varianza 0.3 in questo caso. Ad ogni ciclo sono estratti due numeri random, dalla stessa distribuzione, uno per l&#39;interazione Blocco-Sito ed uno per l&#39;interazione tra due Enlarged Block. . #collapse m=10 NIter=500 rep=50 rep+=1 gmax=2 ling=np.linspace(0,gmax,rep) Evec=np.zeros(rep) Entropy=np.zeros(rep) ggg=0 # inizialize local ops I=np.eye(2,2) Sz=np.array([[1, 0],[0,-1]]) Sx=np.array([[0, 1],[1,0]]) # initial blocks for gm in ling: Ent=0 BlockSz = Sz BlockSx = Sx BlockI = I BlockH = gm*Sx Energy = 0 for l in range(NIter): SystSize = 2*l + 4 g=gm j1=np.random.default_rng().normal(1, .3) # Get the 2m-dimensional operators for the block + site BlockH = np.kron(BlockH, I) + np.kron(BlockI, g*Sx) - j1*np.kron(BlockSz, Sz) BlockSz = np.kron(BlockI, Sz) BlockSx = np.kron(BlockI, Sx) BlockI = np.kron(BlockI, I) # HAMILTONIAN MATRIX for superblock j2=np.random.default_rng().normal(1, .3) H_super = np.kron(BlockH, BlockI) + np.kron(BlockI, BlockH) - j2*np.kron(BlockSz, BlockSz) H_super = 0.5 * (H_super + H_super.T); # ensure H is symmetric # Diagonalizing the Hamiltonian LastEnergy = Energy Energy= ssl.eigsh(H_super,1,which=&#39;SA&#39;)[0] #[0] Psi = ssl.eigsh(H_super,1,which=&#39;SA&#39;)[1].T #[0] EnergyPerBond = (Energy - LastEnergy) / 2 Ener2 = Energy / SystSize # Sigma = Psi&#39; *kron(BlockSz,BlockSz) * Psi; % n.n. ZZ correlation function # Form the reduced density matrix nr=Psi.size Dim = int(np.sqrt(nr)) PsiMatrix = np.reshape(Psi,(Dim,Dim)) Rho = PsiMatrix @ PsiMatrix.T # Diagonalize the density matrix D,V = np.linalg.eigh(Rho) D=D[::-1] # descending Index=np.arange(Dim) Index=Index[::-1] V=V[:,Index] # Construct the truncation operator NKeep = min(D.size, m) Omatr = V[:,:NKeep] TruncationError = 1 - sum(D[:NKeep]) # Transform the block operators into the truncated basis BlockH = Omatr.T @ BlockH @ Omatr BlockSz = Omatr.T @ BlockSz @ Omatr BlockSx = Omatr.T @ BlockSx @ Omatr BlockI = Omatr.T @ BlockI @ Omatr Ent=-sum(D[:NKeep]*np.log(D[:NKeep])) #Evec[ggg]=Energy Entropy[ggg]=Ent ggg+=1 plt.plot(ling[1:],Entropy[1:]) plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Entropy&#39;) plt.show() . . Si pu√≤ notare come l&#39;andamento del grafico non si discosti troppo da quello del caso precedente. Risulta infatti un esserci una prima zona, per bassi valori di $g$, in cui l&#39;entropia √® piccola, un rapido aumento ed un picco nei pressi di $g=1$, in seguito un decremento ed un assestamento sempre dovuto alle condizioni al bordo aperte. Risultano evidenti comunque molte fluttuazioni lungo il grafico, effetto dovuto appunto alle interazioni tra primi vicini a valori random. . Bisogna comunque sottolineare che per ogni valore di $g$ l&#39;entropia calcolata si riferisce ad una catena con valori $J_i$ indipendenti dalle altre catene a diversi di $g$, per ogni valore di $g$ √® necessario infatti calcolare una Hamiltoniana che ha $J_i$ completamente nuovi e questo giustifica in parte le grandi oscillazioni che si possono osservare. . Un altro motivo, che verr√† studiato meglio nel proseguo della relazione, consiste nel fatto che per una catena di Ising &quot;normale&quot; il punto di transizioni si ha, come detto in precedenza, per valori di $g$ e $J$ tali che $g/J=1$, quindi delle fluttuazioni su $J$ possono comportare grandi fluttuazioni sull&#39;entropia finale del sistema. . Fluttuazioni mediate . Al fine di diminuire le fluttuazioni presenti in ogni grafico in modo da ricavare dei comportamenti generali, sono state calcolate varie realizzazioni di catene di Ising con interazioni a valori random aventi la stessa media 1 e la stessa varianza 0.3, per poi calcolarne la media, evidenziata con un tratto nero pi√π spesso. . #collapse m=10 NIter=300 rep=50 rep+=1 gmax=1.5 ling=np.linspace(0.5,gmax,rep) nrip=40 Evec=np.zeros((rep,nrip)) Entropy=np.zeros((rep,nrip)) ggg=0 # inizialize local ops I=np.eye(2,2) Sz=np.array([[1, 0],[0,-1]]) Sx=np.array([[0, 1],[1,0]]) # initial blocks for gm in ling: for rip in range(nrip): BlockSz = Sz BlockSx = Sx BlockI = I BlockH = gm*Sx Energy = 0 for l in range(NIter): SystSize = 2*l + 4 g=gm j1=np.random.default_rng().normal(1, .3) # Get the 2m-dimensional operators for the block + site BlockH = np.kron(BlockH, I) + np.kron(BlockI, g*Sx) - j1*np.kron(BlockSz, Sz) BlockSz = np.kron(BlockI, Sz) BlockSx = np.kron(BlockI, Sx) BlockI = np.kron(BlockI, I) # HAMILTONIAN MATRIX for superblock j2=np.random.default_rng().normal(1, .3) H_super = np.kron(BlockH, BlockI) + np.kron(BlockI, BlockH) - j2*np.kron(BlockSz, BlockSz) H_super = 0.5 * (H_super + H_super.T); # ensure H is symmetric # Diagonalizing the Hamiltonian LastEnergy = Energy Energy= ssl.eigsh(H_super,1,which=&#39;SA&#39;)[0] #[0] Psi = ssl.eigsh(H_super,1,which=&#39;SA&#39;)[1].T #[0] EnergyPerBond = (Energy - LastEnergy) / 2 Ener2 = Energy / SystSize # Sigma = Psi&#39; *kron(BlockSz,BlockSz) * Psi; % n.n. ZZ correlation function # Form the reduced density matrix nr=Psi.size Dim = int(np.sqrt(nr)) PsiMatrix = np.reshape(Psi,(Dim,Dim)) Rho = PsiMatrix @ PsiMatrix.T # Diagonalize the density matrix D,V = np.linalg.eigh(Rho) D=D[::-1] # descending Index=np.arange(Dim) Index=Index[::-1] V=V[:,Index] # Construct the truncation operator NKeep = min(D.size, m) Omatr = V[:,:NKeep] TruncationError = 1 - sum(D[:NKeep]) # Transform the block operators into the truncated basis BlockH = Omatr.T @ BlockH @ Omatr BlockSz = Omatr.T @ BlockSz @ Omatr BlockSx = Omatr.T @ BlockSx @ Omatr BlockI = Omatr.T @ BlockI @ Omatr Ent=-sum(D[:NKeep]*np.log(D[:NKeep])) Evec[ggg,rip]=Energy Entropy[ggg,rip]=Ent ggg+=1 plt.plot(ling[1:],Entropy[1:],color=&quot;b&quot;) plt.plot(ling[1:],np.sum(Entropy[1:],axis=1)/nrip,color=&#39;r&#39;, linewidth=2.0,label=&quot;Media&quot;) plt.legend() plt.show() ## E GLI ERRORI? . . Nonostante la permanenza di fluttuazioni importanti del grafico possiamo notare come la forma generale resti sostanzialmente invariata, ma possiamo osservare uno spostamento del picco verso sinistra, effetto non dovuto alla statistica in quanto altre simulazioni, come mostrato sotto, portano alle stesse conclusioni. . Comportamento per diverse varianze . √à possibile quindi pensare ad una correlazione tra la varianza della distribuzione da cui viengono estratti i $J_i$ e la posizione del picco dell&#39;entropia? . Per rispondere a questa domanda sono state calcolate le medie di 30 realizzazioni di catene di Ising con interazioni a valori random, corrispondenti a 5 valori di varianze, che vanno da 0.1 ad 1. . Di seguito sono riportati i grafici, insieme al codice espandibile, dell&#39;andamento dell&#39;entropia in funzione del parametro $g$, per i diversi valori della varianza. . #collapse m=8 NIter=200 rep=50 rep+=1 gmax=1.5 Nmedie=4 ling=np.linspace(0.5,gmax,rep) linsig=np.linspace(0.1,1,Nmedie) nrip=25 Evec=np.zeros((rep,nrip)) Entropy=np.zeros((rep,nrip)) MEnt=np.zeros((Nmedie,rep-1)) ggg=0 # inizialize local ops I=np.eye(2,2) Sz=np.array([[1, 0],[0,-1]]) Sx=np.array([[0, 1],[1,0]]) # initial blocks sss=0 for sig in linsig: ggg=0 for gm in ling: for rip in range(nrip): BlockSz = Sz BlockSx = Sx BlockI = I BlockH = gm*Sx Energy = 0 for l in range(NIter): SystSize = 2*l + 4 g=gm j1=np.random.default_rng().normal(1, sig) # Get the 2m-dimensional operators for the block + site BlockH = np.kron(BlockH, I) + np.kron(BlockI, g*Sx) - j1*np.kron(BlockSz, Sz) BlockSz = np.kron(BlockI, Sz) BlockSx = np.kron(BlockI, Sx) BlockI = np.kron(BlockI, I) # HAMILTONIAN MATRIX for superblock j2=np.random.default_rng().normal(1, sig) H_super = np.kron(BlockH, BlockI) + np.kron(BlockI, BlockH) - j2*np.kron(BlockSz, BlockSz) H_super = 0.5 * (H_super + H_super.T); # ensure H is symmetric # Diagonalizing the Hamiltonian LastEnergy = Energy Energy= ssl.eigsh(H_super,1,which=&#39;SA&#39;)[0] #[0] Psi = ssl.eigsh(H_super,1,which=&#39;SA&#39;)[1].T #[0] EnergyPerBond = (Energy - LastEnergy) / 2 Ener2 = Energy / SystSize # Sigma = Psi&#39; *kron(BlockSz,BlockSz) * Psi; % n.n. ZZ correlation function # Form the reduced density matrix nr=Psi.size Dim = int(np.sqrt(nr)) PsiMatrix = np.reshape(Psi,(Dim,Dim)) Rho = PsiMatrix @ PsiMatrix.T # Diagonalize the density matrix D,V = np.linalg.eigh(Rho) D=D[::-1] # descending Index=np.arange(Dim) Index=Index[::-1] V=V[:,Index] # Construct the truncation operator NKeep = min(D.size, m) Omatr = V[:,:NKeep] TruncationError = 1 - sum(D[:NKeep]) # Transform the block operators into the truncated basis BlockH = Omatr.T @ BlockH @ Omatr BlockSz = Omatr.T @ BlockSz @ Omatr BlockSx = Omatr.T @ BlockSx @ Omatr BlockI = Omatr.T @ BlockI @ Omatr #print(SystSize, Energy, EnergyPerBond, Ener2, TruncationError) Ent=-sum(D[:NKeep]*np.log(D[:NKeep])) #Evec[ggg,rip]=Energy Entropy[ggg,rip]=Ent ggg+=1 MEnt[sss]=np.sum(Entropy[1:],axis=1)/nrip sss+=1 . . for i in range(Nmedie): plt.plot(ling[1:],MEnt[i],label=linsig[i]) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Entropy&#39;) plt.show() . Possiamo notare come per valori della varianza maggiori di 0.1 il picco, oltre ad abbassarsi notevolmente, sembra spostarsi progressivamente pi√π a sinistra. . Per approfondire meglio questo effetto andremo a fare gli stessi passaggi fatti finora, ma considerando le fluttuazioni random sul campo esterno $g$. . Campi esterni a valori random . Di seguito √® riportato il grafico, ed il codice espandibile, di un modello di Ising 1D in cui i valori di $g_i$ non sono uguali per tutta la catena, ma vengono estratti da una distribuzione normale con media $g_m$ variabile e varianza, in questo caso, uguale a 0.4. . #collapse m=10 NIter=200 rep=50 rep+=1 gmax=2 ling=np.linspace(0,gmax,rep) Evec=np.zeros(rep) Entropy=np.zeros(rep) ggg=0 # inizialize local ops I=np.eye(2,2) Sz=np.array([[1, 0],[0,-1]]) Sx=np.array([[0, 1],[1,0]]) # initial blocks for gm in ling: Ent=0 BlockSz = Sz BlockSx = Sx BlockI = I BlockH = np.random.default_rng().normal(gm, .4)*Sx Energy = 0 for l in range(NIter): SystSize = 2*l + 4 g=np.random.default_rng().normal(gm, .4) # Get the 2m-dimensional operators for the block + site BlockH = np.kron(BlockH, I) + np.kron(BlockI, g*Sx) - np.kron(BlockSz, Sz) BlockSz = np.kron(BlockI, Sz) BlockSx = np.kron(BlockI, Sx) BlockI = np.kron(BlockI, I) # HAMILTONIAN MATRIX for superblock H_super = np.kron(BlockH, BlockI) + np.kron(BlockI, BlockH) - np.kron(BlockSz, BlockSz) H_super = 0.5 * (H_super + H_super.T); # ensure H is symmetric # Diagonalizing the Hamiltonian LastEnergy = Energy Energy= ssl.eigsh(H_super,1,which=&#39;SA&#39;)[0] #[0] Psi = ssl.eigsh(H_super,1,which=&#39;SA&#39;)[1].T #[0] EnergyPerBond = (Energy - LastEnergy) / 2 Ener2 = Energy / SystSize # Sigma = Psi&#39; *kron(BlockSz,BlockSz) * Psi; % n.n. ZZ correlation function # Form the reduced density matrix nr=Psi.size Dim = int(np.sqrt(nr)) PsiMatrix = np.reshape(Psi,(Dim,Dim)) Rho = PsiMatrix @ PsiMatrix.T # Diagonalize the density matrix D,V = np.linalg.eigh(Rho) D=D[::-1] # descending Index=np.arange(Dim) Index=Index[::-1] V=V[:,Index] # Construct the truncation operator NKeep = min(D.size, m) Omatr = V[:,:NKeep] TruncationError = 1 - sum(D[:NKeep]) # Transform the block operators into the truncated basis BlockH = Omatr.T @ BlockH @ Omatr BlockSz = Omatr.T @ BlockSz @ Omatr BlockSx = Omatr.T @ BlockSx @ Omatr BlockI = Omatr.T @ BlockI @ Omatr Ent=-sum(D[:NKeep]*np.log(D[:NKeep])) Entropy[ggg]=Ent ggg+=1 . . plt.plot(ling,Entropy) plt.show() . Oltre a presentare le stesse caratteristiche di prima presenta uno spostamento del picco verso destra, in maniera opposta rispetto a prima. . Fluttuazioni mediate . Di seguito sono state calcolate varie realizzazioni di catene di Ising con campi esterni a valori random, per poi calcolarne la media, in maniera da essere sicuri che questo effetto non sia solo statistico. . #collapse m=10 NIter=300 rep=50 rep+=1 gmax=1.5 ling=np.linspace(0.5,gmax,rep) nrip=30 #ling=ling[int(30*rep/100):] Evec=np.zeros((rep,nrip)) Entropy=np.zeros((rep,nrip)) ggg=0 # inizialize local ops I=np.eye(2,2) Sz=np.array([[1, 0],[0,-1]]) Sx=np.array([[0, 1],[1,0]]) # initial blocks for gm in ling: for rip in range(nrip): BlockSz = Sz BlockSx = Sx BlockI = I BlockH = np.random.default_rng().normal(gm, .5)*Sx Energy = 0 for l in range(NIter): SystSize = 2*l + 4 g=np.random.default_rng().normal(gm, .5) # Get the 2m-dimensional operators for the block + site BlockH = np.kron(BlockH, I) + np.kron(BlockI, g*Sx) - np.kron(BlockSz, Sz) BlockSz = np.kron(BlockI, Sz) BlockSx = np.kron(BlockI, Sx) BlockI = np.kron(BlockI, I) # HAMILTONIAN MATRIX for superblock H_super = np.kron(BlockH, BlockI) + np.kron(BlockI, BlockH) - np.kron(BlockSz, BlockSz) H_super = 0.5 * (H_super + H_super.T); # ensure H is symmetric # Diagonalizing the Hamiltonian LastEnergy = Energy Energy= ssl.eigsh(H_super,1,which=&#39;SA&#39;)[0] #[0] Psi = ssl.eigsh(H_super,1,which=&#39;SA&#39;)[1].T #[0] EnergyPerBond = (Energy - LastEnergy) / 2 Ener2 = Energy / SystSize # Sigma = Psi&#39; *kron(BlockSz,BlockSz) * Psi; % n.n. ZZ correlation function # Form the reduced density matrix nr=Psi.size Dim = int(np.sqrt(nr)) PsiMatrix = np.reshape(Psi,(Dim,Dim)) Rho = PsiMatrix @ PsiMatrix.T # Diagonalize the density matrix D,V = np.linalg.eigh(Rho) D=D[::-1] # descending Index=np.arange(Dim) Index=Index[::-1] V=V[:,Index] # Construct the truncation operator NKeep = min(D.size, m) Omatr = V[:,:NKeep] TruncationError = 1 - sum(D[:NKeep]) # Transform the block operators into the truncated basis BlockH = Omatr.T @ BlockH @ Omatr BlockSz = Omatr.T @ BlockSz @ Omatr BlockSx = Omatr.T @ BlockSx @ Omatr BlockI = Omatr.T @ BlockI @ Omatr Ent=-sum(D[:NKeep]*np.log(D[:NKeep])) Evec[ggg,rip]=Energy Entropy[ggg,rip]=Ent ggg+=1 . . plt.plot(ling[1:],Entropy[1:],color=&quot;b&quot;) plt.plot(ling[1:],np.sum(Entropy[1:],axis=1)/nrip,color=&#39;r&#39;, linewidth=2.0, label=&quot;Media&quot;) plt.legend() plt.show() . Anche in questo caso si pu√≤ osservare come lo spostamento verso destra non dipenda dalla statistica. . Comportamento per diverse varianze . Guardiamo ora il comportamento delle medie dei grafici dell&#39;entropia per alcuni valori di varianze diverse. . #collapse m=8 NIter=200 rep=50 rep+=1 gmax=1.5 Nmedie=4 ling=np.linspace(0.5,gmax,rep) linsig=np.linspace(0.1,1,Nmedie) nrip=25 #ling=ling[int(30*rep/100):] Evec=np.zeros((rep,nrip)) Entropy=np.zeros((rep,nrip)) MEnt2=np.zeros((Nmedie,rep-1)) ggg=0 # inizialize local ops I=np.eye(2,2) Sz=np.array([[1, 0],[0,-1]]) Sx=np.array([[0, 1],[1,0]]) # initial blocks sss=0 for sig in linsig: ggg=0 for gm in ling: for rip in range(nrip): BlockSz = Sz BlockSx = Sx BlockI = I #BlockH = gm*Sx BlockH = np.random.default_rng().normal(gm, sig)*Sx Energy = 0 for l in range(NIter): SystSize = 2*l + 4 g=np.random.default_rng().normal(gm, sig) # Get the 2m-dimensional operators for the block + site BlockH = np.kron(BlockH, I) + np.kron(BlockI, g*Sx) - np.kron(BlockSz, Sz) BlockSz = np.kron(BlockI, Sz) BlockSx = np.kron(BlockI, Sx) BlockI = np.kron(BlockI, I) # HAMILTONIAN MATRIX for superblock H_super = np.kron(BlockH, BlockI) + np.kron(BlockI, BlockH) - np.kron(BlockSz, BlockSz) H_super = 0.5 * (H_super + H_super.T); # ensure H is symmetric # Diagonalizing the Hamiltonian LastEnergy = Energy Energy= ssl.eigsh(H_super,1,which=&#39;SA&#39;)[0] #[0] Psi = ssl.eigsh(H_super,1,which=&#39;SA&#39;)[1].T #[0] EnergyPerBond = (Energy - LastEnergy) / 2 Ener2 = Energy / SystSize # Sigma = Psi&#39; *kron(BlockSz,BlockSz) * Psi; % n.n. ZZ correlation function # Form the reduced density matrix nr=Psi.size Dim = int(np.sqrt(nr)) PsiMatrix = np.reshape(Psi,(Dim,Dim)) Rho = PsiMatrix @ PsiMatrix.T # Diagonalize the density matrix D,V = np.linalg.eigh(Rho) D=D[::-1] # descending Index=np.arange(Dim) Index=Index[::-1] V=V[:,Index] # Construct the truncation operator NKeep = min(D.size, m) Omatr = V[:,:NKeep] TruncationError = 1 - sum(D[:NKeep]) # Transform the block operators into the truncated basis BlockH = Omatr.T @ BlockH @ Omatr BlockSz = Omatr.T @ BlockSz @ Omatr BlockSx = Omatr.T @ BlockSx @ Omatr BlockI = Omatr.T @ BlockI @ Omatr #print(SystSize, Energy, EnergyPerBond, Ener2, TruncationError) Ent=-sum(D[:NKeep]*np.log(D[:NKeep])) Entropy[ggg,rip]=Ent ggg+=1 MEnt2[sss]=np.sum(Entropy[1:],axis=1)/nrip sss+=1 . . for i in range(Nmedie): plt.plot(ling[1:],MEnt2[i],label=linsig[i]) plt.legend() plt.xlabel(&#39;g&#39;) plt.ylabel(&#39;Entropy&#39;) plt.show() . Anche in questo caso si pu√≤ apprezzare un graduale allontanamento del picco verso destra, nel senso opposto che nel caso delle interazioni a primi vicini con valori random. . Una possibile spiegazione . Ricordandoci che il punto critico del modello di Ising &quot;normale&quot; si trova per valori di $g$ e $J$ tali che $g/J=1$ possiamo notare che i due parametri competono in maniera reciproca al punto di transizione, quindi √® legittimo pensare di aver &quot;sbagliato&quot; ad imporre un tipo di rumore gaussiano ad entrambi i parametri, mentre in uno dei due avremmo dovuto imporre un rumore preso dall&#39;inverso di una distribuzione gaussiana. . Per testare questa ipotesi andiamo a cambiare il parametro $J$ nel codice in $1/J$ che rimane comunque con la stessa media poich√® essa √® sempre stata fissata ad 1. . #collapse m=9 NIter=200 rep=100 rep+=1 gmax=1.5 ling=np.linspace(0.5,gmax,rep) nrip=20 Evec=np.zeros((rep,nrip)) Entropy=np.zeros((rep,nrip)) ggg=0 # inizialize local ops I=np.eye(2,2) Sz=np.array([[1, 0],[0,-1]]) Sx=np.array([[0, 1],[1,0]]) # initial blocks for gm in ling: for rip in range(nrip): BlockSz = Sz BlockSx = Sx BlockI = I BlockH = gm*Sx Energy = 0 for l in range(NIter): SystSize = 2*l + 4 g=gm j1=np.random.default_rng().normal(1, .4) # Get the 2m-dimensional operators for the block + site BlockH = np.kron(BlockH, I) + np.kron(BlockI, g*Sx) - np.kron(BlockSz, Sz)/j1 BlockSz = np.kron(BlockI, Sz) BlockSx = np.kron(BlockI, Sx) BlockI = np.kron(BlockI, I) # HAMILTONIAN MATRIX for superblock j2=np.random.default_rng().normal(1, .4) H_super = np.kron(BlockH, BlockI) + np.kron(BlockI, BlockH) - np.kron(BlockSz, BlockSz)/j2 H_super = 0.5 * (H_super + H_super.T); # ensure H is symmetric # Diagonalizing the Hamiltonian LastEnergy = Energy Energy= ssl.eigsh(H_super,1,which=&#39;SA&#39;)[0] #[0] Psi = ssl.eigsh(H_super,1,which=&#39;SA&#39;)[1].T #[0] EnergyPerBond = (Energy - LastEnergy) / 2 Ener2 = Energy / SystSize # Sigma = Psi&#39; *kron(BlockSz,BlockSz) * Psi; % n.n. ZZ correlation function # Form the reduced density matrix nr=Psi.size Dim = int(np.sqrt(nr)) PsiMatrix = np.reshape(Psi,(Dim,Dim)) Rho = PsiMatrix @ PsiMatrix.T # Diagonalize the density matrix D,V = np.linalg.eigh(Rho) D=D[::-1] # descending Index=np.arange(Dim) Index=Index[::-1] V=V[:,Index] # Construct the truncation operator NKeep = min(D.size, m) Omatr = V[:,:NKeep] TruncationError = 1 - sum(D[:NKeep]) # Transform the block operators into the truncated basis BlockH = Omatr.T @ BlockH @ Omatr BlockSz = Omatr.T @ BlockSz @ Omatr BlockSx = Omatr.T @ BlockSx @ Omatr BlockI = Omatr.T @ BlockI @ Omatr Ent=-sum(D[:NKeep]*np.log(D[:NKeep])) Evec[ggg,rip]=Energy Entropy[ggg,rip]=Ent ggg+=1 EntMed1=np.sum(Entropy[1:],axis=1)/nrip plt.plot(ling[1:],Entropy[1:]) plt.plot(ling[1:],EntMed1,color=&#39;k&#39;, linewidth=2.0) plt.show() . . A parte la grande fluttuazione dei singoli grafici, possiamo notare come in media effettivamente il picco si sia spostato verso valori di $g$ maggiori, contrariamente a prima e in maniera concorde con il caso del rumore su $g$. . Per controllare quest&#39;ultima affermazione andremo a fare lo stesso calcolo con gli stessi valori dell&#39;algoritmo e con la stessa varianza ma su $g$ come prima. . #collapse m=9 NIter=200 rep=100 rep+=1 gmax=1.5 ling=np.linspace(0.5,gmax,rep) nrip=20 #ling=ling[int(30*rep/100):] Evec=np.zeros((rep,nrip)) Entropy=np.zeros((rep,nrip)) ggg=0 # inizialize local ops I=np.eye(2,2) Sz=np.array([[1, 0],[0,-1]]) Sx=np.array([[0, 1],[1,0]]) # initial blocks for gm in ling: for rip in range(nrip): BlockSz = Sz BlockSx = Sx BlockI = I BlockH = np.random.default_rng().normal(gm, .4)*Sx Energy = 0 for l in range(NIter): SystSize = 2*l + 4 g=np.random.default_rng().normal(gm, .4) # Get the 2m-dimensional operators for the block + site BlockH = np.kron(BlockH, I) + np.kron(BlockI, g*Sx) - np.kron(BlockSz, Sz) BlockSz = np.kron(BlockI, Sz) BlockSx = np.kron(BlockI, Sx) BlockI = np.kron(BlockI, I) # HAMILTONIAN MATRIX for superblock H_super = np.kron(BlockH, BlockI) + np.kron(BlockI, BlockH) - np.kron(BlockSz, BlockSz) H_super = 0.5 * (H_super + H_super.T); # ensure H is symmetric # Diagonalizing the Hamiltonian LastEnergy = Energy Energy= ssl.eigsh(H_super,1,which=&#39;SA&#39;)[0] #[0] Psi = ssl.eigsh(H_super,1,which=&#39;SA&#39;)[1].T #[0] EnergyPerBond = (Energy - LastEnergy) / 2 Ener2 = Energy / SystSize # Sigma = Psi&#39; *kron(BlockSz,BlockSz) * Psi; % n.n. ZZ correlation function # Form the reduced density matrix nr=Psi.size Dim = int(np.sqrt(nr)) PsiMatrix = np.reshape(Psi,(Dim,Dim)) Rho = PsiMatrix @ PsiMatrix.T # Diagonalize the density matrix D,V = np.linalg.eigh(Rho) D=D[::-1] # descending Index=np.arange(Dim) Index=Index[::-1] V=V[:,Index] # Construct the truncation operator NKeep = min(D.size, m) Omatr = V[:,:NKeep] TruncationError = 1 - sum(D[:NKeep]) # Transform the block operators into the truncated basis BlockH = Omatr.T @ BlockH @ Omatr BlockSz = Omatr.T @ BlockSz @ Omatr BlockSx = Omatr.T @ BlockSx @ Omatr BlockI = Omatr.T @ BlockI @ Omatr #print(SystSize, Energy, EnergyPerBond, Ener2, TruncationError) Ent=-sum(D[:NKeep]*np.log(D[:NKeep])) Evec[ggg,rip]=Energy Entropy[ggg,rip]=Ent ggg+=1 EntMed2=np.sum(Entropy[1:],axis=1)/nrip plt.plot(ling[1:],Entropy[1:]) plt.plot(ling[1:],EntMed2,color=&#39;k&#39;, linewidth=2.0) plt.show() . . Anche in questo caso osserviamo lo stesso spostamento del picco verso destra anche se il valore di $g$ relativo al picco non √® lo stesso che nel caso precedente. Questo fatto potrebbe essere dovuto o alla statistica o, pi√π probabilmente, al fatto che per ritrovare la stessa distribuzione la varianza su $g$ non deve essere la stessa di quella su $J$ che poi deve essere invertita, ma comunque il comportamento generale sembra essere pi√π o meno quello, come dimostrato dalla differenza delle due medie precedenti che mostrano comunque scostamenti non eccessivi dallo zero. . #collapse plt.plot(ling[1:],EntMed1-EntMed2) plt.show() . .",
            "url": "https://edoarder.github.io/Metodi_Numerici/2020/06/06/DMRG-Random_Ising-fin.html",
            "relUrl": "/2020/06/06/DMRG-Random_Ising-fin.html",
            "date": " ‚Ä¢ Jun 6, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Io sono Edoardo Maggioni, studente del secondo anno di Fisica Teorica presso l‚ÄôUniversit√† di Pisa. . Sono nato a Siena, dove sono cresciuto e dove ho frequentato il corso di laurea triennale in Fisica e Tecnologie Avanzate, laureandomi con lode. . Al di fuori del mondo della fisica ho coltivato svariati hobby, mi piace appassionarmi per certi periodi ad argomenti particolari di mio interesse che vanno dall‚Äôinformatica al Judo, dall‚ÄôUltimate Frisbee all‚Äôarte. Il mio interesse principale √® la musica, sia ascoltarla che suonarla: riesco ad apprezzare un gran numero di generi musicali, specialmente contemporanei. Suono tastiera e chitarra, ho studiato armonia e qualcosa di composizione. Conosco le basi della produzione musicale analogica e digitale ed ogni tanto faccio qualche brano. Ho infine seguito e conseguito l‚Äôesame di Fisica Musicale per il quale ho sviluppato interamente un sintetizzatore con Matlab. . Nella mia vita ho viaggiato tanto, specialmente in Europa, ma punto a viaggiare ancora di pi√π in futuro. . Considero l‚Äôoriginalit√† uno dei tratti essenziali della vita in generale. .",
          "url": "https://edoarder.github.io/Metodi_Numerici/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://edoarder.github.io/Metodi_Numerici/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}